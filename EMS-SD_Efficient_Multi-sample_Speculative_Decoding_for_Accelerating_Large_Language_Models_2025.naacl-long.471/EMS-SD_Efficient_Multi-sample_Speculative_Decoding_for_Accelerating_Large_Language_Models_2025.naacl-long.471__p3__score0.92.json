{
  "source_pdf": "/home/zzangmane/2025_null_Figure/pdfs/naacl-2025/EMS-SD_Efficient_Multi-sample_Speculative_Decoding_for_Accelerating_Large_Language_Models_2025.naacl-long.471.pdf",
  "page": 3,
  "figureType": null,
  "name": "3",
  "caption": "Figure 3: The detailed processing of unpad input tokens of decoding step 1 in Figure 2. Sample 0 predicted 5 tokens, while sample 1 predicted 2 tokens. All tokens are concatenated before inference, and the sample/sequence index is restored when attention is computed within the CUDA kernels. Consequently, each token is aware of the specific KV caches to which it can utilize for parallel computation.",
  "regionBoundary": {
    "x1": 321.59999999999997,
    "x2": 507.35999999999996,
    "y1": 479.52,
    "y2": 671.04
  },
  "score": 0.92,
  "reason": "Shows a multi-step conceptual process and relationships in a system's architecture."
}