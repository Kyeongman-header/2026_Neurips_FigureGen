{
  "source_pdf": "/home/zzangmane/2025_null_Figure/pdfs/acl-2025/ProBench_Judging_Multimodal_Foundation_Models_on_Open-ended_Multi-domain_Expert_Tasks_2025.findings-acl.568.pdf",
  "page": 2,
  "figureType": null,
  "name": "4",
  "caption": "Figure 4: Framework of ProBench. Starting with 100K crowdsourced conversations, we identify high-quality user queries to curate single-round, multi-linguistic, and multi-round tracks. Using MLLM-as-a-Judge, we benchmark and rank 24 state-of-theart MLLMs with ELO ratings. To ensure fairness, the ELO ratings are de-biased to remove confounder effects (e.g., MLLM response formats), resulting in the final ProBench leaderboard. Icons in the figure are sourced from (Freepik et al., 2025).",
  "regionBoundary": {
    "x1": 72.0,
    "x2": 530.4,
    "y1": 71.52,
    "y2": 220.32
  },
  "score": 1.0,
  "reason": "Illustrates an end-to-end framework with labeled system components and data flow."
}