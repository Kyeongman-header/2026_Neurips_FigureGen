{
  "source_pdf": "/home/zzangmane/2025_null_Figure/pdfs/emnlp-2024/On_the_Fragility_of_Active_Learners_for_Text_Classification_2024.emnlp-main.1240.pdf",
  "page": 2,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: The space of experiments is shown. See §4.1 for description. All representations are produced by pre-trained models, which are ubiquitous in practice today. The lines between the boxes “Representation” and “Classifier” denote combinations that constitute our prediction pipelines. Note that RoBERTa is an end-to-end predictor, where there are no separate representation and classification steps. Also note that the popular Transformer architecture (Vaswani et al., 2017) is represented by RoBERTa and MPNet here.",
  "regionBoundary": {
    "x1": 70.56,
    "x2": 525.12,
    "y1": 70.56,
    "y2": 336.47999999999996
  },
  "score": 1.0,
  "reason": "Shows complete system overview with datasets, representations, classifiers, and query strategies."
}