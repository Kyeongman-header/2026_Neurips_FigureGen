window.PAPER_DATA = [
  {
    "title": "... ... ...",
    "folder_name": "..._..._...",
    "images": [
      {
        "filename": "..._..._...__p0__score0.90.png",
        "path": "figs_human_eval_papers/..._..._.../..._..._...__p0__score0.90.png",
        "caption": "Figure 1: An example of a topic shift dialogue. The MP2D framework utilizes paths in a Knowledge Graph (KG) to extract entities and facilitates natural topic transitions based on the relations between these entities."
      },
      {
        "filename": "..._..._...__p2__score1.00.png",
        "path": "figs_human_eval_papers/..._..._.../..._..._...__p2__score1.00.png",
        "caption": "Figure 2: An overview of the MP2D framework. In the knowledge graph, paths are identified and passages are retrieved for entities within those paths. Then, the retrieved passages and their relations become the \"answers\", and a LLM generates \"questions\" corresponding to each answer to create dialogues."
      }
    ]
  },
  {
    "title": "AGrail A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection",
    "folder_name": "AGrail_A_Lifelong_Agent_Guardrail_with_Effective_and_Adaptive_Safety_Detection",
    "images": [
      {
        "filename": "AGrail_A_Lifelong_Agent_Guardrail_with_Effective_and_Adaptive_Safety_Detection__p0__score0.95.png",
        "path": "figs_human_eval_papers/AGrail_A_Lifelong_Agent_Guardrail_with_Effective_and_Adaptive_Safety_Detection/AGrail_A_Lifelong_Agent_Guardrail_with_Effective_and_Adaptive_Safety_Detection__p0__score0.95.png",
        "caption": "Figure 1: Risk on Computer-use Agents. Our framework can defend against systemic and task-specific risks and prevent them before agent actions are executed in environment."
      },
      {
        "filename": "AGrail_A_Lifelong_Agent_Guardrail_with_Effective_and_Adaptive_Safety_Detection__p3__score1.00.png",
        "path": "figs_human_eval_papers/AGrail_A_Lifelong_Agent_Guardrail_with_Effective_and_Adaptive_Safety_Detection/AGrail_A_Lifelong_Agent_Guardrail_with_Effective_and_Adaptive_Safety_Detection__p3__score1.00.png",
        "caption": "Figure 2: Workflow of AGrail. When the OS agent moves a file as requested, it may accidently overwrite an existing file in the target path. Our framework, guided by safety criteria, prevents this by generating and performing safety checks to invoke the corresponding tool that verifies if the file already exists, ensuring the action does not cause damage."
      }
    ]
  },
  {
    "title": "AKE Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
    "folder_name": "AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions",
    "images": [
      {
        "filename": "AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions__p0__score0.70.png",
        "path": "figs_human_eval_papers/AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions/AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions__p0__score0.70.png",
        "caption": "Figure 1: An example of our benchmark MQUAKE. Existing knowledge-editing methods often perform well at answering paraphrased questions of the edited fact but fail on multi-hop questions that are entailed consequences of the edited fact."
      },
      {
        "filename": "AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions__p3__score0.80.png",
        "path": "figs_human_eval_papers/AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions/AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions__p3__score0.80.png",
        "caption": "Table 1: An instance in the MQUAKE-CF dataset, which consists of an edit set E , a set of three multi-hop questions Q, the desirable answer pre- and post-editing a, a∗, and the chain of facts pre- and post-editing C,C∗. The edited facts are marked as :: (s, :: r, :::"
      },
      {
        "filename": "AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions__p6__score1.00.png",
        "path": "figs_human_eval_papers/AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions/AKE_Assessing_Knowledge_Editing_in_Language_Models_via_Multi-Hop_Questions__p6__score1.00.png",
        "caption": "Figure 3: The illustration of our proposed method MeLLo. MeLLo decompose a multi-hop question into subquestions iteratively. When a subquestion is generated, the base model generates a tentative answer to the subquestion. Then, the subquestion is used to retrieve a most relevant fact from the edited fact memory. The model checks if the retrieved fact contradicts the generated answer and updates the prediction accordingly. The concrete prompts used in MeLLo are shown in Appedix F."
      }
    ]
  },
  {
    "title": "A Theory of Response Sampling in LLMs Part Descriptive and Part Prescriptive",
    "folder_name": "A_Theory_of_Response_Sampling_in_LLMs_Part_Descriptive_and_Part_Prescriptive",
    "images": [
      {
        "filename": "A_Theory_of_Response_Sampling_in_LLMs_Part_Descriptive_and_Part_Prescriptive__p1__score1.00.png",
        "path": "figs_human_eval_papers/A_Theory_of_Response_Sampling_in_LLMs_Part_Descriptive_and_Part_Prescriptive/A_Theory_of_Response_Sampling_in_LLMs_Part_Descriptive_and_Part_Prescriptive__p1__score1.00.png",
        "caption": "Figure 1: From left to right: when sampling on a concept, the LLM appears to account for the statistical likelihood (A(C)) and prescriptive norm (I(C)) of the concept. Consequently, the sample distribution exhibits a shift (shown as α) away from the true distribution in the direction of the ideal (right most plot)."
      },
      {
        "filename": "A_Theory_of_Response_Sampling_in_LLMs_Part_Descriptive_and_Part_Prescriptive__p4__score0.80.png",
        "path": "figs_human_eval_papers/A_Theory_of_Response_Sampling_in_LLMs_Part_Descriptive_and_Part_Prescriptive/A_Theory_of_Response_Sampling_in_LLMs_Part_Descriptive_and_Part_Prescriptive__p4__score0.80.png",
        "caption": "Figure 2: The figure shows the average, ideal, and sample values reported by the LLM for three different concepts. Positive α shows the deviation in the direction of the ideal."
      }
    ]
  },
  {
    "title": "Active Prompting with Chain-of-Thought for Large Language Models",
    "folder_name": "Active_Prompting_with_Chain-of-Thought_for_Large_Language_Models",
    "images": [
      {
        "filename": "Active_Prompting_with_Chain-of-Thought_for_Large_Language_Models__p1__score1.00.png",
        "path": "figs_human_eval_papers/Active_Prompting_with_Chain-of-Thought_for_Large_Language_Models/Active_Prompting_with_Chain-of-Thought_for_Large_Language_Models__p1__score1.00.png",
        "caption": "Figure 1: Illustrations of our proposed approach. There are four stages. (1) Uncertainty Estimation: with or without a few human-written chain-of-thoughts, we query the large language model k (k “ 5 in this illustration) times to generate possible answers with intermediate steps for a set of training questions. Then, we calculate the uncertainty u based on k answers via an uncertainty metric (we use disagreement in this illustration). (2) Selection: according to the uncertainty, we select the most uncertain questions for annotation. (3) Annotation: we involve humans to annotate the selected questions. (4) Inference: infer each question with the new annotated exemplars."
      }
    ]
  },
  {
    "title": "AdaRewriter Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation",
    "folder_name": "AdaRewriter_Unleashing_the_Power_of_Prompting-based_Conversational_Query_Reformulation_via_Test-Time_Adaptation",
    "images": [
      {
        "filename": "AdaRewriter_Unleashing_the_Power_of_Prompting-based_Conversational_Query_Reformulation_via_Test-Time_Adaptation__p2__score1.00.png",
        "path": "figs_human_eval_papers/AdaRewriter_Unleashing_the_Power_of_Prompting-based_Conversational_Query_Reformulation_via_Test-Time_Adaptation/AdaRewriter_Unleashing_the_Power_of_Prompting-based_Conversational_Query_Reformulation_via_Test-Time_Adaptation__p2__score1.00.png",
        "caption": "Figure 2: Overview of AdaRewriter."
      }
    ]
  },
  {
    "title": "Are LLM-Judges Robust to Expressions of Uncertainty Investigating the effect of Epistemic Markers on LLM-based Evaluation",
    "folder_name": "Are_LLM-Judges_Robust_to_Expressions_of_Uncertainty_Investigating_the_effect_of_Epistemic_Markers_on_LLM-based_Evaluation",
    "images": [
      {
        "filename": "Are_LLM-Judges_Robust_to_Expressions_of_Uncertainty_Investigating_the_effect_of_Epistemic_Markers_on_LLM-based_Evaluation__p4__score1.00.png",
        "path": "figs_human_eval_papers/Are_LLM-Judges_Robust_to_Expressions_of_Uncertainty_Investigating_the_effect_of_Epistemic_Markers_on_LLM-based_Evaluation/Are_LLM-Judges_Robust_to_Expressions_of_Uncertainty_Investigating_the_effect_of_Epistemic_Markers_on_LLM-based_Evaluation__p4__score1.00.png",
        "caption": "Figure 2: Metrics for measuring LLM-judges’ robustness against epistemic markers. Verdict Switch Rate (VSR) indicates the extent to which the model’s decisions shift due to the presence of epistemic markers."
      }
    ]
  },
  {
    "title": "Beyond Demographics Fine-tuning Large Language Models to Predict Individuals Subjective Text Perceptions",
    "folder_name": "Beyond_Demographics_Fine-tuning_Large_Language_Models_to_Predict_Individuals_Subjective_Text_Perceptions",
    "images": [
      {
        "filename": "Beyond_Demographics_Fine-tuning_Large_Language_Models_to_Predict_Individuals_Subjective_Text_Perceptions__p0__score1.00.png",
        "path": "figs_human_eval_papers/Beyond_Demographics_Fine-tuning_Large_Language_Models_to_Predict_Individuals_Subjective_Text_Perceptions/Beyond_Demographics_Fine-tuning_Large_Language_Models_to_Predict_Individuals_Subjective_Text_Perceptions__p0__score1.00.png",
        "caption": "Figure 1: Unlike existing works that majorly rely on zero-shot demographic prompting, we explore whether LLMs can be trained to predict individuals’ subjective text perceptions."
      }
    ]
  },
  {
    "title": "Blinded by Generated Contexts How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts",
    "folder_name": "Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts",
    "images": [
      {
        "filename": "Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts__p2__score0.95.png",
        "path": "figs_human_eval_papers/Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts/Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts__p2__score0.95.png",
        "caption": "Figure 3: The task to study LLMs’ merging mechanisms by tracing the sources of the answers."
      },
      {
        "filename": "Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts__p2__score1.00.png",
        "path": "figs_human_eval_papers/Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts/Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts__p2__score1.00.png",
        "caption": "Figure 2: The frameworks of retrieval-augmented approach, generation-augmented approach, and hybrid approach."
      },
      {
        "filename": "Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts__p3__score1.00.png",
        "path": "figs_human_eval_papers/Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts/Blinded_by_Generated_Contexts_How_Language_Models_Merge_Generated_and_Retrieved_Contexts_When_Knowledge_Conflicts__p3__score1.00.png",
        "caption": "Figure 4: The framework of constructing context-conflicting datasets."
      }
    ]
  },
  {
    "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
    "folder_name": "Boosting_Language_Models_Reasoning_with_Chain-of-Knowledge_Prompting",
    "images": [
      {
        "filename": "Boosting_Language_Models_Reasoning_with_Chain-of-Knowledge_Prompting__p1__score1.00.png",
        "path": "figs_human_eval_papers/Boosting_Language_Models_Reasoning_with_Chain-of-Knowledge_Prompting/Boosting_Language_Models_Reasoning_with_Chain-of-Knowledge_Prompting__p1__score1.00.png",
        "caption": "Figure 1: Comparison of three prompting methods: (a) ICL, (b) Chain-of-Thought (CoT), and (c) Chain-ofKnowledge (CoK) solving a StrategyQA question."
      },
      {
        "filename": "Boosting_Language_Models_Reasoning_with_Chain-of-Knowledge_Prompting__p2__score1.00.png",
        "path": "figs_human_eval_papers/Boosting_Language_Models_Reasoning_with_Chain-of-Knowledge_Prompting/Boosting_Language_Models_Reasoning_with_Chain-of-Knowledge_Prompting__p2__score1.00.png",
        "caption": "Figure 2: The proposed framework. We first construct exemplars with chain-of-knowledge (CoK) prompts. Then, the CoK prompts can be used to let the LLM generate reasoning chains, including evidence triples, explanation hints, and the final answer. Lastly, we estimate the reliability of reasoning chains in terms of factuality and faithfulness, and the unreliable ones will be rethought."
      }
    ]
  },
  {
    "title": "Bridging the Visual Gap Fine-Tuning Multimodal Models with Knowledge-Adapted Captions",
    "folder_name": "Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions",
    "images": [
      {
        "filename": "Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions__p0__score1.00.png",
        "path": "figs_human_eval_papers/Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions/Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions__p0__score1.00.png",
        "caption": "Figure 1: KnowAda identifies knowledge gaps of a VLM and adapts the dense caption accordingly. The KnowAda dense captions are better suited for downstream fine-tuning of the VLM."
      },
      {
        "filename": "Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions__p1__score1.00.png",
        "path": "figs_human_eval_papers/Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions/Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions__p1__score1.00.png",
        "caption": "Figure 2: Our proposed KnowAda pipeline. We first probe the knowledge of the VLM, identifying the known and unknown parts of the image description, by generating questions about the visual content of the image mentioned in the caption. Then, KnowAda identifies the knowledge gaps by judging the VLM answers to these questions. Finally, KnowAda adapt the description to match these gaps (e.g., removing the number of limousines mentioned in the caption, which relates to a question the model failed to answer)."
      },
      {
        "filename": "Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions__p3__score1.00.png",
        "path": "figs_human_eval_papers/Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions/Bridging_the_Visual_Gap_Fine-Tuning_Multimodal_Models_with_Knowledge-Adapted_Captions__p3__score1.00.png",
        "caption": "Figure 4: DNLI Evaluation. Given a generated description by a VLM, we decompose it to atomic propositions. Then, we classify each proposition to either entailed, contradicted or neutral, conditioned on the ground-truth description. Finally, we calculate the descriptiveness and contradiction based on the number of entailed and contradicted propositions."
      }
    ]
  },
  {
    "title": "Can You Trick the Grader Adversarial Persuasion of LLM Judges",
    "folder_name": "Can_You_Trick_the_Grader_Adversarial_Persuasion_of_LLM_Judges",
    "images": [
      {
        "filename": "Can_You_Trick_the_Grader_Adversarial_Persuasion_of_LLM_Judges__p0__score0.90.png",
        "path": "figs_human_eval_papers/Can_You_Trick_the_Grader_Adversarial_Persuasion_of_LLM_Judges/Can_You_Trick_the_Grader_Adversarial_Persuasion_of_LLM_Judges__p0__score0.90.png",
        "caption": "Figure 1: Given a math question and a candidate solution, the LLM judge evaluates the correctness of the response. When persuasive language is embedded in the solution, the model assigns unfairly inflated scores despite no improvement in factual correctness."
      }
    ]
  },
  {
    "title": "Carpe Diem On the Evaluation of World Knowledge in Lifelong Language Models",
    "folder_name": "Carpe_Diem_On_the_Evaluation_of_World_Knowledge_in_Lifelong_Language_Models",
    "images": [
      {
        "filename": "Carpe_Diem_On_the_Evaluation_of_World_Knowledge_in_Lifelong_Language_Models__p0__score0.95.png",
        "path": "figs_human_eval_papers/Carpe_Diem_On_the_Evaluation_of_World_Knowledge_in_Lifelong_Language_Models/Carpe_Diem_On_the_Evaluation_of_World_Knowledge_in_Lifelong_Language_Models__p0__score0.95.png",
        "caption": "Figure 1: An overview of our evaluation benchmark, EvolvingQA. Our benchmark employs LLM to generate question-answer pairs based on the changes in Wikipedia’s snapshots, effectively capturing the temporal evolution of the knowledge base."
      },
      {
        "filename": "Carpe_Diem_On_the_Evaluation_of_World_Knowledge_in_Lifelong_Language_Models__p2__score1.00.png",
        "path": "figs_human_eval_papers/Carpe_Diem_On_the_Evaluation_of_World_Knowledge_in_Lifelong_Language_Models/Carpe_Diem_On_the_Evaluation_of_World_Knowledge_in_Lifelong_Language_Models__p2__score1.00.png",
        "caption": "Figure 2: Construction pipeline of EDITED. The final question-answers pair after filtering processes in this Figure is included in EDITED06. The full description of the pipeline is in Section 2.2 and Appendix C."
      }
    ]
  },
  {
    "title": "CoPL Collaborative Preference Learning for Personalizing LLMs",
    "folder_name": "CoPL_Collaborative_Preference_Learning_for_Personalizing_LLMs",
    "images": [
      {
        "filename": "CoPL_Collaborative_Preference_Learning_for_Personalizing_LLMs__p3__score1.00.png",
        "path": "figs_human_eval_papers/CoPL_Collaborative_Preference_Learning_for_Personalizing_LLMs/CoPL_Collaborative_Preference_Learning_for_Personalizing_LLMs__p3__score1.00.png",
        "caption": "Figure 2: An overview of CoPL. To learn user representations, the GCF model is trained on a user-response bipartite graph. To build a personalized reward model, CoPL uses the learned representations to select a user-specific expert from MoLE, enabling effective modeling of diverse preferences."
      },
      {
        "filename": "CoPL_Collaborative_Preference_Learning_for_Personalizing_LLMs__p4__score0.95.png",
        "path": "figs_human_eval_papers/CoPL_Collaborative_Preference_Learning_for_Personalizing_LLMs/CoPL_Collaborative_Preference_Learning_for_Personalizing_LLMs__p4__score0.95.png",
        "caption": "Figure 3: Illustration of unseen user adaptation. Blue nodes are users who have similar preferences to u∗, and red nodes are users who have dissimilar preferences."
      }
    ]
  },
  {
    "title": "Collaborative Instance Object Navigation Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues",
    "folder_name": "Collaborative_Instance_Object_Navigation_Leveraging_Uncertainty-Awareness_to_Minimize_Human-Agent_Dialogues",
    "images": [
      {
        "filename": "Collaborative_Instance_Object_Navigation_Leveraging_Uncertainty-Awareness_to_Minimize_Human-Agent_Dialogues__p3__score1.00.png",
        "path": "figs_human_eval_papers/Collaborative_Instance_Object_Navigation_Leveraging_Uncertainty-Awareness_to_Minimize_Human-Agent_Dialogues/Collaborative_Instance_Object_Navigation_Leveraging_Uncertainty-Awareness_to_Minimize_Human-Agent_Dialogues__p3__score1.00.png",
        "caption": "Figure 2. Graphical depiction of AIUTA: left shows its interaction cycle with the user, and right provides an exploded view of our method. ① The agent receives an initial instruction I: “Find a c =<object category>”. ② At each timestep t, a zero-shot policy π [53], comprising a frozen object detection module [24], selects the optimal action at. ③ Upon detection, the agent performs the proposed AIUTA. Specifically, ④ the agent first obtains an initial scene description of observation Ot from a VLM. Then, a Self-Questioner module leverages an LLM to automatically generate attribute-specific questions to the VLM, acquiring more information and refining the scene description with reduced attribute-level uncertainty, producing Srefined. ⑤ The Interaction Trigger module then evaluates Srefined against the “facts” related to the target, to determine whether to terminate the navigation (if the agent believes it has located the target object ⑥), or to pose template-free, natural-language questions to a human ⑦, updating the “facts” based on the response ⑧."
      }
    ]
  },
  {
    "title": "Competition of Mechanisms Tracing How Language Models Handle Facts and Counterfactuals",
    "folder_name": "Competition_of_Mechanisms_Tracing_How_Language_Models_Handle_Facts_and_Counterfactuals",
    "images": [
      {
        "filename": "Competition_of_Mechanisms_Tracing_How_Language_Models_Handle_Facts_and_Counterfactuals__p0__score1.00.png",
        "path": "figs_human_eval_papers/Competition_of_Mechanisms_Tracing_How_Language_Models_Handle_Facts_and_Counterfactuals/Competition_of_Mechanisms_Tracing_How_Language_Models_Handle_Facts_and_Counterfactuals__p0__score1.00.png",
        "caption": "Figure 1: Top: An example showing that LLMs can fail to recognize the correct mechanism when multiple possible mechanisms exist. Bottom: Our mechanistic inspection of where and how the competition of mechanisms takes place within the LLMs."
      }
    ]
  },
  {
    "title": "Con dence Improves Self-Consistency in LLMs",
    "folder_name": "Con_dence_Improves_Self-Consistency_in_LLMs",
    "images": [
      {
        "filename": "Con_dence_Improves_Self-Consistency_in_LLMs__p1__score0.90.png",
        "path": "figs_human_eval_papers/Con_dence_Improves_Self-Consistency_in_LLMs/Con_dence_Improves_Self-Consistency_in_LLMs__p1__score0.90.png",
        "caption": "Figure 2: A simplified example comparing self-consistency vs CISC. (1) Given an input question, (2) both methods first sample multiple reasoning paths. (4, top) Self-consistency then simply selects the most frequent answer. Conversely, (3) CISC adds a self-assessment step, where a confidence score is assigned to each path (see §4.1 for more advanced methods). Then, (4, bottom) it selects the final answer via a weighted majority vote."
      }
    ]
  },
  {
    "title": "Conditional MASK Discrete Diffusion Language Model",
    "folder_name": "Conditional_MASK_Discrete_Diffusion_Language_Model",
    "images": [
      {
        "filename": "Conditional_MASK_Discrete_Diffusion_Language_Model__p0__score1.00.png",
        "path": "figs_human_eval_papers/Conditional_MASK_Discrete_Diffusion_Language_Model/Conditional_MASK_Discrete_Diffusion_Language_Model__p0__score1.00.png",
        "caption": "Figure 1: Overview of how our approach (DiffusionEAGS) combines the strengths of MLM and diffusionbased models to overcome the limitations of AR models, achieving a better diversity-quality tradeoff and finegrained controllability"
      },
      {
        "filename": "Conditional_MASK_Discrete_Diffusion_Language_Model__p3__score1.00.png",
        "path": "figs_human_eval_papers/Conditional_MASK_Discrete_Diffusion_Language_Model/Conditional_MASK_Discrete_Diffusion_Language_Model__p3__score1.00.png",
        "caption": "Figure 2: Overview of the training (forward) and inference (backward) processes in Diffusion-EAGS. Training (left): Entropy-based Noise Scheduling (ENS) determines which tokens in the masked sequence, denoted by [M ], should be denoised at each timestep based on the position entropy H(xi). These tokens are then generated using the diffusion model with parameters θ, and the loss is computed using a cross-entropy (C.E.) diffusion loss. Inference (right): Starting from a fully masked sequence conditioned on Y , Entropy-Adaptive Gibbs Sampling (EAGS) iteratively refines the sequence by focusing on high-entropy tokens, denoted as Mt, based on a threshold τt, yielding stable and coherent text generation."
      }
    ]
  },
  {
    "title": "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages",
    "folder_name": "Cross-Lingual_Retrieval_Augmented_Prompt_for_Low-Resource_Languages",
    "images": [
      {
        "filename": "Cross-Lingual_Retrieval_Augmented_Prompt_for_Low-Resource_Languages__p0__score1.00.png",
        "path": "figs_human_eval_papers/Cross-Lingual_Retrieval_Augmented_Prompt_for_Low-Resource_Languages/Cross-Lingual_Retrieval_Augmented_Prompt_for_Low-Resource_Languages__p0__score1.00.png",
        "caption": "Figure 1: Main idea of PARC: we enhance zero-shot learning for low-resource languages (LRLs) by crosslingual retrieval from labeled/unlabeled high-resource languages (HRLs). (a) An LRL input sample is taken as query by the cross-lingual retriever to retrieve the semantically most similar HRL sample from the HRL corpus. The label of the retrieved HRL sample is obtained either from the corpus (labeled setting) or by self-prediction (unlabeled setting). (b) The retrieved HRL sample together with its label and the input sample are reformulated as prompts. The cross-lingual retrievalaugmented prompt is created by concatenation and taken by the MPLM for prediction. Our experiments show that PARC outperforms other zero-shot methods and even finetuning."
      }
    ]
  },
  {
    "title": "DRAGIN Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models",
    "folder_name": "DRAGIN_Dynamic_Retrieval_Augmented_Generation_based_on_the_Information_Needs_of_Large_Language_Models",
    "images": [
      {
        "filename": "DRAGIN_Dynamic_Retrieval_Augmented_Generation_based_on_the_Information_Needs_of_Large_Language_Models__p2__score1.00.png",
        "path": "figs_human_eval_papers/DRAGIN_Dynamic_Retrieval_Augmented_Generation_based_on_the_Information_Needs_of_Large_Language_Models/DRAGIN_Dynamic_Retrieval_Augmented_Generation_based_on_the_Information_Needs_of_Large_Language_Models__p2__score1.00.png",
        "caption": "Figure 1: An illustration of our DRAGIN framework."
      },
      {
        "filename": "DRAGIN_Dynamic_Retrieval_Augmented_Generation_based_on_the_Information_Needs_of_Large_Language_Models__p5__score0.60.png",
        "path": "figs_human_eval_papers/DRAGIN_Dynamic_Retrieval_Augmented_Generation_based_on_the_Information_Needs_of_Large_Language_Models/DRAGIN_Dynamic_Retrieval_Augmented_Generation_based_on_the_Information_Needs_of_Large_Language_Models__p5__score0.60.png",
        "caption": "Table 1: A comparative overview of our selected Retrieval-Augmented Generation baselines."
      }
    ]
  },
  {
    "title": "DiffusionBERT Improving Generative Masked Language Models with Diffusion Models",
    "folder_name": "DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models",
    "images": [
      {
        "filename": "DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models__p0__score1.00.png",
        "path": "figs_human_eval_papers/DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models/DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models__p0__score1.00.png",
        "caption": "Figure 1: In contrast to conventional discrete diffusion models, DiffusionBERT uses BERT as its backbone to perform text generation. The main differences are highlighted in color: (1) DiffusionBERT performs decoding without knowing the current time step while canonical diffusion models are conditioned on time step. (2) The diffusion process of DiffusionBERT is non-Markovian in that it generates noise samples xt conditioning not only on xt−1 but also on x0. Such a non-Markov process is due to our proposed noise schedule."
      },
      {
        "filename": "DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models__p0__score1.00__1.png",
        "path": "figs_human_eval_papers/DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models/DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models__p0__score1.00__1.png",
        "caption": "Figure 1: In contrast to conventional discrete diffusion models, DiffusionBERT uses BERT as its backbone to perform text generation. The main differences are highlighted in color: (1) DiffusionBERT performs decoding without knowing the current time step while canonical diffusion models are conditioned on time step. (2) The diffusion process of DiffusionBERT is non-Markovian in that it generates noise samples xt conditioning not only on xt−1 but also on x0. Such a non-Markov process is due to our proposed noise schedule."
      },
      {
        "filename": "DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models__p0__score1.00__2.png",
        "path": "figs_human_eval_papers/DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models/DiffusionBERT_Improving_Generative_Masked_Language_Models_with_Diffusion_Models__p0__score1.00__2.png",
        "caption": "Figure 1: In contrast to conventional discrete diffusion models, DiffusionBERT uses BERT as its backbone to perform text generation. The main differences are highlighted in color: (1) DiffusionBERT performs decoding without knowing the current time step while canonical diffusion models are conditioned on time step. (2) The diffusion process of DiffusionBERT is non-Markovian in that it generates noise samples xt conditioning not only on xt−1 but also on x0. Such a non-Markov process is due to our proposed noise schedule."
      }
    ]
  },
  {
    "title": "Distilling Step-by-Step Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
    "folder_name": "Distilling_Step-by-Step_Outperforming_Larger_Language_Models_with_Less_Training_Data_and_Smaller_Model_Sizes",
    "images": [
      {
        "filename": "Distilling_Step-by-Step_Outperforming_Larger_Language_Models_with_Less_Training_Data_and_Smaller_Model_Sizes__p0__score0.60.png",
        "path": "figs_human_eval_papers/Distilling_Step-by-Step_Outperforming_Larger_Language_Models_with_Less_Training_Data_and_Smaller_Model_Sizes/Distilling_Step-by-Step_Outperforming_Larger_Language_Models_with_Less_Training_Data_and_Smaller_Model_Sizes__p0__score0.60.png",
        "caption": "Figure 1: While large language models (LLMs) offer strong zero/few-shot performance, they are challenging to serve in practice. Traditional ways of training small task-specific models, on the other hand, requires large amount of training data. We propose Distilling step-by-step, a new paradigm that extracts rationales from LLMs as informative task knowledge into training small models, which reduces both the deployed model size as well as the data required for training."
      },
      {
        "filename": "Distilling_Step-by-Step_Outperforming_Larger_Language_Models_with_Less_Training_Data_and_Smaller_Model_Sizes__p2__score0.95.png",
        "path": "figs_human_eval_papers/Distilling_Step-by-Step_Outperforming_Larger_Language_Models_with_Less_Training_Data_and_Smaller_Model_Sizes/Distilling_Step-by-Step_Outperforming_Larger_Language_Models_with_Less_Training_Data_and_Smaller_Model_Sizes__p2__score0.95.png",
        "caption": "Figure 2: Overview on Distilling step-by-step. We first utilize CoT prompting to extract rationales from an LLM (Section 3.1). We then use the generated rationales to train small task-specific models within a multi-task learning framework where we prepend task prefixes to the input examples and train the model to output differently based on the given task prefix (Section 3.2)."
      }
    ]
  },
  {
    "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning",
    "folder_name": "Do_Large_Language_Models_Latently_Perform_Multi-Hop_Reasoning",
    "images": [
      {
        "filename": "Do_Large_Language_Models_Latently_Perform_Multi-Hop_Reasoning__p0__score0.90.png",
        "path": "figs_human_eval_papers/Do_Large_Language_Models_Latently_Perform_Multi-Hop_Reasoning/Do_Large_Language_Models_Latently_Perform_Multi-Hop_Reasoning__p0__score0.90.png",
        "caption": "Figure 1: We investigate the latent multi-hop reasoning of LLMs. For the first hop, we change the input prompt to refer to the bridge entity (Stevie Wonder) and check how often it increases the model’s internal recall of the bridge entity. For the second hop, we check if increasing this recall causes the model output to be more consistent with respect to what it knows about the bridge entity’s attribute (mother of Stevie Wonder)."
      }
    ]
  },
  {
    "title": "DocFinQA A Long-Context Financial Reasoning Dataset",
    "folder_name": "DocFinQA_A_Long-Context_Financial_Reasoning_Dataset",
    "images": [
      {
        "filename": "DocFinQA_A_Long-Context_Financial_Reasoning_Dataset__p0__score0.80.png",
        "path": "figs_human_eval_papers/DocFinQA_A_Long-Context_Financial_Reasoning_Dataset/DocFinQA_A_Long-Context_Financial_Reasoning_Dataset__p0__score0.80.png",
        "caption": "Figure 1: DocFinQA extends FinQA to documents often over 150 pages long (100K+ tokens), so it is difficult to find the pertinent information. The question for the example above is: “For the quarter December 31, 2012 what was the percent of the total number of shares purchased in December?” The correct answer is 16.5%."
      }
    ]
  },
  {
    "title": "Don t Forget Your ABC s Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems",
    "folder_name": "Don_t_Forget_Your_ABC_s_Evaluating_the_State-of-the-Art_in_Chat-Oriented_Dialogue_Systems",
    "images": [
      {
        "filename": "Don_t_Forget_Your_ABC_s_Evaluating_the_State-of-the-Art_in_Chat-Oriented_Dialogue_Systems__p4__score0.70.png",
        "path": "figs_human_eval_papers/Don_t_Forget_Your_ABC_s_Evaluating_the_State-of-the-Art_in_Chat-Oriented_Dialogue_Systems/Don_t_Forget_Your_ABC_s_Evaluating_the_State-of-the-Art_in_Chat-Oriented_Dialogue_Systems__p4__score0.70.png",
        "caption": "Table 4: The 16 behavior labels within ABC-Eval. Row separators denote evaluation task groupings. Bold indicates behavior labels kept in final set. [1] Gopalakrishnan et al. (2019), [2] Higashinaka et al. (2021), [3] Mehri and Eskenazi (2020a), [4] Mehri and Eskenazi (2020b), [5] Phy et al. (2020), [6] Sanguinetti et al. (2020), [7] Beattie et al. (2022), [8] Sun et al. (2022), [9] Xu et al. (2021), [10] Rashkin et al. (2021), [11] Smith et al. (2020), [12] Majumder et al. (2020), [13] Rashkin et al. (2019), [14] Zhong et al. (2021), [15] Zhou et al. (2021), [16] Zhou et al. (2022), [17] Gupta et al. (2022), [18] Honovich et al. (2021), [19] Santhanam et al. (2021), [20] Shuster et al. (2021), [21] Li et al. (2021), [22] Nie et al. (2021), [23] Welleck et al. (2019), [24] Xu et al. (2022) ."
      }
    ]
  },
  {
    "title": "Error-driven Data-efficient Large Multimodal Model Tuning",
    "folder_name": "Error-driven_Data-efficient_Large_Multimodal_Model_Tuning",
    "images": [
      {
        "filename": "Error-driven_Data-efficient_Large_Multimodal_Model_Tuning__p2__score1.00.png",
        "path": "figs_human_eval_papers/Error-driven_Data-efficient_Large_Multimodal_Model_Tuning/Error-driven_Data-efficient_Large_Multimodal_Model_Tuning__p2__score1.00.png",
        "caption": "Figure 1: Overview of the error-driven data-efficient tuning paradigm."
      },
      {
        "filename": "Error-driven_Data-efficient_Large_Multimodal_Model_Tuning__p3__score0.95.png",
        "path": "figs_human_eval_papers/Error-driven_Data-efficient_Large_Multimodal_Model_Tuning/Error-driven_Data-efficient_Large_Multimodal_Model_Tuning__p3__score0.95.png",
        "caption": "Figure 2: Example for illustrating the process of mistake identification. At each iteration, we append one more reasoning step into the prompt to ask the teacher model to answer the question and track the probability changes of all the candidate option tokens."
      }
    ]
  },
  {
    "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
    "folder_name": "Establishing_Trustworthy_LLM_Evaluation_via_Shortcut_Neuron_Analysis",
    "images": [
      {
        "filename": "Establishing_Trustworthy_LLM_Evaluation_via_Shortcut_Neuron_Analysis__p0__score0.95.png",
        "path": "figs_human_eval_papers/Establishing_Trustworthy_LLM_Evaluation_via_Shortcut_Neuron_Analysis/Establishing_Trustworthy_LLM_Evaluation_via_Shortcut_Neuron_Analysis__p0__score0.95.png",
        "caption": "Figure 1: An example illustrating the core principle of our approach: we prevent the model from relying on shortcuts in contaminated regions to generate answers. This process restores the model’s true capabilities."
      },
      {
        "filename": "Establishing_Trustworthy_LLM_Evaluation_via_Shortcut_Neuron_Analysis__p3__score1.00.png",
        "path": "figs_human_eval_papers/Establishing_Trustworthy_LLM_Evaluation_via_Shortcut_Neuron_Analysis/Establishing_Trustworthy_LLM_Evaluation_via_Shortcut_Neuron_Analysis__p3__score1.00.png",
        "caption": "Figure 2: The overview of our method. We employ neuron analysis to identify regions within the model that may be overestimating its capabilities due to shortcuts. We calculate comparative and causal scores to find shortcut neurons. The former highlights the areas where there is the greatest divergence between parameters of contaminated and uncontaminated models. The latter is derived from neuron patching analysis to assess its causal impact. Subsequently, we use the located shortcut neurons to patch various models under test to obtain trustworthy evaluation results."
      }
    ]
  },
  {
    "title": "Exploring Precision and Recall to assess the quality and diversity of LLMs",
    "folder_name": "Exploring_Precision_and_Recall_to_assess_the_quality_and_diversity_of_LLMs",
    "images": [
      {
        "filename": "Exploring_Precision_and_Recall_to_assess_the_quality_and_diversity_of_LLMs__p3__score0.80.png",
        "path": "figs_human_eval_papers/Exploring_Precision_and_Recall_to_assess_the_quality_and_diversity_of_LLMs/Exploring_Precision_and_Recall_to_assess_the_quality_and_diversity_of_LLMs__p3__score0.80.png",
        "caption": "Figure 4: Precision and Recall for distribution-based metrics. (a) Distributions P and Q. (b) Precision is the proportion of the support of Q that generates P . (c) Recall is the proportion of the support P generated by Q."
      },
      {
        "filename": "Exploring_Precision_and_Recall_to_assess_the_quality_and_diversity_of_LLMs__p4__score1.00.png",
        "path": "figs_human_eval_papers/Exploring_Precision_and_Recall_to_assess_the_quality_and_diversity_of_LLMs/Exploring_Precision_and_Recall_to_assess_the_quality_and_diversity_of_LLMs__p4__score1.00.png",
        "caption": "Figure 5: Our pipeline to compute the Precision and Recall metrics. Texts are projected into a latent space of a pre-trained model, where a k-NN estimation is performed to estimate the relative overlaps of P and Q."
      }
    ]
  },
  {
    "title": "Express Uncertainty",
    "folder_name": "Express_Uncertainty",
    "images": [
      {
        "filename": "Express_Uncertainty__p0__score0.80.png",
        "path": "figs_human_eval_papers/Express_Uncertainty/Express_Uncertainty__p0__score0.80.png",
        "caption": "Figure 1: Overview of experiments on human interpretations of epistemic markers. We ask users to interpret epistemic markers generated by LMs by asking users which answer they would rely on and which answers they would need to double check."
      }
    ]
  },
  {
    "title": "FaST Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data",
    "folder_name": "FaST_Feature-aware_Sampling_and_Tuning_for_Personalized_Preference_Alignment_with_Limited_Data",
    "images": [
      {
        "filename": "FaST_Feature-aware_Sampling_and_Tuning_for_Personalized_Preference_Alignment_with_Limited_Data__p2__score1.00.png",
        "path": "figs_human_eval_papers/FaST_Feature-aware_Sampling_and_Tuning_for_Personalized_Preference_Alignment_with_Limited_Data/FaST_Feature-aware_Sampling_and_Tuning_for_Personalized_Preference_Alignment_with_Limited_Data__p2__score1.00.png",
        "caption": "Figure 1: Overview of the proposed FaST approach. The red-dashed box highlights the user-specific steps."
      }
    ]
  },
  {
    "title": "Finetuning LLMs for Human Behavior Prediction in Social Science Experiments",
    "folder_name": "Finetuning_LLMs_for_Human_Behavior_Prediction_in_Social_Science_Experiments",
    "images": [
      {
        "filename": "Finetuning_LLMs_for_Human_Behavior_Prediction_in_Social_Science_Experiments__p0__score1.00.png",
        "path": "figs_human_eval_papers/Finetuning_LLMs_for_Human_Behavior_Prediction_in_Social_Science_Experiments/Finetuning_LLMs_for_Human_Behavior_Prediction_in_Social_Science_Experiments__p0__score1.00.png",
        "caption": "Figure 1: We release SOCSCI210, a large-scale dataset built from open-source social science experiments. Through finetuning, we create behavioral prediction models SOCRATES-LLAMA-8B and SOCRATESQWEN-14B, which predict responses that are 12.1% and 13.2% respectively more aligned with human response distributions to outcomes under diverse experimental conditions, relative to GPT-4o."
      },
      {
        "filename": "Finetuning_LLMs_for_Human_Behavior_Prediction_in_Social_Science_Experiments__p3__score1.00.png",
        "path": "figs_human_eval_papers/Finetuning_LLMs_for_Human_Behavior_Prediction_in_Social_Science_Experiments/Finetuning_LLMs_for_Human_Behavior_Prediction_in_Social_Science_Experiments__p3__score1.00.png",
        "caption": "Figure 3: Overview of our task formulation, methods, and evaluation. Our dataset contains information on personas, conditions, outcomes, and predictions. We compare SFT, SFT on reasoning traces, and DPO. Our evaluation measures performance gains on both predicting individual accuracy and aggregate distributions under conditions."
      }
    ]
  },
  {
    "title": "Fooling the LVLM Judges Visual Biases in LVLM-Based Evaluation 3.5 4.1",
    "folder_name": "Fooling_the_LVLM_Judges_Visual_Biases_in_LVLM-Based_Evaluation_3.5_4.1",
    "images": [
      {
        "filename": "Fooling_the_LVLM_Judges_Visual_Biases_in_LVLM-Based_Evaluation_3.5_4.1__p0__score0.95.png",
        "path": "figs_human_eval_papers/Fooling_the_LVLM_Judges_Visual_Biases_in_LVLM-Based_Evaluation_3.5_4.1/Fooling_the_LVLM_Judges_Visual_Biases_in_LVLM-Based_Evaluation_3.5_4.1__p0__score0.95.png",
        "caption": "Figure 1: The LVLM judge is influenced by visual manipulations, resulting in an unfairly inflated evaluation score. Embedding the image generation instruction in the image (left) produces a manipulated image (right), leading to unfair assessment."
      },
      {
        "filename": "Fooling_the_LVLM_Judges_Visual_Biases_in_LVLM-Based_Evaluation_3.5_4.1__p2__score0.60.png",
        "path": "figs_human_eval_papers/Fooling_the_LVLM_Judges_Visual_Biases_in_LVLM-Based_Evaluation_3.5_4.1/Fooling_the_LVLM_Judges_Visual_Biases_in_LVLM-Based_Evaluation_3.5_4.1__p2__score0.60.png",
        "caption": "Table 1: Taxonomy of visual biases illustrated through comparisons between original and biased images."
      }
    ]
  },
  {
    "title": "Generating Diverse Hypotheses for Inductive Reasoning",
    "folder_name": "Generating_Diverse_Hypotheses_for_Inductive_Reasoning",
    "images": [
      {
        "filename": "Generating_Diverse_Hypotheses_for_Inductive_Reasoning__p1__score1.00.png",
        "path": "figs_human_eval_papers/Generating_Diverse_Hypotheses_for_Inductive_Reasoning/Generating_Diverse_Hypotheses_for_Inductive_Reasoning__p1__score1.00.png",
        "caption": "Figure 1: A motivation for MoC approach. IID sampling frequently generates redundant hypotheses (top). Increasing the temperature leads to frequent occurrences of text degeneration (middle). MoC allows for the generation of diverse hypotheses without a decline in hypothesis quality (bottom)."
      },
      {
        "filename": "Generating_Diverse_Hypotheses_for_Inductive_Reasoning__p3__score1.00.png",
        "path": "figs_human_eval_papers/Generating_Diverse_Hypotheses_for_Inductive_Reasoning/Generating_Diverse_Hypotheses_for_Inductive_Reasoning__p3__score1.00.png",
        "caption": "Figure 5: An overview of our Mixture of Concepts approach. We generate K distinct concepts (left) and feed them into the LLM separately for hypothesis generation (right)."
      },
      {
        "filename": "Generating_Diverse_Hypotheses_for_Inductive_Reasoning__p4__score0.95.png",
        "path": "figs_human_eval_papers/Generating_Diverse_Hypotheses_for_Inductive_Reasoning/Generating_Diverse_Hypotheses_for_Inductive_Reasoning__p4__score0.95.png",
        "caption": "Figure 6: Example problems in each of four datasets we study. We graphically display the MiniARC examples to help the reader understand."
      },
      {
        "filename": "Generating_Diverse_Hypotheses_for_Inductive_Reasoning__p7__score0.70.png",
        "path": "figs_human_eval_papers/Generating_Diverse_Hypotheses_for_Inductive_Reasoning/Generating_Diverse_Hypotheses_for_Inductive_Reasoning__p7__score0.70.png",
        "caption": "Table 6: Two challenging examples from MiniARC and MBPP+, where the IID baseline fails to generate a valid hypothesis from over 500 hypothesis samples. MoC solves these problems correctly with only 64 hypotheses. The concepts in boldface formulate the correct hypothesis."
      }
    ]
  },
  {
    "title": "How Do Moral Emotions Shape Political Participation A Cross-Cultural Analysis of Online Petitions Using Language Models",
    "folder_name": "How_Do_Moral_Emotions_Shape_Political_Participation_A_Cross-Cultural_Analysis_of_Online_Petitions_Using_Language_Models",
    "images": [
      {
        "filename": "How_Do_Moral_Emotions_Shape_Political_Participation_A_Cross-Cultural_Analysis_of_Online_Petitions_Using_Language_Models__p3__score1.00.png",
        "path": "figs_human_eval_papers/How_Do_Moral_Emotions_Shape_Political_Participation_A_Cross-Cultural_Analysis_of_Online_Petitions_Using_Language_Models/How_Do_Moral_Emotions_Shape_Political_Participation_A_Cross-Cultural_Analysis_of_Online_Petitions_Using_Language_Models__p3__score1.00.png",
        "caption": "Figure 1: Method overview. We propose a full framework for constructing data, modeling classification, and conducting analysis on the theme of moral emotion."
      }
    ]
  },
  {
    "title": "How Johnny Can Persuade LLMs to Jailbreak Them Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
    "folder_name": "How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs",
    "images": [
      {
        "filename": "How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p0__score0.95.png",
        "path": "figs_human_eval_papers/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p0__score0.95.png",
        "caption": "Figure 1: We propose a persuasion taxonomy with persuasion techniques, and apply it to automatically paraphrase plain harmful queries into human-readable persuasive adversarial prompts (PAPs). This method achieves an attack success rate of over 92% on Llama-2, GPT-3.5, and GPT-4 without specialized optimization."
      },
      {
        "filename": "How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p1__score1.00.png",
        "path": "figs_human_eval_papers/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p1__score1.00.png",
        "caption": "Figure 2: Comparison of previous adversarial prompts and PAP, ordered by three levels of humanizing. The first level treats LLMs as algorithmic systems: for instance, GCG (Zou et al., 2023) generates prompts with gibberish suffix via gradient synthesis; Deng et al. (2023b) exploits “side-channels” like low-resource languages. The second level progresses to treat LLMs as instruction followers: they usually rely on unconventional instruction patterns to jailbreak (e.g., virtualization or role-play), e.g., Yu et al. (2023) learn the distribution of virtualization-based jailbreak templates to produce jailbreak variants, while PAIR (Chao et al., 2023) asks LLMs to improve instructions as an “assistant” and often leads to prompts that employ virtualization or persona. We introduce the highest level to humanize and persuade LLMs as human-like communicators, and propose PAP. PAP seamlessly weaves persuasive techniques into jailbreak prompt construction, which highlights the risks associated with more complex and nuanced human-like communication to advance AI safety."
      },
      {
        "filename": "How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p2__score0.95.png",
        "path": "figs_human_eval_papers/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p2__score0.95.png",
        "caption": "Table 1: A systematic taxonomy of persuasion techniques. This table outlines 15 high-level persuasion strategies and 40 fine-grained persuasion techniques drawing from decades of social science research."
      },
      {
        "filename": "How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p3__score0.70.png",
        "path": "figs_human_eval_papers/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p3__score0.70.png",
        "caption": "Figure 5: Fine-tuning template with 3 main components."
      },
      {
        "filename": "How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p3__score1.00.png",
        "path": "figs_human_eval_papers/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p3__score1.00.png",
        "caption": "Figure 3: Overview of our taxonomy-guided scaled study. A. Persuasive Paraphraser Training: Step 1 gathers training data by paraphrasing harmful queries into PAPs. Step 2 fine-tunes a persuasive paraphraser with this data for stable paraphrasing. B. Persuasive Paraphraser Deployment: Step 1 leverages the persuasive paraphraser to generate PAPs from new harmful queries. Step 2 assesses the harmfulness of outputs from the target model."
      },
      {
        "filename": "How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p4__score0.80.png",
        "path": "figs_human_eval_papers/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs/How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs__p4__score0.80.png",
        "caption": "Figure 6: Qualitative example: a PAP using the “non-expert testimonial” technique to paraphrase a harmful query from risk category #8 (adult content). In the top, we see GPT-3.5’s guardrail blocks the original query. Meanwhile, at the bottom, the PAP elicits harmful content with links to real websites. We redact the sensitive information."
      }
    ]
  },
  {
    "title": "Humans or LLMs as the Judge A Study on Judgement Bias",
    "folder_name": "Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias",
    "images": [
      {
        "filename": "Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias__p3__score1.00.png",
        "path": "figs_human_eval_papers/Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias/Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias__p3__score1.00.png",
        "caption": "Figure 1: Sample demonstration. Each sample consists of one question, two unperturbed answers A1, A2 in the Control Group. The perturbed versions of A2 are generated for the Experimental Group. Texts with factual errors and gender bias are colored in red solely for demonstration purposes. Rich contents are rendered in the same way as"
      },
      {
        "filename": "Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias__p4__score1.00.png",
        "path": "figs_human_eval_papers/Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias/Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias__p4__score1.00.png",
        "caption": "Figure 2: Experiment Procedure. For each QA pair, we collect 6 votes with position shuffled. Voting results are tallied for a score, and converted into an answer preference (the shaded area in gray)."
      },
      {
        "filename": "Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias__p4__score1.00__1.png",
        "path": "figs_human_eval_papers/Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias/Humans_or_LLMs_as_the_Judge_A_Study_on_Judgement_Bias__p4__score1.00__1.png",
        "caption": "Figure 3: ASR calculation. We assess evaluators’ robustness against perturbations by calculating the percentage of samples with shifted preference between two groups."
      }
    ]
  },
  {
    "title": "IHEval Evaluating Language Models on Following the Instruction Hierarchy",
    "folder_name": "IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy",
    "images": [
      {
        "filename": "IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy__p1__score1.00.png",
        "path": "figs_human_eval_papers/IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy/IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy__p1__score1.00.png",
        "caption": "Figure 1: Four categories of the instruction hierarchy and the corresponding priority orders of instructions. Conflict instructions are shown in red. Models are expected to follow the instruction with the higher priority."
      },
      {
        "filename": "IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy__p3__score1.00.png",
        "path": "figs_human_eval_papers/IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy/IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy__p3__score1.00.png",
        "caption": "Figure 3: IHEval covers four categories and nine tasks. Detailed examples and instructions are in Figures 8~16."
      },
      {
        "filename": "IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy__p4__score0.80.png",
        "path": "figs_human_eval_papers/IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy/IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy__p4__score0.80.png",
        "caption": "Figure 4: The original data source, the evaluation metric, and the data size of each task."
      }
    ]
  },
  {
    "title": "ImageInWords Unlocking Hyper-Detailed Image Descriptions",
    "folder_name": "ImageInWords_Unlocking_Hyper-Detailed_Image_Descriptions",
    "images": [
      {
        "filename": "ImageInWords_Unlocking_Hyper-Detailed_Image_Descriptions__p1__score0.98.png",
        "path": "figs_human_eval_papers/ImageInWords_Unlocking_Hyper-Detailed_Image_Descriptions/ImageInWords_Unlocking_Hyper-Detailed_Image_Descriptions__p1__score0.98.png",
        "caption": "Figure 1: ImageInWords Seeded Annotation Framework. Humans enrich and refine outputs sequentially, building on prior human or machine inputs. Human annotation starts with fine-grained object captions in Task 1, which are used to compose image-level descriptions in Task 2. VLMs are updated in an active learning loop to produce better object and image-level seeds as annotated data becomes available. UI screenshots are in Appendix B.4."
      },
      {
        "filename": "ImageInWords_Unlocking_Hyper-Detailed_Image_Descriptions__p4__score0.90.png",
        "path": "figs_human_eval_papers/ImageInWords_Unlocking_Hyper-Detailed_Image_Descriptions/ImageInWords_Unlocking_Hyper-Detailed_Image_Descriptions__p4__score0.90.png",
        "caption": "Figure 3: IIW Annotation Tasks. Objects and their attributes are first individually annotated to note the salient objects and focus on coverage of their attributes in Task 1. These outputs, along with a seed VLM caption, are passed to humans to build the initial image-level description. The initial caption is then human augmented and refined in N sequential rounds to attain the final hyper-detailed description in Task 2."
      }
    ]
  },
  {
    "title": "Improve Vision Language Model Chain-of-thought Reasoning",
    "folder_name": "Improve_Vision_Language_Model_Chain-of-thought_Reasoning",
    "images": [
      {
        "filename": "Improve_Vision_Language_Model_Chain-of-thought_Reasoning__p1__score0.98.png",
        "path": "figs_human_eval_papers/Improve_Vision_Language_Model_Chain-of-thought_Reasoning/Improve_Vision_Language_Model_Chain-of-thought_Reasoning__p1__score0.98.png",
        "caption": "Figure 1: The upper figure questions whether training exclusively on direct-answer prediction can effectively teach CoT prediction. In the lower figure, we leverage short annotation as outcome reward for reasoning alignment, allowing the model to improve with self-generated data."
      },
      {
        "filename": "Improve_Vision_Language_Model_Chain-of-thought_Reasoning__p2__score0.70.png",
        "path": "figs_human_eval_papers/Improve_Vision_Language_Model_Chain-of-thought_Reasoning/Improve_Vision_Language_Model_Chain-of-thought_Reasoning__p2__score0.70.png",
        "caption": "Figure 2: Distillation of examples from various VLM task domains, highlighting the specific reasoning capabilities."
      },
      {
        "filename": "Improve_Vision_Language_Model_Chain-of-thought_Reasoning__p3__score0.95.png",
        "path": "figs_human_eval_papers/Improve_Vision_Language_Model_Chain-of-thought_Reasoning/Improve_Vision_Language_Model_Chain-of-thought_Reasoning__p3__score0.95.png",
        "caption": "Figure 4: The upper section displays the data sources used for the SFT experiments, while the lower section illustrates the data composition for model training."
      }
    ]
  },
  {
    "title": "In Prospect and Retrospect Re ective Memory Management for Long-term Personalized Dialogue Agents",
    "folder_name": "In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents",
    "images": [
      {
        "filename": "In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents__p0__score0.95.png",
        "path": "figs_human_eval_papers/In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents/In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents__p0__score0.95.png",
        "caption": "Figure 1: An illustration of a personalized healthcare dialog agent. Key information about a user’s allergy and previous symptoms mentioned in the past sessions is needed to provide a more informed response in the current session."
      },
      {
        "filename": "In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents__p3__score0.95.png",
        "path": "figs_human_eval_papers/In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents/In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents__p3__score0.95.png",
        "caption": "Figure 2: Illustration of Prospective Reflection. After each session, the agent decomposes and summarizes the session into specific topics. These newly generated memories are compared with existing memories in the"
      },
      {
        "filename": "In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents__p4__score1.00.png",
        "path": "figs_human_eval_papers/In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents/In_Prospect_and_Retrospect_Re_ective_Memory_Management_for_Long-term_Personalized_Dialogue_Agents__p4__score1.00.png",
        "caption": "Figure 3: Illustration of Retrospective Reflection. The Retriever fetches Top-K memory entries from the memory bank, which are refined by the learnable Reranker to select the Top-M most relevant entries. These entries are passed to the LLM along with the query to generate the final response. The LLM assigns binary citation scores (+1 for useful and −1 for not useful) to the retrieved memory entries based on their utility in the response. These scores are used as reward signals to update the reranker via an RL update, adapting the selection of relevant memory over time."
      }
    ]
  },
  {
    "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
    "folder_name": "Interpretable_Preferences_via_Multi-Objective_Reward_Modeling_and_Mixture-of-Experts",
    "images": [
      {
        "filename": "Interpretable_Preferences_via_Multi-Objective_Reward_Modeling_and_Mixture-of-Experts__p1__score1.00.png",
        "path": "figs_human_eval_papers/Interpretable_Preferences_via_Multi-Objective_Reward_Modeling_and_Mixture-of-Experts/Interpretable_Preferences_via_Multi-Objective_Reward_Modeling_and_Mixture-of-Experts__p1__score1.00.png",
        "caption": "Figure 1: Architecture of our reward model. It consists of an LLM backbone, a regression layer for multi-objective reward modeling, and a gating layer that outputs coefficients to scalarize the reward objectives into a scalar score."
      }
    ]
  },
  {
    "title": "Know When To Stop A Study of Semantic Drift in Text Generation",
    "folder_name": "Know_When_To_Stop_A_Study_of_Semantic_Drift_in_Text_Generation",
    "images": [
      {
        "filename": "Know_When_To_Stop_A_Study_of_Semantic_Drift_in_Text_Generation__p1__score0.70.png",
        "path": "figs_human_eval_papers/Know_When_To_Stop_A_Study_of_Semantic_Drift_in_Text_Generation/Know_When_To_Stop_A_Study_of_Semantic_Drift_in_Text_Generation__p1__score0.70.png",
        "caption": "Figure 1: A visual example of calculating semantic drift (SD) score for paragraph P . The position which best splits the paragraph is k = 8. The proportion of supported facts to the left is 0.88 and the proportion of not-supported facts to the right is 0.78, giving an average of 0.83. The other positions all have lower SD scores, therefore the SD score of paragraph P is 0.83."
      }
    ]
  },
  {
    "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
    "folder_name": "Knowledge_Unlearning_for_Mitigating_Privacy_Risks_in_Language_Models",
    "images": [
      {
        "filename": "Knowledge_Unlearning_for_Mitigating_Privacy_Risks_in_Language_Models__p1__score1.00.png",
        "path": "figs_human_eval_papers/Knowledge_Unlearning_for_Mitigating_Privacy_Risks_in_Language_Models/Knowledge_Unlearning_for_Mitigating_Privacy_Risks_in_Language_Models__p1__score1.00.png",
        "caption": "Figure 1: Comparison of previous approaches and knowledge unlearning when an individual practices his/her Right-To-Be-Forgotten (RTBF)."
      },
      {
        "filename": "Knowledge_Unlearning_for_Mitigating_Privacy_Risks_in_Language_Models__p1__score1.00__1.png",
        "path": "figs_human_eval_papers/Knowledge_Unlearning_for_Mitigating_Privacy_Risks_in_Language_Models/Knowledge_Unlearning_for_Mitigating_Privacy_Risks_in_Language_Models__p1__score1.00__1.png",
        "caption": "Figure 1: Comparison of previous approaches and knowledge unlearning when an individual practices his/her Right-To-Be-Forgotten (RTBF)."
      }
    ]
  },
  {
    "title": "Ko-LongRAG A Korean Long-Context RAG Benchmark Built with a Retrieval-Free Approach",
    "folder_name": "Ko-LongRAG_A_Korean_Long-Context_RAG_Benchmark_Built_with_a_Retrieval-Free_Approach",
    "images": [
      {
        "filename": "Ko-LongRAG_A_Korean_Long-Context_RAG_Benchmark_Built_with_a_Retrieval-Free_Approach__p1__score1.00.png",
        "path": "figs_human_eval_papers/Ko-LongRAG_A_Korean_Long-Context_RAG_Benchmark_Built_with_a_Retrieval-Free_Approach/Ko-LongRAG_A_Korean_Long-Context_RAG_Benchmark_Built_with_a_Retrieval-Free_Approach__p1__score1.00.png",
        "caption": "Figure 1: Ko-LongRAG Construction Pipeline. The C, Q, A triplet (long-Context passage, Question, Answer) is created through SCK-based document clustering, LLM-based QA generation, and manual quality assurance."
      },
      {
        "filename": "Ko-LongRAG_A_Korean_Long-Context_RAG_Benchmark_Built_with_a_Retrieval-Free_Approach__p6__score0.90.png",
        "path": "figs_human_eval_papers/Ko-LongRAG_A_Korean_Long-Context_RAG_Benchmark_Built_with_a_Retrieval-Free_Approach/Ko-LongRAG_A_Korean_Long-Context_RAG_Benchmark_Built_with_a_Retrieval-Free_Approach__p6__score0.90.png",
        "caption": "Figure 2: SCK Extraction Prompt."
      }
    ]
  },
  {
    "title": "LINC A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
    "folder_name": "LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers",
    "images": [
      {
        "filename": "LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers__p1__score1.00.png",
        "path": "figs_human_eval_papers/LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers/LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers__p1__score1.00.png",
        "caption": "Figure 1: This figure showcases the essence of our approach. Starting from a problem in natural language, in Step 1, the LLM semantic parser samples logic formulas expressing estimates of the semantics. It is possible that some of these might contain errors, e.g., the second example shows a syntax error involving an extra parenthesis, whereas the fourth example highlights a semantic error caused by mismatched predicates. In Step 2, these are then each offloaded to an automated theorem prover, filtering out syntax errors, and producing labels for the remaining samples. In Step 3, the remaining candidate outputs are passed through a majority-vote sieve to arrive at the best estimate for a single output label."
      },
      {
        "filename": "LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers__p1__score1.00__1.png",
        "path": "figs_human_eval_papers/LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers/LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers__p1__score1.00__1.png",
        "caption": "Figure 1: This figure showcases the essence of our approach. Starting from a problem in natural language, in Step 1, the LLM semantic parser samples logic formulas expressing estimates of the semantics. It is possible that some of these might contain errors, e.g., the second example shows a syntax error involving an extra parenthesis, whereas the fourth example highlights a semantic error caused by mismatched predicates. In Step 2, these are then each offloaded to an automated theorem prover, filtering out syntax errors, and producing labels for the remaining samples. In Step 3, the remaining candidate outputs are passed through a majority-vote sieve to arrive at the best estimate for a single output label."
      },
      {
        "filename": "LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers__p3__score0.98.png",
        "path": "figs_human_eval_papers/LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers/LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers__p3__score0.98.png",
        "caption": "Figure 2: This figure outlines the string concatenation workflow for each of our conditions. We start with the original problem, provide ICL examples through an intermediate markup language, and finally append the problem to evaluate. At this stage, we allow the model to autoregressively sample until producing a stop token."
      },
      {
        "filename": "LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers__p3__score1.00.png",
        "path": "figs_human_eval_papers/LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers/LINC_A_Neurosymbolic_Approach_for_Logical_Reasoning_by_Combining_Language_Models_with_First-Order_Logic_Provers__p3__score1.00.png",
        "caption": "Figure 2: This figure outlines the string concatenation workflow for each of our conditions. We start with the original problem, provide ICL examples through an intermediate markup language, and finally append the problem to evaluate. At this stage, we allow the model to autoregressively sample until producing a stop token."
      }
    ]
  },
  {
    "title": "LLMs Trust Humans More That s a Problem Unveiling and Mitigating the Authority Bias in Retrieval-Augmented Generation",
    "folder_name": "LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation",
    "images": [
      {
        "filename": "LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation__p1__score1.00.png",
        "path": "figs_human_eval_papers/LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation/LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation__p1__score1.00.png",
        "caption": "Figure 1: Illustration of Authority Bias in RAG systems. In simple queries, the LLM relies solely on database knowledge for the answer. However, in more complex scenarios with conflicting user-provided and database knowledge, the LLM tends to favor the user’s input, even if incorrect. We characterize this phenomenon of LLMs in RAG as Authority Bias."
      },
      {
        "filename": "LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation__p4__score1.00.png",
        "path": "figs_human_eval_papers/LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation/LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation__p4__score1.00.png",
        "caption": "Figure 2: A step-by-step illustration of constructing the ABDD."
      }
    ]
  },
  {
    "title": "Language Models as Inductive Reasoners",
    "folder_name": "Language_Models_as_Inductive_Reasoners",
    "images": [
      {
        "filename": "Language_Models_as_Inductive_Reasoners__p1__score0.70.png",
        "path": "figs_human_eval_papers/Language_Models_as_Inductive_Reasoners/Language_Models_as_Inductive_Reasoners__p1__score0.70.png",
        "caption": "Table 1: An example of inductive reasoning in DEER dataset. We embolden the words in facts that contain the key information to induce this rule (just to explain the relation between facts and rule, in DEER there’s no special word annotations for fact)."
      },
      {
        "filename": "Language_Models_as_Inductive_Reasoners__p4__score1.00.png",
        "path": "figs_human_eval_papers/Language_Models_as_Inductive_Reasoners/Language_Models_as_Inductive_Reasoners__p4__score1.00.png",
        "caption": "Figure 1: Our proposed framework (CoLM) for inductive reasoning with natural language representation task. Rule Proposer is a generative model based on input facts and desired rule template, aiming at generating (a large number of) rule candidates. Deductive consistency evaluator, indiscriminate confirmation handler, generalization checker, and triviality detector are classification models that filter improper rules according to four requirements of the induced rules in inductive reasoning. Texts with ✗ are representative filtered rules for each module."
      }
    ]
  },
  {
    "title": "Learning from Diverse Reasoning Paths with Routing and Collaboration",
    "folder_name": "Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration",
    "images": [
      {
        "filename": "Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration__p0__score0.95.png",
        "path": "figs_human_eval_papers/Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration/Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration__p0__score0.95.png",
        "caption": "Figure 1: Distillation effectiveness of teacher-generated reasoning paths are path-, task-, and student-dependent."
      },
      {
        "filename": "Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration__p2__score0.95.png",
        "path": "figs_human_eval_papers/Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration/Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration__p2__score0.95.png",
        "caption": "Figure 2: Prompt templates of different reasoning paths."
      },
      {
        "filename": "Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration__p3__score1.00.png",
        "path": "figs_human_eval_papers/Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration/Learning_from_Diverse_Reasoning_Paths_with_Routing_and_Collaboration__p3__score1.00.png",
        "caption": "Figure 3: Overview of our framework, including (1) Quality Filtering that drops flawed chains-of-thought; (2) Conditional Routing that sends each reasoning path to the most suitable students for fine-tuning; (3) MutualStudent Distillation that shares and refines learned insights of different students."
      }
    ]
  },
  {
    "title": "Less is More Mitigating Multimodal Hallucination from an EOS Decision Perspective",
    "folder_name": "Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective",
    "images": [
      {
        "filename": "Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective__p0__score0.60.png",
        "path": "figs_human_eval_papers/Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective/Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective__p0__score0.60.png",
        "caption": "Figure 1: Top: An example from the LLaVA instruction data. The training data can be overly detailed to exceed the model’s visual perception limits. Bottom: Average log-likelihood of the LLaVA (7b) model predicting EOS at positions labeled as EOS during instruction tuning. Training the model with overly detailed data leads to a decrease in its tendency to stop generation."
      },
      {
        "filename": "Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective__p4__score0.70.png",
        "path": "figs_human_eval_papers/Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective/Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective__p4__score0.70.png",
        "caption": "Figure 4: Illustration of the probability distribution derived from our proposed Selective EOS Supervision. Arrows indicate the maximizing and minimizing effects of the training objective on the probability of each word. When the label is not EOS, the EOS token is excluded from the probability distribution."
      }
    ]
  },
  {
    "title": "Locating and Extracting Relational Concepts in Large Language Models",
    "folder_name": "Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models",
    "images": [
      {
        "filename": "Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models__p0__score0.95.png",
        "path": "figs_human_eval_papers/Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models/Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models__p0__score0.95.png",
        "caption": "Figure 1: Our motivating observation in a fact recall process. At the last position, only hidden states in shallow layers solely express the relational causal effect, which provides us inspiration to treat these hidden states as relational representations."
      },
      {
        "filename": "Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models__p3__score1.00.png",
        "path": "figs_human_eval_papers/Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models/Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models__p3__score1.00.png",
        "caption": "Figure 4: The illustration of hidden States transplantation, which includes a sliding pointer to dynamically indicate the layer range."
      },
      {
        "filename": "Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models__p6__score0.97.png",
        "path": "figs_human_eval_papers/Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models/Locating_and_Extracting_Relational_Concepts_in_Large_Language_Models__p6__score0.97.png",
        "caption": "Figure 8: The illustration of our relation rewriting method, which can rewrite relational concepts contained in the input inquiries."
      }
    ]
  },
  {
    "title": "Long-text Uncertainty Quantification for LLMs",
    "folder_name": "Long-text_Uncertainty_Quantification_for_LLMs",
    "images": [
      {
        "filename": "Long-text_Uncertainty_Quantification_for_LLMs__p1__score1.00.png",
        "path": "figs_human_eval_papers/Long-text_Uncertainty_Quantification_for_LLMs/Long-text_Uncertainty_Quantification_for_LLMs__p1__score1.00.png",
        "caption": "Figure 1: The illustration of the LUQ and LUQ-ENSEMBLE framework. Given a question, various LLMs exhibit differing levels of uncertainty. We generate n sample responses from each LLM and then assess the uncertainty based on the diversity of these samples (the LUQ metric). Green highlights indicate consistency across responses (low uncertainty) and red highlights discrepancies (high uncertainty). The LUQ-ENSEMBLE method selects the response from the LLM with the lowest uncertainty score as the final answer."
      }
    ]
  },
  {
    "title": "Lost in the Middle How Language Models Use Long Contexts",
    "folder_name": "Lost_in_the_Middle_How_Language_Models_Use_Long_Contexts",
    "images": [
      {
        "filename": "Lost_in_the_Middle_How_Language_Models_Use_Long_Contexts__p3__score0.60.png",
        "path": "figs_human_eval_papers/Lost_in_the_Middle_How_Language_Models_Use_Long_Contexts/Lost_in_the_Middle_How_Language_Models_Use_Long_Contexts__p3__score0.60.png",
        "caption": "Figure 3: Modulating the position of relevant information within the input context for the multi-document question answering example presented in Figure 2. Re-ordering the documents in the input context does not affect the desired output."
      }
    ]
  },
  {
    "title": "MARVEL Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin",
    "folder_name": "MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin",
    "images": [
      {
        "filename": "MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin__p0__score1.00.png",
        "path": "figs_human_eval_papers/MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin/MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin__p0__score1.00.png",
        "caption": "Figure 1: Retrieval Pipeline with Our MARVEL Model. MARVEL incorporates the visual module plugin, aiming to unlock the multi-modal capabilities of well trained dense retrieval model."
      },
      {
        "filename": "MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin__p3__score1.00.png",
        "path": "figs_human_eval_papers/MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin/MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin__p3__score1.00.png",
        "caption": "Figure 2: The Architecture of Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL). We first pretrain the visual modules using the image-caption alignment task (Figure 2(a)) and then finetune the language model to conduct multi-modal retrieval (Figure 2(b))."
      }
    ]
  },
  {
    "title": "MASTER A Multi-Agent System with LLM Specialized MCTS",
    "folder_name": "MASTER_A_Multi-Agent_System_with_LLM_Specialized_MCTS",
    "images": [
      {
        "filename": "MASTER_A_Multi-Agent_System_with_LLM_Specialized_MCTS__p2__score1.00.png",
        "path": "figs_human_eval_papers/MASTER_A_Multi-Agent_System_with_LLM_Specialized_MCTS/MASTER_A_Multi-Agent_System_with_LLM_Specialized_MCTS__p2__score1.00.png",
        "caption": "Figure 1: Reasoning Tree of MASTER. Starting from Agent0, Agent1 and Agent2 are created in the first expansion. Then the system first selects Agent1 for expansion due to its higher UCT. Its child agent Agent3 is a terminal agent that failed evaluation which triggers a backpropagation and lowers the UCT of Agent1. Now Agent2 has the highest UCT and is selected for next expansion. Its child agent, Agent6 is a terminal agent and passes evaluation. The answer in it is the final answer."
      }
    ]
  },
  {
    "title": "Machine Unlearning of Pre-trained Large Language Models",
    "folder_name": "Machine_Unlearning_of_Pre-trained_Large_Language_Models",
    "images": [
      {
        "filename": "Machine_Unlearning_of_Pre-trained_Large_Language_Models__p1__score1.00.png",
        "path": "figs_human_eval_papers/Machine_Unlearning_of_Pre-trained_Large_Language_Models/Machine_Unlearning_of_Pre-trained_Large_Language_Models__p1__score1.00.png",
        "caption": "Figure 1: Overview of unlearning pre-trained LLMs to address user removal requests."
      }
    ]
  },
  {
    "title": "Make Every Penny Count Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
    "folder_name": "Make_Every_Penny_Count_Difficulty-Adaptive_Self-Consistency_for_Cost-Efficient_Reasoning",
    "images": [
      {
        "filename": "Make_Every_Penny_Count_Difficulty-Adaptive_Self-Consistency_for_Cost-Efficient_Reasoning__p1__score1.00.png",
        "path": "figs_human_eval_papers/Make_Every_Penny_Count_Difficulty-Adaptive_Self-Consistency_for_Cost-Efficient_Reasoning/Make_Every_Penny_Count_Difficulty-Adaptive_Self-Consistency_for_Cost-Efficient_Reasoning__p1__score1.00.png",
        "caption": "Figure 2: Leveraging their prior knowledge, humans assess the difficulty level of a problem before solving it, and allocate appropriate time for its resolution based on the difficulty."
      },
      {
        "filename": "Make_Every_Penny_Count_Difficulty-Adaptive_Self-Consistency_for_Cost-Efficient_Reasoning__p2__score1.00.png",
        "path": "figs_human_eval_papers/Make_Every_Penny_Count_Difficulty-Adaptive_Self-Consistency_for_Cost-Efficient_Reasoning/Make_Every_Penny_Count_Difficulty-Adaptive_Self-Consistency_for_Cost-Efficient_Reasoning__p2__score1.00.png",
        "caption": "Figure 3: Overall workflow of proposed Difficulty-Adaptive Self-Consistency. DSC first ranks problem difficulty using LLM itself (step 1), then partitions problems into easy and hard to save sampling cost for easy ones (step 2), and finally pre-allocates sample sizes to reduce resampling costs for hard problems (step 3)."
      }
    ]
  },
  {
    "title": "Making Long-Context Language Models Better Multi-Hop Reasoners",
    "folder_name": "Making_Long-Context_Language_Models_Better_Multi-Hop_Reasoners",
    "images": [
      {
        "filename": "Making_Long-Context_Language_Models_Better_Multi-Hop_Reasoners__p3__score1.00.png",
        "path": "figs_human_eval_papers/Making_Long-Context_Language_Models_Better_Multi-Hop_Reasoners/Making_Long-Context_Language_Models_Better_Multi-Hop_Reasoners__p3__score1.00.png",
        "caption": "Figure 1: Comparison of the proposed auxiliary tasks."
      }
    ]
  },
  {
    "title": "Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps",
    "folder_name": "Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps",
    "images": [
      {
        "filename": "Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps__p0__score0.95.png",
        "path": "figs_human_eval_papers/Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps/Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps__p0__score0.95.png",
        "caption": "Figure 1: An illustration of PFF and FUR. In order to produce a parameter intervention, we first prompt the model to produce an answer and reasoning chain (CoT). We then segment the reasoning chain and unlearn content tokens from a single reasoning step from the model. The unlearned model is then prompted to produce an answer. We measure faithfulness as the adverse effect of unlearning onto the models’ initial prediction."
      },
      {
        "filename": "Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps__p3__score1.00.png",
        "path": "figs_human_eval_papers/Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps/Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps__p3__score1.00.png",
        "caption": "Figure 2: A high level overview of the two stages of PFF: (1) parameter intervention and (2) evaluation. We instantiate PFF with FUR by using NPO+KL, controls to assure precision of unlearning and faithfulness metrics."
      },
      {
        "filename": "Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps__p7__score0.80.png",
        "path": "figs_human_eval_papers/Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps/Measuring_Chain_of_Thought_Faithfulness_by_Unlearning_Reasoning_Steps__p7__score0.80.png",
        "caption": "Figure 4: Heatmap produced by unlearning reasoning steps. ∆p denotes FF-SOFT: the change in initial answer probability. Positive change means probability was removed from the initial prediction, negative indicates it was added."
      }
    ]
  },
  {
    "title": "MemInsight Autonomous Memory Augmentation for LLM Agents",
    "folder_name": "MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents",
    "images": [
      {
        "filename": "MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents__p2__score1.00.png",
        "path": "figs_human_eval_papers/MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents/MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents__p2__score1.00.png",
        "caption": "Figure 1: MemInsight framework, consisting of three core modules: (1) Attribute Mining, which captures perspectives and levels of granularity; (2) Annotation, which performs attribute prioritization; and (3) Memory Retrieval, encompassing both refined and comprehensive retrieval. These modules are triggered by various downstream tasks, including answering questions, summarizing events, and conversational recommendation."
      },
      {
        "filename": "MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents__p3__score0.95.png",
        "path": "figs_human_eval_papers/MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents/MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents__p3__score0.95.png",
        "caption": "Figure 2: An example for Turn level and Session level annotations for a sample dialogue conversation from the LoCoMo Dataset."
      },
      {
        "filename": "MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents__p7__score1.00.png",
        "path": "figs_human_eval_papers/MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents/MemInsight_Autonomous_Memory_Augmentation_for_LLM_Agents__p7__score1.00.png",
        "caption": "Figure 3: Evaluation framework for event summarization with MemInsight, exploring augmentation at both turn and session levels, considering attributes alone or in combination with dialogues for richer summaries."
      }
    ]
  },
  {
    "title": "Memory OS of AI Agent",
    "folder_name": "Memory_OS_of_AI_Agent",
    "images": [
      {
        "filename": "Memory_OS_of_AI_Agent__p2__score1.00.png",
        "path": "figs_human_eval_papers/Memory_OS_of_AI_Agent/Memory_OS_of_AI_Agent__p2__score1.00.png",
        "caption": "Figure 1: The overview architecture of MemoryOS, including memory Store, Updating, Retrieval, Response."
      },
      {
        "filename": "Memory_OS_of_AI_Agent__p7__score0.90.png",
        "path": "figs_human_eval_papers/Memory_OS_of_AI_Agent/Memory_OS_of_AI_Agent__p7__score0.90.png",
        "caption": "Figure 4: Case study demonstrating the positive impact of introducing our memory management system. Left: default LLMs; Right: with MemoryOS."
      }
    ]
  },
  {
    "title": "Mind the Value-Action Gap Do LLMs Act in Alignment with Their Values",
    "folder_name": "Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values",
    "images": [
      {
        "filename": "Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values__p0__score0.95.png",
        "path": "figs_human_eval_papers/Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values/Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values__p0__score0.95.png",
        "caption": "Figure 1: An illustrative example of a “Value-Action Gap” in LLM. We observed a misalignment when prompting LLM to 1) state their inclination (i.e., Disagree) and 2) select their value-informed action (i.e., Agree), indicating 3) value-action gap towards the value of ‘Social Power’ in a scenario of Health in Nigeria."
      },
      {
        "filename": "Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values__p1__score1.00.png",
        "path": "figs_human_eval_papers/Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values/Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values__p1__score1.00.png",
        "caption": "Figure 2: We introduce the ValueActionLens framework to assess the alignment between LLMs’ stated values and their actions informed by those values. The framework encompasses (1) the data generation of value-informed actions across diverse cultural and social contexts; (2) two tasks for evaluating LLMs’ stated values (i.e., Task1) and value-informed actions (i.e., Task2); and (3) three measures to evaluate their value-action alignment, including value-action alignment rate, alignment distance, and alignment ranking."
      },
      {
        "filename": "Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values__p3__score1.00.png",
        "path": "figs_human_eval_papers/Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values/Mind_the_Value-Action_Gap_Do_LLMs_Act_in_Alignment_with_Their_Values__p3__score1.00.png",
        "caption": "Figure 3: The human-in-the-loop process of generating value-informed actions with three steps: (1) build prompt variants; (2) optimal prompt selection by AI experts; and (3) assessment of data quality by humans with diverse cultures. We show the optimal prompt and example of generated data format in Figure 6."
      }
    ]
  },
  {
    "title": "Mission Impossible Language Models",
    "folder_name": "Mission_Impossible_Language_Models",
    "images": [
      {
        "filename": "Mission_Impossible_Language_Models__p0__score0.95.png",
        "path": "figs_human_eval_papers/Mission_Impossible_Language_Models/Mission_Impossible_Language_Models__p0__score0.95.png",
        "caption": "Figure 1: Partial impossibility continuum of languages based on complexity. We assess the learnability of languages at different points in the continuum and push the (currently unclear) boundary between possible and impossible."
      },
      {
        "filename": "Mission_Impossible_Language_Models__p7__score0.95.png",
        "path": "figs_human_eval_papers/Mission_Impossible_Language_Models/Mission_Impossible_Language_Models__p7__score0.95.png",
        "caption": "Figure 4: An interchange intervention on the NOHOP model with base input b = The man be and source input s = The men be. The intervention is performed at the second layer and second token position, causing a"
      }
    ]
  },
  {
    "title": "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination",
    "folder_name": "Mitigating_Biases_for_Instruction-following_Language_Models_via_Bias_Neurons_Elimination",
    "images": [
      {
        "filename": "Mitigating_Biases_for_Instruction-following_Language_Models_via_Bias_Neurons_Elimination__p0__score0.95.png",
        "path": "figs_human_eval_papers/Mitigating_Biases_for_Instruction-following_Language_Models_via_Bias_Neurons_Elimination/Mitigating_Biases_for_Instruction-following_Language_Models_via_Bias_Neurons_Elimination__p0__score0.95.png",
        "caption": "Figure 1: The example contains an ambiguous context; thus, the correct answer is \"can’t answer\" since it is impossible to judge which group corresponds to the answer for a given negative question (e.g., \"What group of people uses drugs?\"). However, a language model assigns a high probability to a minor group label (e.g., \"poor people\"). Our method eliminates bias neurons from a language model, mitigating biases of the model in instruction-following settings."
      }
    ]
  },
  {
    "title": "Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding",
    "folder_name": "Mitigating_Hallucinations_in_Large_Vision-Language_Models_with_Instruction_Contrastive_Decoding",
    "images": [
      {
        "filename": "Mitigating_Hallucinations_in_Large_Vision-Language_Models_with_Instruction_Contrastive_Decoding__p3__score1.00.png",
        "path": "figs_human_eval_papers/Mitigating_Hallucinations_in_Large_Vision-Language_Models_with_Instruction_Contrastive_Decoding/Mitigating_Hallucinations_in_Large_Vision-Language_Models_with_Instruction_Contrastive_Decoding__p3__score1.00.png",
        "caption": "Figure 1: An illustration on inference framework and contrastive decoding process of ICD method. At the core (middle orange box), the framework integrates a frozen image encoder, LLM, and query vectors (gray box) within the Q-Former, focusing solely on adjusting the standard and disturbance instructions. The latter, exemplified by adding role prefixes like ‘You are a confused object detector,’ aims to increase multimodal alignment uncertainty. This results in two distinct distributions: one from the standard instruction and another influenced by the disturbance. The contrastive decoding method (right orange box) highlights how disturbance instructions amplify hallucinated concepts (‘person and fork’), which are then corrected by subtracting probabilities derived from the standard instruction, ensuring accurate recognition of the correct concept ‘dog’."
      }
    ]
  },
  {
    "title": "Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression",
    "folder_name": "Mitigating_Hallucinations_in_Vision-Language_Models_through_Image-Guided_Head_Suppression",
    "images": [
      {
        "filename": "Mitigating_Hallucinations_in_Vision-Language_Models_through_Image-Guided_Head_Suppression__p0__score0.95.png",
        "path": "figs_human_eval_papers/Mitigating_Hallucinations_in_Vision-Language_Models_through_Image-Guided_Head_Suppression/Mitigating_Hallucinations_in_Vision-Language_Models_through_Image-Guided_Head_Suppression__p0__score0.95.png",
        "caption": "Figure 1: Caption generation using LLaVA-1.5 and SPIN. LLaVA-1.5’s generated text description mentions a “chair” in the background, which is clearly a hallucinated object. SPIN mitigates hallucination while successfully identifying the objects present in the image."
      }
    ]
  },
  {
    "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning",
    "folder_name": "Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning",
    "images": [
      {
        "filename": "Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning__p2__score0.90.png",
        "path": "figs_human_eval_papers/Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning/Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning__p2__score0.90.png",
        "caption": "Figure 2: Illustration of layer-level and token-level attention weights. (a) The layer-level attention weights of image tokens across different response token positions. (b) The token-level attention weights at the middle layer. It shows that the model’s attention to the image gradually decreases during the reasoning process."
      },
      {
        "filename": "Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning__p3__score1.00.png",
        "path": "figs_human_eval_papers/Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning/Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning__p3__score1.00.png",
        "caption": "Figure 3: Overview of TVC System Design. We enable the model to have take-along visual conditioning capabilities through two stages: training and inference."
      },
      {
        "filename": "Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning__p4__score1.00.png",
        "path": "figs_human_eval_papers/Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning/Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning__p4__score1.00.png",
        "caption": "Figure 4: Data Generation Pipeline of TVC. We use iterative distillation to collect long-chain reasoning data, followed by a comprehensive response filtering process to ensure high-quality reasoning."
      },
      {
        "filename": "Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning__p7__score0.93.png",
        "path": "figs_human_eval_papers/Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning/Mitigating_Visual_Forgetting_via_Take-along_Visual_Conditioning_for_Multi-modal_Long_CoT_Reasoning__p7__score0.93.png",
        "caption": "Figure 6: Case Study of TVC. TVC effectively re-examines the image during the reflection process to correct mistakes, guiding the model to the correct answer."
      }
    ]
  },
  {
    "title": "Model Editing by Standard Fine-Tuning",
    "folder_name": "Model_Editing_by_Standard_Fine-Tuning",
    "images": [
      {
        "filename": "Model_Editing_by_Standard_Fine-Tuning__p6__score0.90.png",
        "path": "figs_human_eval_papers/Model_Editing_by_Standard_Fine-Tuning/Model_Editing_by_Standard_Fine-Tuning__p6__score0.90.png",
        "caption": "Figure 2: The few-shot prompt we use for paraphrase generation. We selected in-context examples from COUNTERFACT."
      }
    ]
  },
  {
    "title": "Models Fine-grained Gender Control in Machine Translation with Large Language",
    "folder_name": "Models_Fine-grained_Gender_Control_in_Machine_Translation_with_Large_Language",
    "images": [
      {
        "filename": "Models_Fine-grained_Gender_Control_in_Machine_Translation_with_Large_Language__p1__score0.70.png",
        "path": "figs_human_eval_papers/Models_Fine-grained_Gender_Control_in_Machine_Translation_with_Large_Language/Models_Fine-grained_Gender_Control_in_Machine_Translation_with_Large_Language__p1__score0.70.png",
        "caption": "Figure 1: Four gender control scenarios in machine translation investigated in our study."
      },
      {
        "filename": "Models_Fine-grained_Gender_Control_in_Machine_Translation_with_Large_Language__p2__score0.80.png",
        "path": "figs_human_eval_papers/Models_Fine-grained_Gender_Control_in_Machine_Translation_with_Large_Language/Models_Fine-grained_Gender_Control_in_Machine_Translation_with_Large_Language__p2__score0.80.png",
        "caption": "Table 1: Instruction template for our proposed Genderof-Entity (GoE) prompting."
      }
    ]
  },
  {
    "title": "OVM utcome-supervised alue odels for Planning in Mathematical Reasoning",
    "folder_name": "OVM_utcome-supervised_alue_odels_for_Planning_in_Mathematical_Reasoning",
    "images": [
      {
        "filename": "OVM_utcome-supervised_alue_odels_for_Planning_in_Mathematical_Reasoning__p2__score1.00.png",
        "path": "figs_human_eval_papers/OVM_utcome-supervised_alue_odels_for_Planning_in_Mathematical_Reasoning/OVM_utcome-supervised_alue_odels_for_Planning_in_Mathematical_Reasoning__p2__score1.00.png",
        "caption": "Figure 1: (a): When evaluating partial paths (here for the first two steps), reward focuses on the current states, while value focuses on the unseen future outcomes. (b): Given a question q and a solution path [s1, · · · , sm, a], models are trained to predict path correctness (circled output scalar on the last token). Outcome supervision replicates the final answer’s correctness label across all steps (indicated by shaded labels), causing the model to implicitly learn to foresee the future, predicting values for partial paths. By contrast, process supervision details per-step correctness labels, causing the model to learn to predict step-level correctness, i.e. reward. Correct steps and answers are colored in yellow and incorrect ones in grey."
      }
    ]
  },
  {
    "title": "On LLM-Based Scientific Inductive Reasoning Beyond Equations",
    "folder_name": "On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations",
    "images": [
      {
        "filename": "On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations__p0__score1.00.png",
        "path": "figs_human_eval_papers/On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations/On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations__p0__score1.00.png",
        "caption": "Figure 1: Illustrative comparison of scientific inductive reasoning: on the left, tasks focused on equation discovery (Shojaee et al., 2025), and on the right, tasks representing broader forms of scientific induction beyond equation generation."
      },
      {
        "filename": "On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations__p4__score1.00.png",
        "path": "figs_human_eval_papers/On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations/On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations__p4__score1.00.png",
        "caption": "Figure 2: Our benchmark includes 7 tasks spanning two scientific disciplines: biology and chemistry. denotes"
      },
      {
        "filename": "On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations__p6__score1.00.png",
        "path": "figs_human_eval_papers/On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations/On_LLM-Based_Scientific_Inductive_Reasoning_Beyond_Equations__p6__score1.00.png",
        "caption": "Figure 3: Comparison of four inference strategies: (1) Implicit induction - directly providing output; (2) Explicit induction - formulating clear hypotheses explicitly; (3) Self-consistency - using multiple reasoning paths to reach consensus; and (4) Hypothesis refinement - iteratively improving hypothesis on feedback."
      }
    ]
  },
  {
    "title": "Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models",
    "folder_name": "Performance_Gap_in_Entity_Knowledge_Extraction_Across_Modalities_in_Vision_Language_Models",
    "images": [
      {
        "filename": "Performance_Gap_in_Entity_Knowledge_Extraction_Across_Modalities_in_Vision_Language_Models__p1__score1.00.png",
        "path": "figs_human_eval_papers/Performance_Gap_in_Entity_Knowledge_Extraction_Across_Modalities_in_Vision_Language_Models/Performance_Gap_in_Entity_Knowledge_Extraction_Across_Modalities_in_Vision_Language_Models__p1__score1.00.png",
        "caption": "Figure 1: Illustration of our motivating observation. The model successfully extracts its knowledge about the entity when the entity is represented in text. It further successfully identifies the entity in the image, but it fails to combine the two."
      },
      {
        "filename": "Performance_Gap_in_Entity_Knowledge_Extraction_Across_Modalities_in_Vision_Language_Models__p2__score1.00.png",
        "path": "figs_human_eval_papers/Performance_Gap_in_Entity_Knowledge_Extraction_Across_Modalities_in_Vision_Language_Models/Performance_Gap_in_Entity_Knowledge_Extraction_Across_Modalities_in_Vision_Language_Models__p2__score1.00.png",
        "caption": "Figure 2: The left pane is an illustration for the experiment in Section 5. The hidden states at the positions of the visual tokens are copied from the forward pass of the injected entity at the source layer (in this case layer 1) replacing the hidden states of the original entity’s forward pass at the same layer and position. The right pane is an illustration of the experiment in Section 6. Starting from the source layer, the hidden states at the positions of the visual tokens are copied and patched instead of the hidden states at the same positions in the subsequent layers until the target layer, effectively \"freezing\" their processing by the LLM."
      }
    ]
  },
  {
    "title": "PopAlign Diversifying Contrasting Patterns for a More Comprehensive Alignment",
    "folder_name": "PopAlign_Diversifying_Contrasting_Patterns_for_a_More_Comprehensive_Alignment",
    "images": [
      {
        "filename": "PopAlign_Diversifying_Contrasting_Patterns_for_a_More_Comprehensive_Alignment__p0__score0.90.png",
        "path": "figs_human_eval_papers/PopAlign_Diversifying_Contrasting_Patterns_for_a_More_Comprehensive_Alignment/PopAlign_Diversifying_Contrasting_Patterns_for_a_More_Comprehensive_Alignment__p0__score0.90.png",
        "caption": "Figure 1: Illustration of the effects of alignment considering the contrasting patterns. πi ref denotes the distribution of the reference model under pattern i. πdpoi denotes the overall distribution of the model after DPO alignment on pattern i."
      },
      {
        "filename": "PopAlign_Diversifying_Contrasting_Patterns_for_a_More_Comprehensive_Alignment__p2__score1.00.png",
        "path": "figs_human_eval_papers/PopAlign_Diversifying_Contrasting_Patterns_for_a_More_Comprehensive_Alignment/PopAlign_Diversifying_Contrasting_Patterns_for_a_More_Comprehensive_Alignment__p2__score1.00.png",
        "caption": "Figure 2: The workflow of PopAlign. PopAlign involves three kinds of contrasting strategies: (1) Prompt Contrast such as Prefix Contrast, Demon Contrast (i.e., Demonstration Contrast, and Elicitive Contrast), (2) Model Contrast such as NParam (number of parameters) Contrast and Leaderboard Contrast, as well as (3) Pipeline Contrast such as Refinement Contrast. By mixing the preference data synthesized with diverse contrasting strategies and conducting DPO alignment training on it, we can easily align the LLM without either human annotation or reward labeling."
      }
    ]
  },
  {
    "title": "Progressive Multimodal Reasoning via Active Retrieval",
    "folder_name": "Progressive_Multimodal_Reasoning_via_Active_Retrieval",
    "images": [
      {
        "filename": "Progressive_Multimodal_Reasoning_via_Active_Retrieval__p2__score0.95.png",
        "path": "figs_human_eval_papers/Progressive_Multimodal_Reasoning_via_Active_Retrieval/Progressive_Multimodal_Reasoning_via_Active_Retrieval__p2__score0.95.png",
        "caption": "Figure 1: The statistics of our hybrid retrieval corpus."
      },
      {
        "filename": "Progressive_Multimodal_Reasoning_via_Active_Retrieval__p3__score1.00.png",
        "path": "figs_human_eval_papers/Progressive_Multimodal_Reasoning_via_Active_Retrieval/Progressive_Multimodal_Reasoning_via_Active_Retrieval__p3__score1.00.png",
        "caption": "Figure 2: The pipeline of our unified multimodal retrieval module."
      },
      {
        "filename": "Progressive_Multimodal_Reasoning_via_Active_Retrieval__p5__score1.00.png",
        "path": "figs_human_eval_papers/Progressive_Multimodal_Reasoning_via_Active_Retrieval/Progressive_Multimodal_Reasoning_via_Active_Retrieval__p5__score1.00.png",
        "caption": "Figure 3: Our AR-MCTS: The retrieval module actively retrieves key insights at each step of the MCTS process"
      }
    ]
  },
  {
    "title": "R-VLM Region-Aware Vision Language Model for Precise GUI Grounding",
    "folder_name": "R-VLM_Region-Aware_Vision_Language_Model_for_Precise_GUI_Grounding",
    "images": [
      {
        "filename": "R-VLM_Region-Aware_Vision_Language_Model_for_Precise_GUI_Grounding__p1__score1.00.png",
        "path": "figs_human_eval_papers/R-VLM_Region-Aware_Vision_Language_Model_for_Precise_GUI_Grounding/R-VLM_Region-Aware_Vision_Language_Model_for_Precise_GUI_Grounding__p1__score1.00.png",
        "caption": "Figure 1: Illustration of the Region-Aware Vision Language Model (R-VLM). Our approach consists of two modules for precise GUI grounding: (a) A two-stage zoom-in grounding process that refines predictions via a zoomed-in view of region proposal. After obtaining an initial prediction from the model using GUI screenshot and user instruction, which serves as a region proposal, we zoom-in around this region and pass it through the model again for second-stage grounding. (b) An IoU-aware weighted cross-entropy loss that provides a smooth learning signal based on the IoU value rather than strictly fitting to ground-truth bounding box. This loss assigns weights to pseudo bounding boxes according to their IoU value with ground-truth to emphasize high IoU grounding predictions."
      },
      {
        "filename": "R-VLM_Region-Aware_Vision_Language_Model_for_Precise_GUI_Grounding__p4__score1.00.png",
        "path": "figs_human_eval_papers/R-VLM_Region-Aware_Vision_Language_Model_for_Precise_GUI_Grounding/R-VLM_Region-Aware_Vision_Language_Model_for_Precise_GUI_Grounding__p4__score1.00.png",
        "caption": "Figure 4: Efficient IoU-aware weighted cross-entropy computation. (a) M pseudo boxes are generated from the ground-truth box using a GIoU threshold (here, M=3), and concatenated with the original label, enabling a single forward pass for M+1 predictions. (b) The attention map is masked to prevent pseudo boxes from attending to each other. (c) The positional embedding of the ground-truth box is assigned to all pseudo boxes, ensuring a single prediction at inference. Crossentropy is weighted by GIoU relative to ground-truth."
      }
    ]
  },
  {
    "title": "RAG-Instruct Boosting LLMs with Diverse Retrieval-Augmented Instructions",
    "folder_name": "RAG-Instruct_Boosting_LLMs_with_Diverse_Retrieval-Augmented_Instructions",
    "images": [
      {
        "filename": "RAG-Instruct_Boosting_LLMs_with_Diverse_Retrieval-Augmented_Instructions__p3__score0.95.png",
        "path": "figs_human_eval_papers/RAG-Instruct_Boosting_LLMs_with_Diverse_Retrieval-Augmented_Instructions/RAG-Instruct_Boosting_LLMs_with_Diverse_Retrieval-Augmented_Instructions__p3__score0.95.png",
        "caption": "Figure 1: The process of synthesizing data with RAG-Instruct involves ensuring instruction data diversity through five RAG paradigms and Instruction Simulation. The visualization of the question topic is generated using Atlas."
      },
      {
        "filename": "RAG-Instruct_Boosting_LLMs_with_Diverse_Retrieval-Augmented_Instructions__p4__score0.95.png",
        "path": "figs_human_eval_papers/RAG-Instruct_Boosting_LLMs_with_Diverse_Retrieval-Augmented_Instructions/RAG-Instruct_Boosting_LLMs_with_Diverse_Retrieval-Augmented_Instructions__p4__score0.95.png",
        "caption": "Figure 3: The prompt of RAG-Instruct. <document> and <Simulated Instruction> represent input variables for the document and simulated instruction, respectively. (Blue text) indicates RAG Paradigms, illustrating the prompt for r4; other paradigms are shown in Appendix C.2. (Red text) represents Instruction Simulation."
      }
    ]
  },
  {
    "title": "RAG Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models",
    "folder_name": "RAG_Overcoming_Imperfect_Retrieval_Augmentation_and_Knowledge_Conflicts_for_Large_Language_Models",
    "images": [
      {
        "filename": "RAG_Overcoming_Imperfect_Retrieval_Augmentation_and_Knowledge_Conflicts_for_Large_Language_Models__p1__score0.95.png",
        "path": "figs_human_eval_papers/RAG_Overcoming_Imperfect_Retrieval_Augmentation_and_Knowledge_Conflicts_for_Large_Language_Models/RAG_Overcoming_Imperfect_Retrieval_Augmentation_and_Knowledge_Conflicts_for_Large_Language_Models__p1__score0.95.png",
        "caption": "Figure 1: Knowledge conflicts between the LLMs’ internal knowledge and retrieved knowledge from external sources. We report the overall results with Claude under the setting in Sec. 5.1."
      },
      {
        "filename": "RAG_Overcoming_Imperfect_Retrieval_Augmentation_and_Knowledge_Conflicts_for_Large_Language_Models__p3__score1.00.png",
        "path": "figs_human_eval_papers/RAG_Overcoming_Imperfect_Retrieval_Augmentation_and_Knowledge_Conflicts_for_Large_Language_Models/RAG_Overcoming_Imperfect_Retrieval_Augmentation_and_Knowledge_Conflicts_for_Large_Language_Models__p3__score1.00.png",
        "caption": "Figure 4: Overview of the ASTUTE RAG framework. ASTUTE RAG is designed to better combine the information from the external sources (e.g. web, domain-specific corpora) and internal knowledge of the LLMs by employing a consolidation mechanism to address the conflicts, which eventually leads to superior generation quality."
      }
    ]
  },
  {
    "title": "Retrieval-Augmented Black-Box Language Models",
    "folder_name": "Retrieval-Augmented_Black-Box_Language_Models",
    "images": [
      {
        "filename": "Retrieval-Augmented_Black-Box_Language_Models__p0__score1.00.png",
        "path": "figs_human_eval_papers/Retrieval-Augmented_Black-Box_Language_Models/Retrieval-Augmented_Black-Box_Language_Models__p0__score1.00.png",
        "caption": "Figure 1: Different from previous retrieval-augmented approaches (Borgeaud et al., 2022) that enhance a language model with retrieval by updating the LM’s parameters, REPLUG treats the LM as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs, which are often served via APIs."
      },
      {
        "filename": "Retrieval-Augmented_Black-Box_Language_Models__p2__score1.00.png",
        "path": "figs_human_eval_papers/Retrieval-Augmented_Black-Box_Language_Models/Retrieval-Augmented_Black-Box_Language_Models__p2__score1.00.png",
        "caption": "Figure 2: REPLUG at inference (§3). Given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1 Document Retrieval). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation)."
      },
      {
        "filename": "Retrieval-Augmented_Black-Box_Language_Models__p4__score1.00.png",
        "path": "figs_human_eval_papers/Retrieval-Augmented_Black-Box_Language_Models/Retrieval-Augmented_Black-Box_Language_Models__p4__score1.00.png",
        "caption": "Figure 3: REPLUG LSR training process (§4). The retriever is trained using the output of a frozen language model as supervision signals."
      }
    ]
  },
  {
    "title": "Reverse Thinking Makes LLMs Stronger Reasoners",
    "folder_name": "Reverse_Thinking_Makes_LLMs_Stronger_Reasoners",
    "images": [
      {
        "filename": "Reverse_Thinking_Makes_LLMs_Stronger_Reasoners__p0__score0.98.png",
        "path": "figs_human_eval_papers/Reverse_Thinking_Makes_LLMs_Stronger_Reasoners/Reverse_Thinking_Makes_LLMs_Stronger_Reasoners__p0__score0.98.png",
        "caption": "Figure 1: Comparison between symbolic knowledge distillation (SKD) and our method. (1) the teacher model generates multiple reasoning chains for a given question, (2) SKD supervised fine-tunes on the correct reasoning chains, and (3) our method incorporates bidirectional reasoning, learning from both Q-to-A and Ato-Q using our multi-task objectives."
      },
      {
        "filename": "Reverse_Thinking_Makes_LLMs_Stronger_Reasoners__p3__score1.00.png",
        "path": "figs_human_eval_papers/Reverse_Thinking_Makes_LLMs_Stronger_Reasoners/Reverse_Thinking_Makes_LLMs_Stronger_Reasoners__p3__score1.00.png",
        "caption": "Figure 2: REVTHINK consists of two stages: (1) Data augmentation and (2) Student model learning. First, given a dataset D = {(Q(i), A(i))}ni=1, we augment it by prompting the teacher model to generate forward reasoning, backward question, and backward reasoning. We keep instances only with correct forward reasoning (validated by the ground truth) and consistent forward-backward reasoning (validated by the teacher model). This yields an augmented dataset Daug = (Q(i), R (i) f , Q (i) b , R (i) b )ni=1. Next, we train the student model with three objectives: Q → Rf , Q → Qb and Qb → Rb, enabling the student to reason in both directions during training. At test time, the student model performs only forward reasoning, making test-time compute as efficient as zero-shot prompting."
      }
    ]
  },
  {
    "title": "Rewarding the Unlikely Lifting GRPO Beyond Distribution Sharpening",
    "folder_name": "Rewarding_the_Unlikely_Lifting_GRPO_Beyond_Distribution_Sharpening",
    "images": [
      {
        "filename": "Rewarding_the_Unlikely_Lifting_GRPO_Beyond_Distribution_Sharpening__p0__score0.90.png",
        "path": "figs_human_eval_papers/Rewarding_the_Unlikely_Lifting_GRPO_Beyond_Distribution_Sharpening/Rewarding_the_Unlikely_Lifting_GRPO_Beyond_Distribution_Sharpening__p0__score0.90.png",
        "caption": "Figure 1: We identify a rank bias in GRPO in which model updates only reinforce already probable solutions and fail to surface new ones. This sharpens the distribution and impairs pass@N performance for large N. Our unlikeliness reward addresses rank bias by explicitly encouraging uplifting low-probability correct solutions."
      }
    ]
  },
  {
    "title": "SafeDecoding Defending against Jailbreak Attacks via Safety-Aware Decoding",
    "folder_name": "SafeDecoding_Defending_against_Jailbreak_Attacks_via_Safety-Aware_Decoding",
    "images": [
      {
        "filename": "SafeDecoding_Defending_against_Jailbreak_Attacks_via_Safety-Aware_Decoding__p0__score0.95.png",
        "path": "figs_human_eval_papers/SafeDecoding_Defending_against_Jailbreak_Attacks_via_Safety-Aware_Decoding/SafeDecoding_Defending_against_Jailbreak_Attacks_via_Safety-Aware_Decoding__p0__score0.95.png",
        "caption": "Figure 1: This example illustrates the token probabilities of Vicuna-7B model under GCG attack (Zou et al., 2023). The words in red are GCG suffixes. We note that although the token representing the word \"Sure\" has a dominant probability, safety disclaimers such as \"I\", \"Sorry\", and \"As\" are still present in the sample space, which is sorted in descending order in token probabilities. When a safety disclaimer token is sampled, the model would reject the attacker’s harmful query."
      },
      {
        "filename": "SafeDecoding_Defending_against_Jailbreak_Attacks_via_Safety-Aware_Decoding__p4__score1.00.png",
        "path": "figs_human_eval_papers/SafeDecoding_Defending_against_Jailbreak_Attacks_via_Safety-Aware_Decoding/SafeDecoding_Defending_against_Jailbreak_Attacks_via_Safety-Aware_Decoding__p4__score1.00.png",
        "caption": "Figure 2: This figure illustrates the detail of SafeDecoding. During the training phase, we fine-tune the original LLM to construct an expert model with strengthened safety. In the inference phase, a user query is passed to both the original and expert models. Based on their outputs, SafeDecoding constructs a new token probability distribution. This constructed probability distribution attenuates the probabilities of tokens that are aligned with the attacker’s goal, and amplifies the probabilities of tokens that are aligned with human values. In this example, SafeDecoding is applied only to the first 2 tokens, while the remaining tokens are generated through normal decoding."
      }
    ]
  },
  {
    "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models",
    "folder_name": "Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models",
    "images": [
      {
        "filename": "Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models__p0__score0.95.png",
        "path": "figs_human_eval_papers/Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models/Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models__p0__score0.95.png",
        "caption": "Figure 1: Comparison between two responses given by InstructGPT. The retrieved passages are relevant but not particularly helpful for solving the question, which influences the model’s judgment and leads to incorrect answers."
      },
      {
        "filename": "Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models__p2__score1.00.png",
        "path": "figs_human_eval_papers/Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models/Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models__p2__score1.00.png",
        "caption": "Figure 2: The overall pipeline of our SKR method. We first collect self-knowledge from training questions according to the performance with or without external information (§ 3.1). Then we use the LLMs themselves or explicit small trainable models to elicit self-knowledge of a question qt by referring to the collected self-knowledge from training questions (§ 3.2). Finally, we use the self-knowledge to the new question and adaptively call a retriever (§ 3.3)."
      },
      {
        "filename": "Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models__p3__score1.00.png",
        "path": "figs_human_eval_papers/Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models/Self-Knowledge_Guided_Retrieval_Augmentation_for_Large_Language_Models__p3__score1.00.png",
        "caption": "Figure 3: Illustration of k-nearest-neighbor search to elicit the self-knowledge to the question qt."
      }
    ]
  },
  {
    "title": "Shifting Attention to Relevance Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models",
    "folder_name": "Shifting_Attention_to_Relevance_Towards_the_Predictive_Uncertainty_Quantification_of_Free-Form_Large_Language_Models",
    "images": [
      {
        "filename": "Shifting_Attention_to_Relevance_Towards_the_Predictive_Uncertainty_Quantification_of_Free-Form_Large_Language_Models__p0__score0.90.png",
        "path": "figs_human_eval_papers/Shifting_Attention_to_Relevance_Towards_the_Predictive_Uncertainty_Quantification_of_Free-Form_Large_Language_Models/Shifting_Attention_to_Relevance_Towards_the_Predictive_Uncertainty_Quantification_of_Free-Form_Large_Language_Models__p0__score0.90.png",
        "caption": "Figure 1: Irrelevant tokens (or sentences) may commit majority uncertainty in free-form generations, such as the token “of” committing extremely large uncertainty misleads the uncertainty quantification of LLMs. We term these observations as generative inequalities and tackle them by shifting attention to more relevant components."
      }
    ]
  },
  {
    "title": "Sociodemographic Persona Prompts Evaluation",
    "folder_name": "Sociodemographic_Persona_Prompts_Evaluation",
    "images": [
      {
        "filename": "Sociodemographic_Persona_Prompts_Evaluation__p0__score0.95.png",
        "path": "figs_human_eval_papers/Sociodemographic_Persona_Prompts_Evaluation/Sociodemographic_Persona_Prompts_Evaluation__p0__score0.95.png",
        "caption": "Figure 1: Evaluation Framework for Sociodemographic Persona Prompting. We construct sociodemographic persona prompts using combinations of three different role adoption formats and three strategies for demographic priming. We populate these prompts in conjunction with various sociodemographic groups and systematically evaluate them across both open- and closed-ended tasks using a broad set of bias and alignment measures."
      }
    ]
  },
  {
    "title": "Spiral of Silence How is Large Language Model Killing Information Retrieval A Case Study on Open Domain Question Answering",
    "folder_name": "Spiral_of_Silence_How_is_Large_Language_Model_Killing_Information_Retrieval_A_Case_Study_on_Open_Domain_Question_Answering",
    "images": [
      {
        "filename": "Spiral_of_Silence_How_is_Large_Language_Model_Killing_Information_Retrieval_A_Case_Study_on_Open_Domain_Question_Answering__p0__score1.00.png",
        "path": "figs_human_eval_papers/Spiral_of_Silence_How_is_Large_Language_Model_Killing_Information_Retrieval_A_Case_Study_on_Open_Domain_Question_Answering/Spiral_of_Silence_How_is_Large_Language_Model_Killing_Information_Retrieval_A_Case_Study_on_Open_Domain_Question_Answering__p0__score1.00.png",
        "caption": "Figure 1: The evolution of RAG systems after introducing LLM-generated texts, where the “Spiral of Silence” effect gradually emerges."
      }
    ]
  },
  {
    "title": "Steering Llama 2 via Contrastive Activation Addition",
    "folder_name": "Steering_Llama_2_via_Contrastive_Activation_Addition",
    "images": [
      {
        "filename": "Steering_Llama_2_via_Contrastive_Activation_Addition__p0__score1.00.png",
        "path": "figs_human_eval_papers/Steering_Llama_2_via_Contrastive_Activation_Addition/Steering_Llama_2_via_Contrastive_Activation_Addition__p0__score1.00.png",
        "caption": "Figure 1: We perform forward passes on contrastive examples of answers to multiple-choice questions, extracting residual stream activations at a particular layer at the token position of the answer. We then take the mean activation difference over many contrast pairs. At inference time, this vector is added back into the residual stream with a chosen multiplier at all token positions after the instruction to control the behavior."
      }
    ]
  },
  {
    "title": "Step-level Value Preference Optimization for Mathematical Reasoning",
    "folder_name": "Step-level_Value_Preference_Optimization_for_Mathematical_Reasoning",
    "images": [
      {
        "filename": "Step-level_Value_Preference_Optimization_for_Mathematical_Reasoning__p2__score1.00.png",
        "path": "figs_human_eval_papers/Step-level_Value_Preference_Optimization_for_Mathematical_Reasoning/Step-level_Value_Preference_Optimization_for_Mathematical_Reasoning__p2__score1.00.png",
        "caption": "Figure 1: Comparison of different frameworks: SFT, DPO, and SVPO. The top panel shows the typical pipeline of SFT and DPO, where GPT-4 does not indicate which step in yl led to the mistake. The bottom panel illustrates the pipeline of SVPO. Step-level preferences are autonomously generated via MCTS, where Q-values (represented by node colors) indicate potential reasoning errors."
      }
    ]
  },
  {
    "title": "To Mask or to Mirror Human-AI Alignment in Collective Reasoning",
    "folder_name": "To_Mask_or_to_Mirror_Human-AI_Alignment_in_Collective_Reasoning",
    "images": [
      {
        "filename": "To_Mask_or_to_Mirror_Human-AI_Alignment_in_Collective_Reasoning__p5__score1.00.png",
        "path": "figs_human_eval_papers/To_Mask_or_to_Mirror_Human-AI_Alignment_in_Collective_Reasoning/To_Mask_or_to_Mirror_Human-AI_Alignment_in_Collective_Reasoning__p5__score1.00.png",
        "caption": "Figure 1: Overview of experimental stages and representative interface images for the Lost at Sea implementation. 1) Participants are randomly assigned to either an identified or pseudonymous condition, 2) deliberate in groups of four, 3) self-nominate for leader eligibility, and 4) elect a representative via ranked-choice voting. 5) Each participant also completes the survival task individually, allowing leader quality to be measured."
      }
    ]
  },
  {
    "title": "Towards Statistical Factuality Guarantee for Large Vision-Language Models",
    "folder_name": "Towards_Statistical_Factuality_Guarantee_for_Large_Vision-Language_Models",
    "images": [
      {
        "filename": "Towards_Statistical_Factuality_Guarantee_for_Large_Vision-Language_Models__p1__score1.00.png",
        "path": "figs_human_eval_papers/Towards_Statistical_Factuality_Guarantee_for_Large_Vision-Language_Models/Towards_Statistical_Factuality_Guarantee_for_Large_Vision-Language_Models__p1__score1.00.png",
        "caption": "Figure 1: Overview: given user-specified error tolerance λ, error rate α, and a calibration dataset, CONFLVLM returns a more reliable response for any new image and prompt at inference time through sampling, decomposing D, filtering F , and merging M , to ensure that the risk of the final response Y ∗ is controlled with high probability. Illustrative examples, one for each application domain, are provided for outcome demonstration, where claims are highlighted to indicate CONFLVLM’s confidence using specific conformity score and error tolerance level. Unhighlighted claims correspond to low confidence in factuality check."
      }
    ]
  },
  {
    "title": "VISTA Visualized Text Embedding For Universal Multi-Modal Retrieval",
    "folder_name": "VISTA_Visualized_Text_Embedding_For_Universal_Multi-Modal_Retrieval",
    "images": [
      {
        "filename": "VISTA_Visualized_Text_Embedding_For_Universal_Multi-Modal_Retrieval__p2__score1.00.png",
        "path": "figs_human_eval_papers/VISTA_Visualized_Text_Embedding_For_Universal_Multi-Modal_Retrieval/VISTA_Visualized_Text_Embedding_For_Universal_Multi-Modal_Retrieval__p2__score1.00.png",
        "caption": "Figure 1: The model architecture of our VISTA model. We use the pre-trained language model as the foundation, making the ViT encoder transfer the Image to recognized tokens of the text encoder."
      },
      {
        "filename": "VISTA_Visualized_Text_Embedding_For_Universal_Multi-Modal_Retrieval__p3__score1.00.png",
        "path": "figs_human_eval_papers/VISTA_Visualized_Text_Embedding_For_Universal_Multi-Modal_Retrieval/VISTA_Visualized_Text_Embedding_For_Universal_Multi-Modal_Retrieval__p3__score1.00.png",
        "caption": "Figure 2: The construction pipeline of Image&Text To Image (IT2T) dataset."
      }
    ]
  },
  {
    "title": "VLind-Bench Measuring Language Priors in Large Vision-Language Models",
    "folder_name": "VLind-Bench_Measuring_Language_Priors_in_Large_Vision-Language_Models",
    "images": [
      {
        "filename": "VLind-Bench_Measuring_Language_Priors_in_Large_Vision-Language_Models__p1__score1.00.png",
        "path": "figs_human_eval_papers/VLind-Bench_Measuring_Language_Priors_in_Large_Vision-Language_Models/VLind-Bench_Measuring_Language_Priors_in_Large_Vision-Language_Models__p1__score1.00.png",
        "caption": "Figure 1: (a) An example from VLind-Bench. Our benchmark consists of four types of questions (i-iv). (b) Evaluation pipeline of VLind-Bench. In the pipeline, both true and false statements of the current stage must be correctly evaluated to proceed to the next stage."
      }
    ]
  },
  {
    "title": "Vision-Language Models Can Self-Improve Reasoning via Reflection",
    "folder_name": "Vision-Language_Models_Can_Self-Improve_Reasoning_via_Reflection",
    "images": [
      {
        "filename": "Vision-Language_Models_Can_Self-Improve_Reasoning_via_Reflection__p3__score1.00.png",
        "path": "figs_human_eval_papers/Vision-Language_Models_Can_Self-Improve_Reasoning_via_Reflection/Vision-Language_Models_Can_Self-Improve_Reasoning_via_Reflection__p3__score1.00.png",
        "caption": "Figure 2: Overview of our multimodal self-training framework of R3V . It boosts vision-language reasoning by iteratively reflecting on bootstrapped CoT rationales and enables self-reflection through test-time computing."
      }
    ]
  },
  {
    "title": "Visual Evidence Prompting Mitigates Hallucinations in Large Vision-Language Models",
    "folder_name": "Visual_Evidence_Prompting_Mitigates_Hallucinations_in_Large_Vision-Language_Models",
    "images": [
      {
        "filename": "Visual_Evidence_Prompting_Mitigates_Hallucinations_in_Large_Vision-Language_Models__p0__score0.90.png",
        "path": "figs_human_eval_papers/Visual_Evidence_Prompting_Mitigates_Hallucinations_in_Large_Vision-Language_Models/Visual_Evidence_Prompting_Mitigates_Hallucinations_in_Large_Vision-Language_Models__p0__score0.90.png",
        "caption": "Figure 1: Visualization of the image attribution map for LLaVA-1.5-7B when hallucination happens and after the integration of visual evidence. Best viewed zoomed in. More cases can be found in Appendix E.2 and E.3."
      },
      {
        "filename": "Visual_Evidence_Prompting_Mitigates_Hallucinations_in_Large_Vision-Language_Models__p3__score1.00.png",
        "path": "figs_human_eval_papers/Visual_Evidence_Prompting_Mitigates_Hallucinations_in_Large_Vision-Language_Models/Visual_Evidence_Prompting_Mitigates_Hallucinations_in_Large_Vision-Language_Models__p3__score1.00.png",
        "caption": "Figure 3: An overview of visual evidence prompting, which mitigates hallucinations in LVLMs via referring to visual evidence from small visual models. Given the input image, the small visual models generate visual evidence about different aspects of the image, e.g., object categories, and relations between objects. Then the “visual evidence” prompts are used to extract the answer from the image and evidence context."
      }
    ]
  },
  {
    "title": "Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering",
    "folder_name": "Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering",
    "images": [
      {
        "filename": "Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering__p0__score0.80.png",
        "path": "figs_human_eval_papers/Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering/Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering__p0__score0.80.png",
        "caption": "Figure 1: Overview of task setup on Natural Language Visual Reasoning (top) and WIKITABLEQUESTIONS (bottom) dataset. The datasets include only utterance x, world w and denotation y (ground truth program z is not given). Spurious programs like z′, whose meaning is wrong but execution result is correct, are major challenges of the task."
      },
      {
        "filename": "Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering__p2__score0.70.png",
        "path": "figs_human_eval_papers/Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering/Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering__p2__score0.70.png",
        "caption": "Figure 2: Illustration of our program representation scheme and filtering based on majority vote. Retrieved worlds (wj’s) partition the programs into several groups by their execution results and are represented as lines in the figure.4 By running majority vote based on the execution results, programs in the gray regions may be filtered."
      },
      {
        "filename": "Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering__p3__score0.70.png",
        "path": "figs_human_eval_papers/Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering/Weakly_Supervised_Semantic_Parsing_with_Execution-based_Spurious_Program_Filtering__p3__score0.70.png",
        "caption": "Figure 3: Illustration of column and entity replacement. Here, z1 and z2 are programs conditioned on the source table, and z′1 and z′2 are their counterparts modified to be executed on the target table. Within the programs, column and entity names of the same type are displayed in the same color."
      }
    ]
  },
  {
    "title": "WebEvolver Enhancing Web Agent Self-Improvement with Co-evolving World Model",
    "folder_name": "WebEvolver_Enhancing_Web_Agent_Self-Improvement_with_Co-evolving_World_Model",
    "images": [
      {
        "filename": "WebEvolver_Enhancing_Web_Agent_Self-Improvement_with_Co-evolving_World_Model__p0__score1.00.png",
        "path": "figs_human_eval_papers/WebEvolver_Enhancing_Web_Agent_Self-Improvement_with_Co-evolving_World_Model/WebEvolver_Enhancing_Web_Agent_Self-Improvement_with_Co-evolving_World_Model__p0__score1.00.png",
        "caption": "Figure 1: Overview of WebEvolver – A Self-Improving Framework with World-Model Look-Ahead. Our framework co-trains a world model with the agent to predict next-step observations based on current states and actions. The world model serves as a virtual web engine, which generates synthetic trajectories for policy training and enables look-ahead planning to select optimal actions during inference."
      },
      {
        "filename": "WebEvolver_Enhancing_Web_Agent_Self-Improvement_with_Co-evolving_World_Model__p2__score1.00.png",
        "path": "figs_human_eval_papers/WebEvolver_Enhancing_Web_Agent_Self-Improvement_with_Co-evolving_World_Model/WebEvolver_Enhancing_Web_Agent_Self-Improvement_with_Co-evolving_World_Model__p2__score1.00.png",
        "caption": "Figure 2: An illustration of the World Model trajectory synthesizing process and World Model Look-ahead for inference-time action selection."
      }
    ]
  },
  {
    "title": "When Not to Trust Language Models Investigating Effectiveness of Parametric and Non-Parametric Memories",
    "folder_name": "When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories",
    "images": [
      {
        "filename": "When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories__p0__score0.70.png",
        "path": "figs_human_eval_papers/When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories/When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories__p0__score0.70.png",
        "caption": "Figure 1: Relationship between subject entity popularity in a question and GPT-3 performance in open-domain QA, with and without retrieved passages. Adaptive Retrieval only retrieves when necessary (orange bars) based on the heuristically-decided threshold (red line)."
      },
      {
        "filename": "When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories__p0__score0.80.png",
        "path": "figs_human_eval_papers/When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories/When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories__p0__score0.80.png",
        "caption": "Figure 1: Relationship between subject entity popularity in a question and GPT-3 performance in open-domain QA, with and without retrieved passages. Adaptive Retrieval only retrieves when necessary (orange bars) based on the heuristically-decided threshold (red line)."
      },
      {
        "filename": "When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories__p2__score1.00.png",
        "path": "figs_human_eval_papers/When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories/When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories__p2__score1.00.png",
        "caption": "Figure 2: POPQA is created by sampling knowledge triples from Wikidata and converting them to natural language questions, followed by popularity calculation."
      },
      {
        "filename": "When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories__p2__score1.00__1.png",
        "path": "figs_human_eval_papers/When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories/When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of_Parametric_and_Non-Parametric_Memories__p2__score1.00__1.png",
        "caption": "Figure 2: POPQA is created by sampling knowledge triples from Wikidata and converting them to natural language questions, followed by popularity calculation."
      }
    ]
  },
  {
    "title": "ZoomEye Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration",
    "folder_name": "ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration",
    "images": [
      {
        "filename": "ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration__p0__score1.00.png",
        "path": "figs_human_eval_papers/ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration/ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration__p0__score1.00.png",
        "caption": "Figure 1: Top: When dealing with a high-resolution image, MLLMs effectively perceive the dominant objects but often fail to recognize finer details, highlighting the need for vision-level reasoning. Bottom: Applied with Zoom Eye, MLLMs could perform vision-level reasoning, allowed to explore the image details until they can answer the question."
      },
      {
        "filename": "ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration__p1__score0.95.png",
        "path": "figs_human_eval_papers/ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration/ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration__p1__score0.95.png",
        "caption": "Figure 2: Zoom Eye enables MLLMs to (a) answer the question directly when the visual information is adequate, (b) zoom in gradually for a closer examination, and (c) zoom out to the previous view and explore other regions if the desired information is not initially found."
      },
      {
        "filename": "ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration__p3__score0.95.png",
        "path": "figs_human_eval_papers/ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration/ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration__p3__score0.95.png",
        "caption": "Figure 3: Two image input methods for MLLMs with distinct image processing."
      },
      {
        "filename": "ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration__p7__score0.95.png",
        "path": "figs_human_eval_papers/ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration/ZoomEye_Enhancing_Multimodal_LLMs_with_Human-Like_Zooming_Capabilities_through_Tree-Based_Image_Exploration__p7__score0.95.png",
        "caption": "Figure 5: Examples of Zoom Eye. The resolution of the image is displayed. Red rectangles are patches searched by Zoom Eye."
      }
    ]
  },
  {
    "title": "of Multimodal Large Language Models Multimodal Needle in a Haystack Benchmarking Long-Context Capability",
    "folder_name": "of_Multimodal_Large_Language_Models_Multimodal_Needle_in_a_Haystack_Benchmarking_Long-Context_Capability",
    "images": [
      {
        "filename": "of_Multimodal_Large_Language_Models_Multimodal_Needle_in_a_Haystack_Benchmarking_Long-Context_Capability__p1__score1.00.png",
        "path": "figs_human_eval_papers/of_Multimodal_Large_Language_Models_Multimodal_Needle_in_a_Haystack_Benchmarking_Long-Context_Capability/of_Multimodal_Large_Language_Models_Multimodal_Needle_in_a_Haystack_Benchmarking_Long-Context_Capability__p1__score1.00.png",
        "caption": "Figure 1: MMNeedle evaluation overview. Correct answers are marked with checkmark (✓), while the incorrect answers are marked with cross (×). Our evaluation setup involves the following key components: (a) Needle Sub-Image: The needle sub-image to be retrieved based on the given caption. (b) Haystack Image Inputs: The long-context visual inputs consist of M images, each stitched from N × N sub-images. (c) Text Inputs (Instructions and Caption): Detailed instructions to MLLMs, followed by a caption describing the needle, i.e., sub-image 20. See Sec. A for MMNeedle’s complete instructions. (d) LLM Outputs: The answers from different MLLMs, indicating their ability to accurately locate the needle in the haystack based on the given caption. The expected output is composed of the model’s identification of the index, row, and column of the matching sub-image. The results showcase the comparative performance of various models: GPT-4o correctly predicts the exact location of the needle; Gemini Pro 1.5 only correctly predicts the image index of the needle; other API models predict incorrect locations; open-source models often output with wrong formats."
      }
    ]
  }
];