window.PAPER_DATA = [
  {
    "id": 1,
    "folder_name": "AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446",
    "title": "AIMSCheck Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions",
    "images": [
      {
        "filename": "AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446/AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446__p0__score1.00.png",
        "caption": "Figure 1: The common and unique mandatory reporting criteria associated with the Modern Slavery Acts in Australia, the United Kingdom, and Canada. The three datasets (AIMS.au, AIMS.uk, AIMS.ca) allow for crossjurisdictional evaluation of compliance checking tools."
      },
      {
        "filename": "AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446/AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446__p1__score1.00.png",
        "caption": "Figure 2: The AIMSCheck pipeline is designed to process modern slavery statements mandated by the Modern Slavery Acts of Australia, the United Kingdom and Canada. It generates predictions at sentence-level across multiple reporting criteria, complemented by token-level explainability techniques and evidence tracking. These outputs enable human analysts to efficiently and thoroughly review company compliance."
      },
      {
        "filename": "AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446/AIMSCheck_Leveraging_LLMs_for_AI-Assisted_Review_of_Modern_Slavery_Statements_Across_Jurisdictions_2025.acl-long.446__p3__score1.00.png",
        "caption": "Figure 3: Mapping the AU MSA mandatory criteria, UK MSA reporting suggestions, and Canadian Act reporting obligations based on their common criteria."
      }
    ],
    "acl_id": "2025.acl-long.446"
  },
  {
    "id": 2,
    "folder_name": "AMPS_ASR_with_Multimodal_Paraphrase_Supervision_2025.naacl-short.35",
    "title": "AMPS ASR with Multimodal Paraphrase Supervision",
    "images": [
      {
        "filename": "AMPS_ASR_with_Multimodal_Paraphrase_Supervision_2025.naacl-short.35__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/AMPS_ASR_with_Multimodal_Paraphrase_Supervision_2025.naacl-short.35/AMPS_ASR_with_Multimodal_Paraphrase_Supervision_2025.naacl-short.35__p1__score1.00.png",
        "caption": "Figure 1: Multimodal AMPSτ Pipeline. AMPSτ applies a dual pass through the S2T pipeline with an ASR objective and the T2T pipeline with a paraphrasing objective. The paraphrasing loss is only incorporated when the ASR loss exceeds a predefined threshold."
      }
    ],
    "acl_id": "2025.naacl-short.35"
  },
  {
    "id": 3,
    "folder_name": "A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887",
    "title": "A Law Reasoning Benchmark for LLM with Tree-Organized Structures including Factum Probandum, Evidence and Experiences",
    "images": [
      {
        "filename": "A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p0__score1.00.png",
        "caption": "Figure 1: Case “Rex v. Bywaters and Thompson” that demonstrates different experiences have impacted different results (LEFT vs. RIGHT). The case description and evidence are shared, but the experiences of both sides are different, which leads to different ultimate probandum."
      },
      {
        "filename": "A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p1__score0.95.png",
        "path": "human_eval_dataset_dirs/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p1__score0.95.png",
        "caption": "Figure 2: Illustration of the schema."
      },
      {
        "filename": "A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p2__score1.00.png",
        "caption": "Figure 3: Illustration of the task. For convenience, we showcase examples for each sub-task. The output of the 3 sub-tasks is collected to form the complete law reasoning structure."
      },
      {
        "filename": "A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p3__score0.70.png",
        "path": "human_eval_dataset_dirs/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p3__score0.70.png",
        "caption": "Figure 4: Illustration of the factum probandum generation."
      },
      {
        "filename": "A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p3__score0.80.png",
        "path": "human_eval_dataset_dirs/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p3__score0.80.png",
        "caption": "Figure 6: Illustration of the evidence reasoning in subtask2."
      },
      {
        "filename": "A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p5__score1.00.png",
        "path": "human_eval_dataset_dirs/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887/A_Law_Reasoning_Benchmark_for_LLM_with_Tree-Organized_Structures_including_Factum_Probandum_Evidence_and_Experiences_2025.findings-acl.887__p5__score1.00.png",
        "caption": "Figure 8: Illustration of our approach."
      }
    ],
    "acl_id": "2025.findings-acl.887"
  },
  {
    "id": 4,
    "folder_name": "A_Mechanistic_Interpretation_of_Arithmetic_Reasoning_in_Language_Models_using_Causal_Mediation_Analysis_2023.emnlp-main.435",
    "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
    "images": [
      {
        "filename": "A_Mechanistic_Interpretation_of_Arithmetic_Reasoning_in_Language_Models_using_Causal_Mediation_Analysis_2023.emnlp-main.435__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/A_Mechanistic_Interpretation_of_Arithmetic_Reasoning_in_Language_Models_using_Causal_Mediation_Analysis_2023.emnlp-main.435/A_Mechanistic_Interpretation_of_Arithmetic_Reasoning_in_Language_Models_using_Causal_Mediation_Analysis_2023.emnlp-main.435__p0__score0.95.png",
        "caption": "Figure 1: Visualization of our findings. We trace the flow of numerical information within Transformerbased LMs: given an input query, the model processes the representations of numbers and operators with early layers (A). Then, the relevant information is conveyed by the attention mechanism to the end of the input sequence (B). Here, it is processed by late MLP modules, which output result-related information into the residual stream (C)."
      },
      {
        "filename": "A_Mechanistic_Interpretation_of_Arithmetic_Reasoning_in_Language_Models_using_Causal_Mediation_Analysis_2023.emnlp-main.435__p2__score0.98.png",
        "path": "human_eval_dataset_dirs/A_Mechanistic_Interpretation_of_Arithmetic_Reasoning_in_Language_Models_using_Causal_Mediation_Analysis_2023.emnlp-main.435/A_Mechanistic_Interpretation_of_Arithmetic_Reasoning_in_Language_Models_using_Causal_Mediation_Analysis_2023.emnlp-main.435__p2__score0.98.png",
        "caption": "Figure 2: By intervening on the activation values of specific components within a language model and computing the corresponding effects, we identify the subset of parameters responsible for specific predictions."
      }
    ],
    "acl_id": "2023.emnlp-main.435"
  },
  {
    "id": 5,
    "folder_name": "Adapters_Selector_Cross-domains_and_Multi-tasks_LoRA_Modules_Integration_Usage_Method_2025.coling-main.40",
    "title": "Adapters Selector Cross-domains and Multi-tasks LoRA Modules Integration Usage Method",
    "images": [
      {
        "filename": "Adapters_Selector_Cross-domains_and_Multi-tasks_LoRA_Modules_Integration_Usage_Method_2025.coling-main.40__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Adapters_Selector_Cross-domains_and_Multi-tasks_LoRA_Modules_Integration_Usage_Method_2025.coling-main.40/Adapters_Selector_Cross-domains_and_Multi-tasks_LoRA_Modules_Integration_Usage_Method_2025.coling-main.40__p2__score1.00.png",
        "caption": "Figure 1: The overall architecture of our proposed AS. The framework involves three processes. Firstly, every domain-specific task-specific dataset is used to fine-tune the model and obtain the adapters index. Secondly, each dataset should undergo data selection and the resulting subsets should then be randomly mixed with shuffling in order to train the selector. Finally, the adapter and selector are integrated into the base model for inference."
      },
      {
        "filename": "Adapters_Selector_Cross-domains_and_Multi-tasks_LoRA_Modules_Integration_Usage_Method_2025.coling-main.40__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Adapters_Selector_Cross-domains_and_Multi-tasks_LoRA_Modules_Integration_Usage_Method_2025.coling-main.40/Adapters_Selector_Cross-domains_and_Multi-tasks_LoRA_Modules_Integration_Usage_Method_2025.coling-main.40__p3__score1.00.png",
        "caption": "Figure 3: The specific steps of selection, switching, and inference using the inference flow of the AS framework simulate involve separating the model from the selector and combining it with the adapter selected by the selector to generate an output from an input with a domain-task-specific template or instruction."
      }
    ],
    "acl_id": "2025.coling-main.40"
  },
  {
    "id": 6,
    "folder_name": "Advancing_MoE_Efficiency_A_Collaboration-Constrained_Routing_C2R_Strategy_for_Better_Expert_Parallelism_Design_2025.naacl-long.347",
    "title": "Advancing MoE Efficiency A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design",
    "images": [
      {
        "filename": "Advancing_MoE_Efficiency_A_Collaboration-Constrained_Routing_C2R_Strategy_for_Better_Expert_Parallelism_Design_2025.naacl-long.347__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Advancing_MoE_Efficiency_A_Collaboration-Constrained_Routing_C2R_Strategy_for_Better_Expert_Parallelism_Design_2025.naacl-long.347/Advancing_MoE_Efficiency_A_Collaboration-Constrained_Routing_C2R_Strategy_for_Better_Expert_Parallelism_Design_2025.naacl-long.347__p3__score1.00.png",
        "caption": "Figure 1: Overview of C2R. (a) shows the process of expert profiling where we obtain the expert collaboration matrix for each layer of the MoE model; (b) describes the mechanism of our C2R strategy. It first selects the top-1 expert for a given token (Expert i here) and then selects the remaining K− 1 experts from list Top-T(Expert i); (c) shows our efficient expert parallelism design."
      }
    ],
    "acl_id": "2025.naacl-long.347"
  },
  {
    "id": 7,
    "folder_name": "Aggregating_Multiple_Heuristic_Signals_as_Supervision_for_Unsupervised_Automated_Essay_Scoring_2023.acl-long.782",
    "title": "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring",
    "images": [
      {
        "filename": "Aggregating_Multiple_Heuristic_Signals_as_Supervision_for_Unsupervised_Automated_Essay_Scoring_2023.acl-long.782__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Aggregating_Multiple_Heuristic_Signals_as_Supervision_for_Unsupervised_Automated_Essay_Scoring_2023.acl-long.782/Aggregating_Multiple_Heuristic_Signals_as_Supervision_for_Unsupervised_Automated_Essay_Scoring_2023.acl-long.782__p2__score1.00.png",
        "caption": "Figure 1: Illustration of the proposed ULRA framework for unsupervised AES task."
      }
    ],
    "acl_id": "2023.acl-long.782"
  },
  {
    "id": 8,
    "folder_name": "Analyzing_register_variation_in_web_texts_through_automatic_segmentation_2025.nlp4dh-1.2",
    "title": "Analyzing register variation in web texts through automatic segmentation",
    "images": [
      {
        "filename": "Analyzing_register_variation_in_web_texts_through_automatic_segmentation_2025.nlp4dh-1.2__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Analyzing_register_variation_in_web_texts_through_automatic_segmentation_2025.nlp4dh-1.2/Analyzing_register_variation_in_web_texts_through_automatic_segmentation_2025.nlp4dh-1.2__p3__score1.00.png",
        "caption": "Figure 2: The recursive segmentation process."
      }
    ],
    "acl_id": "2025.nlp4dh-1.2"
  },
  {
    "id": 9,
    "folder_name": "Are_Fairy_Tales_Fair_Analyzing_Gender_Bias_in_Temporal_Narrative_Event_Chains_of_Childrens_Fairy_Tales_2023.acl-long.359",
    "title": "Are Fairy Tales Fair Analyzing Gender Bias in Temporal Narrative Event Chains of Childrens Fairy Tales",
    "images": [
      {
        "filename": "Are_Fairy_Tales_Fair_Analyzing_Gender_Bias_in_Temporal_Narrative_Event_Chains_of_Childrens_Fairy_Tales_2023.acl-long.359__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Are_Fairy_Tales_Fair_Analyzing_Gender_Bias_in_Temporal_Narrative_Event_Chains_of_Childrens_Fairy_Tales_2023.acl-long.359/Are_Fairy_Tales_Fair_Analyzing_Gender_Bias_in_Temporal_Narrative_Event_Chains_of_Childrens_Fairy_Tales_2023.acl-long.359__p3__score1.00.png",
        "caption": "Figure 1: Character and Event Extraction Pipeline"
      }
    ],
    "acl_id": "2023.acl-long.359"
  },
  {
    "id": 10,
    "folder_name": "Automated_Progressive_Red_Teaming_2025.coling-main.260",
    "title": "Automated Progressive Red Teaming",
    "images": [
      {
        "filename": "Automated_Progressive_Red_Teaming_2025.coling-main.260__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Automated_Progressive_Red_Teaming_2025.coling-main.260/Automated_Progressive_Red_Teaming_2025.coling-main.260__p1__score1.00.png",
        "caption": "Figure 1: Illustration of APRT. In the training process, the Intention Expanding LLM first generates diverse samples that are relatively easy to jailbreak the Target LLM after intention concealment. For each prompt generated by the Intention Expanding LLM, the Intention Hiding LLM transforms it into multiple effective samples with deceptive behavior towards the Target LLM, without changing the original intention of the prompt. The Target LLM dedicates to generating safe responses to resist the attacks from the Intention Hiding LLM. Two Reward LLMs provide a bias to select new incremental training samples for the Intention Hiding LLM. To swiftly enhance the capability of concealing the intentions within input prompts, the Intention Hiding LLM employs an active learning strategy to prioritize selecting samples that can successfully elicit unsafe yet helpful responses from the Target LLM with intentions that are difficult to perceive."
      }
    ],
    "acl_id": "2025.coling-main.260"
  },
  {
    "id": 11,
    "folder_name": "Automated_Scoring_of_a_German_Written_Elicited_Imitation_Test_2025.bea-1.18",
    "title": "Automated Scoring of a German Written Elicited Imitation Test",
    "images": [
      {
        "filename": "Automated_Scoring_of_a_German_Written_Elicited_Imitation_Test_2025.bea-1.18__p4__score0.80.png",
        "path": "human_eval_dataset_dirs/Automated_Scoring_of_a_German_Written_Elicited_Imitation_Test_2025.bea-1.18/Automated_Scoring_of_a_German_Written_Elicited_Imitation_Test_2025.bea-1.18__p4__score0.80.png",
        "caption": "Figure 2: Token mapping by the aligner function for an example sentence. Tokens in red are misspelled and tokens in orange are missing or additional. Green arrows denote aligned tokens and blue arrows transpositions."
      },
      {
        "filename": "Automated_Scoring_of_a_German_Written_Elicited_Imitation_Test_2025.bea-1.18__p4__score1.00.png",
        "path": "human_eval_dataset_dirs/Automated_Scoring_of_a_German_Written_Elicited_Imitation_Test_2025.bea-1.18/Automated_Scoring_of_a_German_Written_Elicited_Imitation_Test_2025.bea-1.18__p4__score1.00.png",
        "caption": "Figure 1: Flow diagram illustrating the rule-based model’s data processing pipeline."
      }
    ],
    "acl_id": "2025.bea-1.18"
  },
  {
    "id": 12,
    "folder_name": "Automatic_Generation_of_Inference_Making_Questions_for_Reading_Comprehension_Assessments_2025.bea-1.31",
    "title": "Automatic Generation of Inference Making Questions for Reading Comprehension Assessments",
    "images": [
      {
        "filename": "Automatic_Generation_of_Inference_Making_Questions_for_Reading_Comprehension_Assessments_2025.bea-1.31__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Automatic_Generation_of_Inference_Making_Questions_for_Reading_Comprehension_Assessments_2025.bea-1.31/Automatic_Generation_of_Inference_Making_Questions_for_Reading_Comprehension_Assessments_2025.bea-1.31__p1__score1.00.png",
        "caption": "Figure 1: Overview of automatic item generation and human evaluation. We use GPT-4o to generate bridginginference RC items for given reading passages via few-shot prompting, comparing conditions with and without chain-of-thought prompts. We prompt each inference type separately: pronominal bridging, text-connecting, and gap-filling inferences. Human evaluation focuses on general item quality, inference type appropriateness, and LLM rationales."
      },
      {
        "filename": "Automatic_Generation_of_Inference_Making_Questions_for_Reading_Comprehension_Assessments_2025.bea-1.31__p4__score0.90.png",
        "path": "human_eval_dataset_dirs/Automatic_Generation_of_Inference_Making_Questions_for_Reading_Comprehension_Assessments_2025.bea-1.31/Automatic_Generation_of_Inference_Making_Questions_for_Reading_Comprehension_Assessments_2025.bea-1.31__p4__score0.90.png",
        "caption": "Figure 3: Few-shot prompt for generating pronominal bridging inference questions. The system prompt (beige background) defines the inference type and outlines expert-inspired steps. Training examples (provided in the prompt) follow. In the standard condition, only the question and answer key (green) are shown; in the CoT condition, text hints and reasoning (blue) are also included. A new passage is provided in the user prompt (orange background) to generate new questions."
      }
    ],
    "acl_id": "2025.bea-1.31"
  },
  {
    "id": 13,
    "folder_name": "Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287",
    "title": "Best of Both Worlds Towards Improving Temporal Knowledge Base Question Answering via Targeted Fact Extraction",
    "images": [
      {
        "filename": "Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287__p0__score0.90.png",
        "path": "human_eval_dataset_dirs/Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287/Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287__p0__score0.90.png",
        "caption": "Figure 1: Example question, its AMR and λ-expressions"
      },
      {
        "filename": "Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287/Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287__p2__score1.00.png",
        "caption": "Figure 2: An illustration of the proposed approach. Upper line of modules correspond to the KBQA pipeline, while lower line of modules are related to targeted fact extraction from textual resources."
      },
      {
        "filename": "Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287__p4__score0.80.png",
        "path": "human_eval_dataset_dirs/Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287/Best_of_Both_Worlds_Towards_Improving_Temporal_Knowledge_Base_Question_Answering_via_Targeted_Fact_Extraction_2023.emnlp-main.287__p4__score0.80.png",
        "caption": "Figure 3: Illustration of a working example showing the KBQA failure occurring due to missing auxiliary fact that is substituted by temporal fact extraction and finally reforming the lambda expression by hard-coding the missing fact."
      }
    ],
    "acl_id": "2023.emnlp-main.287"
  },
  {
    "id": 14,
    "folder_name": "Bridging_the_Gap_between_Expert_and_Language_Models_Concept-guided_Chess_Commentary_Generation_and_Evaluation_2025.naacl-long.481",
    "title": "Bridging the Gap between Expert and Language Models Concept-guided Chess Commentary Generation and Evaluation",
    "images": [
      {
        "filename": "Bridging_the_Gap_between_Expert_and_Language_Models_Concept-guided_Chess_Commentary_Generation_and_Evaluation_2025.naacl-long.481__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Bridging_the_Gap_between_Expert_and_Language_Models_Concept-guided_Chess_Commentary_Generation_and_Evaluation_2025.naacl-long.481/Bridging_the_Gap_between_Expert_and_Language_Models_Concept-guided_Chess_Commentary_Generation_and_Evaluation_2025.naacl-long.481__p0__score0.95.png",
        "caption": "Figure 1: Comparison of chess commentary generation methods. The red color indicates incorrect information."
      },
      {
        "filename": "Bridging_the_Gap_between_Expert_and_Language_Models_Concept-guided_Chess_Commentary_Generation_and_Evaluation_2025.naacl-long.481__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Bridging_the_Gap_between_Expert_and_Language_Models_Concept-guided_Chess_Commentary_Generation_and_Evaluation_2025.naacl-long.481/Bridging_the_Gap_between_Expert_and_Language_Models_Concept-guided_Chess_Commentary_Generation_and_Evaluation_2025.naacl-long.481__p3__score1.00.png",
        "caption": "Figure 2: Overview of CCC, consists of (a) extracting concept vectors and (b) generating concept-guided commentary."
      }
    ],
    "acl_id": "2025.naacl-long.481"
  },
  {
    "id": 15,
    "folder_name": "Bring_Your_Own_Knowledge_A_Survey_of_Methods_for_LLM_Knowledge_Expansion_2025.l2m2-1.12",
    "title": "Bring Your Own Knowledge A Survey of Methods for LLM Knowledge Expansion",
    "images": [
      {
        "filename": "Bring_Your_Own_Knowledge_A_Survey_of_Methods_for_LLM_Knowledge_Expansion_2025.l2m2-1.12__p1__score0.95.png",
        "path": "human_eval_dataset_dirs/Bring_Your_Own_Knowledge_A_Survey_of_Methods_for_LLM_Knowledge_Expansion_2025.l2m2-1.12/Bring_Your_Own_Knowledge_A_Survey_of_Methods_for_LLM_Knowledge_Expansion_2025.l2m2-1.12__p1__score0.95.png",
        "caption": "Figure 1: Taxonomy of current methods for expanding LLM knowledge. Due to space constraints, please refer to Appendix A.1 for a comprehensive review of methods and their corresponding citations."
      }
    ],
    "acl_id": "2025.l2m2-1.12"
  },
  {
    "id": 16,
    "folder_name": "COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102",
    "title": "COVE COntext and VEracity prediction for out-of-context images",
    "images": [
      {
        "filename": "COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102/COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102__p0__score0.95.png",
        "caption": "Figure 1: The two steps of COVE: (1) Generating the true context of the image. (2) Predicting the veracity of a caption by comparing it with the generated context."
      },
      {
        "filename": "COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102/COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102__p2__score1.00.png",
        "caption": "Figure 2: The architecture of COVE consists of six steps. The first three are performed in parallel and consist of retrieving evidence. Step 4 predicts the context items in a QA setting. Step 5 updates missing items based on the existing ones and Wikipedia knowledge. Step 6 predicts the veracity of the caption based on the predicted context."
      },
      {
        "filename": "COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102__p3__score0.95.png",
        "path": "human_eval_dataset_dirs/COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102/COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102__p3__score0.95.png",
        "caption": "Figure 3: Wikipedia entities collection. The candidate set is composed of the entities in the caption and those that are most similar to the image. Candidates are retained if the similarity between the image and their name or their Wikipedia images passes a threshold."
      },
      {
        "filename": "COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102__p3__score0.95__1.png",
        "path": "human_eval_dataset_dirs/COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102/COVE_COntext_and_VEracity_prediction_for_out-of-context_images_2025.naacl-long.102__p3__score0.95__1.png",
        "caption": "Figure 4: Knowledge gap completion. Questions are generated based on the predicted context and answered with Wikipedia passages. If the answers are relevant, the context is updated."
      }
    ],
    "acl_id": "2025.naacl-long.102"
  },
  {
    "id": 17,
    "folder_name": "CRaSh_Clustering_Removing_and_Sharing_Enhance_Fine-tuning_without_Full_Large_Language_Model_2023.emnlp-main.597",
    "title": "CRaSh Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model",
    "images": [
      {
        "filename": "CRaSh_Clustering_Removing_and_Sharing_Enhance_Fine-tuning_without_Full_Large_Language_Model_2023.emnlp-main.597__p4__score0.95.png",
        "path": "human_eval_dataset_dirs/CRaSh_Clustering_Removing_and_Sharing_Enhance_Fine-tuning_without_Full_Large_Language_Model_2023.emnlp-main.597/CRaSh_Clustering_Removing_and_Sharing_Enhance_Fine-tuning_without_Full_Large_Language_Model_2023.emnlp-main.597__p4__score0.95.png",
        "caption": "Figure 4: Overview of Offsite-Tuning and CRaSh strategy."
      }
    ],
    "acl_id": "2023.emnlp-main.597"
  },
  {
    "id": 18,
    "folder_name": "CVE-Bench_Benchmarking_LLM-based_Software_Engineering_Agents_Ability_to_Repair_Real-World_CVE_Vulnerabilities_2025.naacl-long.212",
    "title": "CVE-Bench Benchmarking LLM-based Software Engineering Agents Ability to Repair Real-World CVE Vulnerabilities",
    "images": [
      {
        "filename": "CVE-Bench_Benchmarking_LLM-based_Software_Engineering_Agents_Ability_to_Repair_Real-World_CVE_Vulnerabilities_2025.naacl-long.212__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/CVE-Bench_Benchmarking_LLM-based_Software_Engineering_Agents_Ability_to_Repair_Real-World_CVE_Vulnerabilities_2025.naacl-long.212/CVE-Bench_Benchmarking_LLM-based_Software_Engineering_Agents_Ability_to_Repair_Real-World_CVE_Vulnerabilities_2025.naacl-long.212__p3__score1.00.png",
        "caption": "Figure 2: There are four steps in CVE-Bench: (1) Environment construction: CVE-Bench uses the three-level information to generate a vulnerability issue as the input to the agent. CVE-Bench clones the CVEs’ corresponding repositories and checks out the repository to the parent commits. (2) Repair patch generation: Next, CVE-Bench sends the queried information divided into multiple levels to the agents for patch generation. (3) Repair validation: At last, CVE-Bench performs execution-based repair patch validation using the unit tests. CVE-Bench also considers comparing the generated patches with the ground truth repair code crafted by the man."
      },
      {
        "filename": "CVE-Bench_Benchmarking_LLM-based_Software_Engineering_Agents_Ability_to_Repair_Real-World_CVE_Vulnerabilities_2025.naacl-long.212__p7__score0.95.png",
        "path": "human_eval_dataset_dirs/CVE-Bench_Benchmarking_LLM-based_Software_Engineering_Agents_Ability_to_Repair_Real-World_CVE_Vulnerabilities_2025.naacl-long.212/CVE-Bench_Benchmarking_LLM-based_Software_Engineering_Agents_Ability_to_Repair_Real-World_CVE_Vulnerabilities_2025.naacl-long.212__p7__score0.95.png",
        "caption": "Figure 8: We show an example of a formatted task instance, the original code, ground-truth repair code (extracted from the database, made by the developers), the 3-level information, generated issue, and the agent-generated code. In the code blocks, grey highlights are additions."
      }
    ],
    "acl_id": "2025.naacl-long.212"
  },
  {
    "id": 19,
    "folder_name": "Can_Automatic_Metrics_Assess_High-Quality_Translations_2024.emnlp-main.802",
    "title": "Can Automatic Metrics Assess High-Quality Translations",
    "images": [
      {
        "filename": "Can_Automatic_Metrics_Assess_High-Quality_Translations_2024.emnlp-main.802__p2__score0.95.png",
        "path": "human_eval_dataset_dirs/Can_Automatic_Metrics_Assess_High-Quality_Translations_2024.emnlp-main.802/Can_Automatic_Metrics_Assess_High-Quality_Translations_2024.emnlp-main.802__p2__score0.95.png",
        "caption": "Figure 1: Ranking analysis configurations. ρ: Spearman correlation."
      }
    ],
    "acl_id": "2024.emnlp-main.802"
  },
  {
    "id": 20,
    "folder_name": "ChronoSense_Exploring_Temporal_Understanding_in_Large_Language_Models_with_Time_Intervals_of_Events_2025.acl-short.46",
    "title": "ChronoSense Exploring Temporal Understanding in Large Language Models with Time Intervals of Events",
    "images": [
      {
        "filename": "ChronoSense_Exploring_Temporal_Understanding_in_Large_Language_Models_with_Time_Intervals_of_Events_2025.acl-short.46__p0__score0.90.png",
        "path": "human_eval_dataset_dirs/ChronoSense_Exploring_Temporal_Understanding_in_Large_Language_Models_with_Time_Intervals_of_Events_2025.acl-short.46/ChronoSense_Exploring_Temporal_Understanding_in_Large_Language_Models_with_Time_Intervals_of_Events_2025.acl-short.46__p0__score0.90.png",
        "caption": "Figure 1: 13 Allen relations between two intervals, covering all combinations."
      },
      {
        "filename": "ChronoSense_Exploring_Temporal_Understanding_in_Large_Language_Models_with_Time_Intervals_of_Events_2025.acl-short.46__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/ChronoSense_Exploring_Temporal_Understanding_in_Large_Language_Models_with_Time_Intervals_of_Events_2025.acl-short.46/ChronoSense_Exploring_Temporal_Understanding_in_Large_Language_Models_with_Time_Intervals_of_Events_2025.acl-short.46__p1__score1.00.png",
        "caption": "Figure 2: An example for comparing two temporal events with LLMs."
      }
    ],
    "acl_id": "2025.acl-short.46"
  },
  {
    "id": 21,
    "folder_name": "ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585",
    "title": "ConsistencyChecker Tree-based Evaluation of LLM Generalization Capabilities",
    "images": [
      {
        "filename": "ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585/ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585__p0__score1.00.png",
        "caption": "Figure 1: Overview of the ConsistencyChecker. It shows a self-consistency tree for evaluating LLMs on machine translations. The root node (r) is the initial English sentence generated by the evaluator. Subsequent nodes (vi) are produced by the evaluatee using transformation pairs, such as English→Spanish→English (f1, f ′ 1) and English→German→English (f2, f ′ 2). The evaluation framework supports multilingual translations (e.g., French, Czech, Japanese) and can be extended to programming tasks."
      },
      {
        "filename": "ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585/ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585__p2__score1.00.png",
        "caption": "Figure 2: Key concepts in the self-consistency tree (operation, node, and edge). It provides a concrete example of the self-consistency tree in the AI-assisted programming task. The node v contains a function that returns the product of two positive integers, and I is its set of inputs. The prompt p asks the evaluatee LLM to rewrite the code in node v to use a looped sum instead of just multiplying, while p′ asks the same LLM to alter it back to simply multiplying."
      },
      {
        "filename": "ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585__p7__score0.90.png",
        "path": "human_eval_dataset_dirs/ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585/ConsistencyChecker_Tree-based_Evaluation_of_LLM_Generalization_Capabilities_2025.acl-long.1585__p7__score0.90.png",
        "caption": "Table 4: Ablation study on evaluator models. (Top) machine translation consistency scores (%) at complexity level C3(F) for model scales (7B–72B). (Bottom) AI-assisted programming consistency scores (%) at complexity level C3(F) for code transformations."
      }
    ],
    "acl_id": "2025.acl-long.1585"
  },
  {
    "id": 22,
    "folder_name": "Consultant_Decoding_Yet_Another_Synergistic_Mechanism_2025.findings-acl.797",
    "title": "Consultant Decoding Yet Another Synergistic Mechanism",
    "images": [
      {
        "filename": "Consultant_Decoding_Yet_Another_Synergistic_Mechanism_2025.findings-acl.797__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Consultant_Decoding_Yet_Another_Synergistic_Mechanism_2025.findings-acl.797/Consultant_Decoding_Yet_Another_Synergistic_Mechanism_2025.findings-acl.797__p0__score0.95.png",
        "caption": "Figure 1: (a) A visual description of the sub-optimal problem of distribution-based method, where the blue box is high-quality nucleus of Top-P sampling and the xd is the draft token. (b) The CD verification algorithm. q(x) and p(x) denote distribution of draft model and target model, respectively. The ε is the approximate convergence loss in the training phase of target model."
      },
      {
        "filename": "Consultant_Decoding_Yet_Another_Synergistic_Mechanism_2025.findings-acl.797__p3__score0.95.png",
        "path": "human_eval_dataset_dirs/Consultant_Decoding_Yet_Another_Synergistic_Mechanism_2025.findings-acl.797/Consultant_Decoding_Yet_Another_Synergistic_Mechanism_2025.findings-acl.797__p3__score0.95.png",
        "caption": "Figure 2: The comparison between Consultant Decoding and Top-P sampling. xi, x′ i, and x̂i denote draft token, resample token by CD, and sample token by TopP, respectively."
      }
    ],
    "acl_id": "2025.findings-acl.797"
  },
  {
    "id": 23,
    "folder_name": "Controllable_Mixed-Initiative_Dialogue_Generation_through_Prompting_2023.acl-short.82",
    "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting",
    "images": [
      {
        "filename": "Controllable_Mixed-Initiative_Dialogue_Generation_through_Prompting_2023.acl-short.82__p0__score0.90.png",
        "path": "human_eval_dataset_dirs/Controllable_Mixed-Initiative_Dialogue_Generation_through_Prompting_2023.acl-short.82/Controllable_Mixed-Initiative_Dialogue_Generation_through_Prompting_2023.acl-short.82__p0__score0.90.png",
        "caption": "Figure 1: Excerpt of a conversation between an emotional help-seeker and a supporter about a breakup, with candidate responses attempting to use the support strategy “Restatement or Paraphrasing.”"
      },
      {
        "filename": "Controllable_Mixed-Initiative_Dialogue_Generation_through_Prompting_2023.acl-short.82__p2__score0.80.png",
        "path": "human_eval_dataset_dirs/Controllable_Mixed-Initiative_Dialogue_Generation_through_Prompting_2023.acl-short.82/Controllable_Mixed-Initiative_Dialogue_Generation_through_Prompting_2023.acl-short.82__p2__score0.80.png",
        "caption": "Figure 2: Parts of an example prompt for ESC (yellow background). Task Background: ground truth annotations describing the conversation. Conversation History: dialogue context with natural language forms of annotated dialogue intents. Full situation in Appendix B.1."
      }
    ],
    "acl_id": "2023.acl-short.82"
  },
  {
    "id": 24,
    "folder_name": "Creating_Hierarchical_Relations_in_a_Multilingual_Event-type_Ontology_2025.law-1.19",
    "title": "Creating Hierarchical Relations in a Multilingual Event-type Ontology",
    "images": [
      {
        "filename": "Creating_Hierarchical_Relations_in_a_Multilingual_Event-type_Ontology_2025.law-1.19__p3__score0.70.png",
        "path": "human_eval_dataset_dirs/Creating_Hierarchical_Relations_in_a_Multilingual_Event-type_Ontology_2025.law-1.19/Creating_Hierarchical_Relations_in_a_Multilingual_Event-type_Ontology_2025.law-1.19__p3__score0.70.png",
        "caption": "Figure 2: The hierarchical concept Ownership Transfer (abbreviated; shown in the editing tool)"
      },
      {
        "filename": "Creating_Hierarchical_Relations_in_a_Multilingual_Event-type_Ontology_2025.law-1.19__p4__score0.85.png",
        "path": "human_eval_dataset_dirs/Creating_Hierarchical_Relations_in_a_Multilingual_Event-type_Ontology_2025.law-1.19/Creating_Hierarchical_Relations_in_a_Multilingual_Event-type_Ontology_2025.law-1.19__p4__score0.85.png",
        "caption": "Figure 3: The tree w/path to Possession or Ownership"
      }
    ],
    "acl_id": "2025.law-1.19"
  },
  {
    "id": 25,
    "folder_name": "CritiQ_Mining_Data_Quality_Criteria_from_Human_Preferences_2025.acl-long.792",
    "title": "CritiQ Mining Data Quality Criteria from Human Preferences",
    "images": [
      {
        "filename": "CritiQ_Mining_Data_Quality_Criteria_from_Human_Preferences_2025.acl-long.792__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/CritiQ_Mining_Data_Quality_Criteria_from_Human_Preferences_2025.acl-long.792/CritiQ_Mining_Data_Quality_Criteria_from_Human_Preferences_2025.acl-long.792__p2__score1.00.png",
        "caption": "Figure 2: CRITIQ Flow comprises two major components: multi-criteria pairwise judgment and the criteria evolution process. The multi-criteria pairwise judgment process employs a series of worker agents to make quality comparisons under a certain criterion. The criteria evolution process aims to obtain data quality criteria that highly align with human judgment through an iterative evolution. The initial criteria are retrieved from the knowledge base. After evolution, we select the final criteria to annotate the dataset for training CRITIQ Scorer."
      }
    ],
    "acl_id": "2025.acl-long.792"
  },
  {
    "id": 26,
    "folder_name": "Croppable_Knowledge_Graph_Embedding_2025.acl-long.579",
    "title": "Croppable Knowledge Graph Embedding",
    "images": [
      {
        "filename": "Croppable_Knowledge_Graph_Embedding_2025.acl-long.579__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/Croppable_Knowledge_Graph_Embedding_2025.acl-long.579/Croppable_Knowledge_Graph_Embedding_2025.acl-long.579__p0__score1.00.png",
        "caption": "Figure 1: Diverse KGE dimensions for a KG."
      },
      {
        "filename": "Croppable_Knowledge_Graph_Embedding_2025.acl-long.579__p2__score0.95.png",
        "path": "human_eval_dataset_dirs/Croppable_Knowledge_Graph_Embedding_2025.acl-long.579/Croppable_Knowledge_Graph_Embedding_2025.acl-long.579__p2__score0.95.png",
        "caption": "Figure 2: Overview of MED."
      }
    ],
    "acl_id": "2025.acl-long.579"
  },
  {
    "id": 27,
    "folder_name": "CulturalBench_A_Robust_Diverse_and_Challenging_Benchmark_for_Measuring_LMs_Cultural_Knowledge_Through_Human-AI_Red-Team_2025.acl-long.1247",
    "title": "CulturalBench A Robust, Diverse and Challenging Benchmark for Measuring LMs Cultural Knowledge Through Human-AI Red-Team",
    "images": [
      {
        "filename": "CulturalBench_A_Robust_Diverse_and_Challenging_Benchmark_for_Measuring_LMs_Cultural_Knowledge_Through_Human-AI_Red-Team_2025.acl-long.1247__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/CulturalBench_A_Robust_Diverse_and_Challenging_Benchmark_for_Measuring_LMs_Cultural_Knowledge_Through_Human-AI_Red-Team_2025.acl-long.1247/CulturalBench_A_Robust_Diverse_and_Challenging_Benchmark_for_Measuring_LMs_Cultural_Knowledge_Through_Human-AI_Red-Team_2025.acl-long.1247__p1__score1.00.png",
        "caption": "Figure 1: The human-AI collaborative data collection pipeline of CULTURALBENCH."
      }
    ],
    "acl_id": "2025.acl-long.1247"
  },
  {
    "id": 28,
    "folder_name": "Customizing_In-context_Learning_for_Dynamic_Interest_Adaption_in_LLM-based_Recommendation_2025.findings-acl.735",
    "title": "Customizing In-context Learning for Dynamic Interest Adaption in LLM-based Recommendation",
    "images": [
      {
        "filename": "Customizing_In-context_Learning_for_Dynamic_Interest_Adaption_in_LLM-based_Recommendation_2025.findings-acl.735__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Customizing_In-context_Learning_for_Dynamic_Interest_Adaption_in_LLM-based_Recommendation_2025.findings-acl.735/Customizing_In-context_Learning_for_Dynamic_Interest_Adaption_in_LLM-based_Recommendation_2025.findings-acl.735__p0__score0.95.png",
        "caption": "Figure 1: An illustration of user interest shift in realworld scenarios."
      },
      {
        "filename": "Customizing_In-context_Learning_for_Dynamic_Interest_Adaption_in_LLM-based_Recommendation_2025.findings-acl.735__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Customizing_In-context_Learning_for_Dynamic_Interest_Adaption_in_LLM-based_Recommendation_2025.findings-acl.735/Customizing_In-context_Learning_for_Dynamic_Interest_Adaption_in_LLM-based_Recommendation_2025.findings-acl.735__p3__score1.00.png",
        "caption": "Figure 4: Overview of our RecICL pipeline, primarily consists of three stages: Data Construction, Model Training, and Dynamic Interest Adaption. Here we define the few-shot number as 4."
      }
    ],
    "acl_id": "2025.findings-acl.735"
  },
  {
    "id": 29,
    "folder_name": "DEPN_Detecting_and_Editing_Privacy_Neurons_in_Pretrained_Language_Models_2023.emnlp-main.174",
    "title": "DEPN Detecting and Editing Privacy Neurons in Pretrained Language Models",
    "images": [
      {
        "filename": "DEPN_Detecting_and_Editing_Privacy_Neurons_in_Pretrained_Language_Models_2023.emnlp-main.174__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/DEPN_Detecting_and_Editing_Privacy_Neurons_in_Pretrained_Language_Models_2023.emnlp-main.174/DEPN_Detecting_and_Editing_Privacy_Neurons_in_Pretrained_Language_Models_2023.emnlp-main.174__p2__score1.00.png",
        "caption": "Figure 1: The diagram of DEPN. When a language model leaks privacy information, DEPN calculates privacy attribution scores using the Privacy Neuron Detector. It then selects the top z privacy neurons with the Privacy Neuron Aggregator and eliminates the model memorization of privacy information using the Privacy Editor."
      }
    ],
    "acl_id": "2023.emnlp-main.174"
  },
  {
    "id": 30,
    "folder_name": "DaCoM_Strategies_to_Construct_Domain-specific_Low-resource_Language_Machine_Translation_Dataset_2025.coling-industry.53",
    "title": "DaCoM Strategies to Construct Domain-specific Low-resource Language Machine Translation Dataset",
    "images": [
      {
        "filename": "DaCoM_Strategies_to_Construct_Domain-specific_Low-resource_Language_Machine_Translation_Dataset_2025.coling-industry.53__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/DaCoM_Strategies_to_Construct_Domain-specific_Low-resource_Language_Machine_Translation_Dataset_2025.coling-industry.53/DaCoM_Strategies_to_Construct_Domain-specific_Low-resource_Language_Machine_Translation_Dataset_2025.coling-industry.53__p2__score1.00.png",
        "caption": "Figure 1: Pipeline of DaCoM. In phase 1, PaLM2-unicorn and GNMT are used as LLM and NMT model. In phase 2, LaBSE is used as a sentence-transformer."
      }
    ],
    "acl_id": "2025.coling-industry.53"
  },
  {
    "id": 31,
    "folder_name": "Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256",
    "title": "Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations A Multimodal LLM-Based Detection and Rea",
    "images": [
      {
        "filename": "Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256__p1__score0.90.png",
        "path": "human_eval_dataset_dirs/Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256/Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256__p1__score0.90.png",
        "caption": "Figure 1: A conversation between Doctor and Patient, from our dataset with corresponding Emotion and Cognitive Distortion (CoD) Labels and Reasoning."
      },
      {
        "filename": "Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256/Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256__p3__score1.00.png",
        "caption": "Figure 2: Architectural diagram of our proposed framework, ZS-CoDR"
      },
      {
        "filename": "Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256__p7__score0.95.png",
        "path": "human_eval_dataset_dirs/Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256/Deciphering_Cognitive_Distortions_in_Patient-Doctor_Mental_Health_Conversations_A_Multimodal_LLM-Based_Detection_and_Rea_2024.emnlp-main.1256__p7__score0.95.png",
        "caption": "Figure 4: Comparisons among ground truth reasoning and reasoning generated by our model ZS-CoDR and zero-shot baseline ZERONLG. Additionally, we also generate resasoning using ZS-CoDR with only Tect modality. ZS-CoDR’s(multimodal) response is better aligned with the ground truth as it mentions the patient’s remark on the arrangement of letters and links it with cognitive distortion.ZS-CoDR’s( only Text) response falls short in comparison to the multimodal in terms of coherence with ground truth and clarity.While ZeroNLG’s response is more generic and not very informative."
      }
    ],
    "acl_id": "2024.emnlp-main.1256"
  },
  {
    "id": 32,
    "folder_name": "Distill-C_Enhanced_NL2SQL_via_Distilled_Customization_with_LLMs_2025.naacl-industry.64",
    "title": "Distill-C Enhanced NL2SQL via Distilled Customization with LLMs",
    "images": [
      {
        "filename": "Distill-C_Enhanced_NL2SQL_via_Distilled_Customization_with_LLMs_2025.naacl-industry.64__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Distill-C_Enhanced_NL2SQL_via_Distilled_Customization_with_LLMs_2025.naacl-industry.64/Distill-C_Enhanced_NL2SQL_via_Distilled_Customization_with_LLMs_2025.naacl-industry.64__p1__score1.00.png",
        "caption": "Figure 1: The Proposed Distill-C Framework."
      },
      {
        "filename": "Distill-C_Enhanced_NL2SQL_via_Distilled_Customization_with_LLMs_2025.naacl-industry.64__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Distill-C_Enhanced_NL2SQL_via_Distilled_Customization_with_LLMs_2025.naacl-industry.64/Distill-C_Enhanced_NL2SQL_via_Distilled_Customization_with_LLMs_2025.naacl-industry.64__p2__score1.00.png",
        "caption": "Figure 2: The Multi-Step Filtering Pipeline in our Distill-C Framework."
      }
    ],
    "acl_id": "2025.naacl-industry.64"
  },
  {
    "id": 33,
    "folder_name": "Do_Vision-Language_Models_Have_Internal_World_Models_Towards_an_Atomic_Evaluation_2025.findings-acl.1342",
    "title": "Do Vision-Language Models Have Internal World Models Towards an Atomic Evaluation",
    "images": [
      {
        "filename": "Do_Vision-Language_Models_Have_Internal_World_Models_Towards_an_Atomic_Evaluation_2025.findings-acl.1342__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Do_Vision-Language_Models_Have_Internal_World_Models_Towards_an_Atomic_Evaluation_2025.findings-acl.1342/Do_Vision-Language_Models_Have_Internal_World_Models_Towards_an_Atomic_Evaluation_2025.findings-acl.1342__p3__score1.00.png",
        "caption": "Figure 2: Overview of WM-ABench tasks. The Perception stage (top) covers Spatial, Temporal, Visual, Quantity, and Motion dimensions, each shown with example questions and outputs. The Prediction stage (bottom) includes Mechanistic Simulation, which covers Intuitive Physics (e.g., drop), Agent Navigation (e.g., turn left), and Agent Manipulation (e.g., push), plus Transitivity and Compositionality tasks that build on these transitions."
      }
    ],
    "acl_id": "2025.findings-acl.1342"
  },
  {
    "id": 34,
    "folder_name": "EMS-SD_Efficient_Multi-sample_Speculative_Decoding_for_Accelerating_Large_Language_Models_2025.naacl-long.471",
    "title": "EMS-SD Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models",
    "images": [
      {
        "filename": "EMS-SD_Efficient_Multi-sample_Speculative_Decoding_for_Accelerating_Large_Language_Models_2025.naacl-long.471__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/EMS-SD_Efficient_Multi-sample_Speculative_Decoding_for_Accelerating_Large_Language_Models_2025.naacl-long.471/EMS-SD_Efficient_Multi-sample_Speculative_Decoding_for_Accelerating_Large_Language_Models_2025.naacl-long.471__p2__score1.00.png",
        "caption": "Figure 2: Our Method v.s. Vanilla Method. We specify the location of the KV cache for each sample individually, thus eliminating the necessity for the addition of padding to the KV cache. And we concatenate all input tokens of each sample into a single sequence without padding tokens when the number of prediction tokens differs between samples. Our method demonstrates superior performance than the vanilla method, without the need for additional computational and memory access overhead."
      },
      {
        "filename": "EMS-SD_Efficient_Multi-sample_Speculative_Decoding_for_Accelerating_Large_Language_Models_2025.naacl-long.471__p3__score0.92.png",
        "path": "human_eval_dataset_dirs/EMS-SD_Efficient_Multi-sample_Speculative_Decoding_for_Accelerating_Large_Language_Models_2025.naacl-long.471/EMS-SD_Efficient_Multi-sample_Speculative_Decoding_for_Accelerating_Large_Language_Models_2025.naacl-long.471__p3__score0.92.png",
        "caption": "Figure 3: The detailed processing of unpad input tokens of decoding step 1 in Figure 2. Sample 0 predicted 5 tokens, while sample 1 predicted 2 tokens. All tokens are concatenated before inference, and the sample/sequence index is restored when attention is computed within the CUDA kernels. Consequently, each token is aware of the specific KV caches to which it can utilize for parallel computation."
      }
    ],
    "acl_id": "2025.naacl-long.471"
  },
  {
    "id": 35,
    "folder_name": "Efficient_Domain_Adaptation_for_Non-Autoregressive_Machine_Translation_2024.findings-acl.810",
    "title": "Efficient Domain Adaptation for Non-Autoregressive Machine Translation",
    "images": [
      {
        "filename": "Efficient_Domain_Adaptation_for_Non-Autoregressive_Machine_Translation_2024.findings-acl.810__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Efficient_Domain_Adaptation_for_Non-Autoregressive_Machine_Translation_2024.findings-acl.810/Efficient_Domain_Adaptation_for_Non-Autoregressive_Machine_Translation_2024.findings-acl.810__p3__score1.00.png",
        "caption": "Figure 1: Overview of our proposed Bi-kNN. ⇑ represents the process of bidirectional datastore construction and Meta-network training, while ↑ represents the process of iterative decoding with kNN."
      }
    ],
    "acl_id": "2024.findings-acl.810"
  },
  {
    "id": 36,
    "folder_name": "EmoCharacter_Evaluating_the_Emotional_Fidelity_of_Role-Playing_Agents_in_Dialogues_2025.naacl-long.316",
    "title": "EmoCharacter Evaluating the Emotional Fidelity of Role-Playing Agents in Dialogues",
    "images": [
      {
        "filename": "EmoCharacter_Evaluating_the_Emotional_Fidelity_of_Role-Playing_Agents_in_Dialogues_2025.naacl-long.316__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/EmoCharacter_Evaluating_the_Emotional_Fidelity_of_Role-Playing_Agents_in_Dialogues_2025.naacl-long.316/EmoCharacter_Evaluating_the_Emotional_Fidelity_of_Role-Playing_Agents_in_Dialogues_2025.naacl-long.316__p1__score1.00.png",
        "caption": "Figure 1: Framework of EmoCharacter. Left: format of evaluation tasks for assessing emotional fidelity in RPAs. Upper right: three evaluation settings. Lower right: metrics designed from both micro and macro perspectives."
      }
    ],
    "acl_id": "2025.naacl-long.316"
  },
  {
    "id": 37,
    "folder_name": "Empowering_Oneida_Language_Revitalization_Development_of_an_Oneida_Verb_Conjugator_2024.lrec-main.511",
    "title": "Empowering Oneida Language Revitalization Development of an Oneida Verb Conjugator",
    "images": [
      {
        "filename": "Empowering_Oneida_Language_Revitalization_Development_of_an_Oneida_Verb_Conjugator_2024.lrec-main.511__p0__score0.70.png",
        "path": "human_eval_dataset_dirs/Empowering_Oneida_Language_Revitalization_Development_of_an_Oneida_Verb_Conjugator_2024.lrec-main.511/Empowering_Oneida_Language_Revitalization_Development_of_an_Oneida_Verb_Conjugator_2024.lrec-main.511__p0__score0.70.png",
        "caption": "Figure 1: User interface of the Oneida verb conjugator (pilot version)"
      },
      {
        "filename": "Empowering_Oneida_Language_Revitalization_Development_of_an_Oneida_Verb_Conjugator_2024.lrec-main.511__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Empowering_Oneida_Language_Revitalization_Development_of_an_Oneida_Verb_Conjugator_2024.lrec-main.511/Empowering_Oneida_Language_Revitalization_Development_of_an_Oneida_Verb_Conjugator_2024.lrec-main.511__p1__score1.00.png",
        "caption": "Figure 2: Positions of elements that form Oneida verbs (Michelson and Doxtator, 2002, p. 14)"
      }
    ],
    "acl_id": "2024.lrec-main.511"
  },
  {
    "id": 38,
    "folder_name": "Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587",
    "title": "Enhancing Rhetorical Figure Annotation An Ontology-Based Web Application with RAG Integration",
    "images": [
      {
        "filename": "Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587__p4__score0.90.png",
        "path": "human_eval_dataset_dirs/Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587/Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587__p4__score0.90.png",
        "caption": "Figure 2: Scheme of the SQL Lite Database. The arrows indicate foreign key (FK) relations. PK denotes primary keys."
      },
      {
        "filename": "Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587__p5__score0.60.png",
        "path": "human_eval_dataset_dirs/Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587/Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587__p5__score0.60.png",
        "caption": "Figure 3: The page FyF.html helps users to find the name of a rhetorical figure hidden in a text. Properties can be selected from the dropdown lists."
      },
      {
        "filename": "Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587__p6__score1.00.png",
        "path": "human_eval_dataset_dirs/Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587/Enhancing_Rhetorical_Figure_Annotation_An_Ontology-Based_Web_Application_with_RAG_Integration_2025.coling-main.587__p6__score1.00.png",
        "caption": "Figure 5: Overview of the integrated RAG Pipeline for the GRhOOT ontology."
      }
    ],
    "acl_id": "2025.coling-main.587"
  },
  {
    "id": 39,
    "folder_name": "Entity_Pair-guided_Relation_Summarization_and_Retrieval_in_LLMs_for_Document-level_Relation_Extraction_2025.findings-naacl.224",
    "title": "Entity Pair-guided Relation Summarization and Retrieval in LLMs for Document-level Relation Extraction",
    "images": [
      {
        "filename": "Entity_Pair-guided_Relation_Summarization_and_Retrieval_in_LLMs_for_Document-level_Relation_Extraction_2025.findings-naacl.224__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Entity_Pair-guided_Relation_Summarization_and_Retrieval_in_LLMs_for_Document-level_Relation_Extraction_2025.findings-naacl.224/Entity_Pair-guided_Relation_Summarization_and_Retrieval_in_LLMs_for_Document-level_Relation_Extraction_2025.findings-naacl.224__p0__score0.95.png",
        "caption": "Figure 1: Differences between traditional approach and our approach in LLM-based document-level relation extraction. Additionally, we provide a preliminary comparison of the F1 scores between the document-level and entity-pair-level candidate relation filtering methods."
      },
      {
        "filename": "Entity_Pair-guided_Relation_Summarization_and_Retrieval_in_LLMs_for_Document-level_Relation_Extraction_2025.findings-naacl.224__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Entity_Pair-guided_Relation_Summarization_and_Retrieval_in_LLMs_for_Document-level_Relation_Extraction_2025.findings-naacl.224/Entity_Pair-guided_Relation_Summarization_and_Retrieval_in_LLMs_for_Document-level_Relation_Extraction_2025.findings-naacl.224__p3__score1.00.png",
        "caption": "Figure 2: The overview of our model EP-RSR. It contains three key parts: (1) Select potential entity pairs in a document and obtain enhanced relation summarization for each entity pair. (2) Retrieve entity-pair-level candidate relations using the relation summarizations based on double filtering mechanisms. (3) Judge and extract triplet facts based on the candidate relations and their descriptions."
      }
    ],
    "acl_id": "2025.findings-naacl.224"
  },
  {
    "id": 40,
    "folder_name": "Estimation_of_Text_Difficulty_in_the_Context_of_Language_Learning_2025.bea-1.43",
    "title": "Estimation of Text Difficulty in the Context of Language Learning",
    "images": [
      {
        "filename": "Estimation_of_Text_Difficulty_in_the_Context_of_Language_Learning_2025.bea-1.43__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/Estimation_of_Text_Difficulty_in_the_Context_of_Language_Learning_2025.bea-1.43/Estimation_of_Text_Difficulty_in_the_Context_of_Language_Learning_2025.bea-1.43__p0__score1.00.png",
        "caption": "Figure 1: Text simplification using GPT-4o guided by level-aware feedback from a difficulty classifier as critic."
      }
    ],
    "acl_id": "2025.bea-1.43"
  },
  {
    "id": 41,
    "folder_name": "Evaluating_Defeasible_Reasoning_in_LLMs_with_DEFREASING_2025.naacl-long.529",
    "title": "Evaluating Defeasible Reasoning in LLMs with DEFREASING",
    "images": [
      {
        "filename": "Evaluating_Defeasible_Reasoning_in_LLMs_with_DEFREASING_2025.naacl-long.529__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Evaluating_Defeasible_Reasoning_in_LLMs_with_DEFREASING_2025.naacl-long.529/Evaluating_Defeasible_Reasoning_in_LLMs_with_DEFREASING_2025.naacl-long.529__p0__score0.95.png",
        "caption": "Figure 1: Overview of the defeasible reasoning task."
      },
      {
        "filename": "Evaluating_Defeasible_Reasoning_in_LLMs_with_DEFREASING_2025.naacl-long.529__p6__score0.80.png",
        "path": "human_eval_dataset_dirs/Evaluating_Defeasible_Reasoning_in_LLMs_with_DEFREASING_2025.naacl-long.529/Evaluating_Defeasible_Reasoning_in_LLMs_with_DEFREASING_2025.naacl-long.529__p6__score0.80.png",
        "caption": "Table 2: Example prompt format. The system instruction is in italics above the dashed line."
      }
    ],
    "acl_id": "2025.naacl-long.529"
  },
  {
    "id": 42,
    "folder_name": "Evaluating_Numeracy_of_Language_Models_as_a_Natural_Language_Inference_Task_2025.findings-naacl.467",
    "title": "Evaluating Numeracy of Language Models as a Natural Language Inference Task",
    "images": [
      {
        "filename": "Evaluating_Numeracy_of_Language_Models_as_a_Natural_Language_Inference_Task_2025.findings-naacl.467__p5__score0.60.png",
        "path": "human_eval_dataset_dirs/Evaluating_Numeracy_of_Language_Models_as_a_Natural_Language_Inference_Task_2025.findings-naacl.467/Evaluating_Numeracy_of_Language_Models_as_a_Natural_Language_Inference_Task_2025.findings-naacl.467__p5__score0.60.png",
        "caption": "Table 3: Template and formula in arithmetic task. Instances I1 and I2 use the same template (T1). Instances I3 and I4 share the same formula (F1), while I2 and I4 have F2"
      }
    ],
    "acl_id": "2025.findings-naacl.467"
  },
  {
    "id": 43,
    "folder_name": "Evaluating_and_Enhancing_Large_Language_Models_for_Novelty_Assessment_in_Scholarly_Publications_2025.aisd-main.5",
    "title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
    "images": [
      {
        "filename": "Evaluating_and_Enhancing_Large_Language_Models_for_Novelty_Assessment_in_Scholarly_Publications_2025.aisd-main.5__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Evaluating_and_Enhancing_Large_Language_Models_for_Novelty_Assessment_in_Scholarly_Publications_2025.aisd-main.5/Evaluating_and_Enhancing_Large_Language_Models_for_Novelty_Assessment_in_Scholarly_Publications_2025.aisd-main.5__p3__score1.00.png",
        "caption": "Figure 1: The overview of RAG-Novelty"
      }
    ],
    "acl_id": "2025.aisd-main.5"
  },
  {
    "id": 44,
    "folder_name": "Exploring_the_Integration_of_Eye_Movement_Data_on_Word_Embeddings_2025.cmcl-1.9",
    "title": "Exploring the Integration of Eye Movement Data on Word Embeddings",
    "images": [
      {
        "filename": "Exploring_the_Integration_of_Eye_Movement_Data_on_Word_Embeddings_2025.cmcl-1.9__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Exploring_the_Integration_of_Eye_Movement_Data_on_Word_Embeddings_2025.cmcl-1.9/Exploring_the_Integration_of_Eye_Movement_Data_on_Word_Embeddings_2025.cmcl-1.9__p1__score1.00.png",
        "caption": "Figure 1: Gaze embedding pipeline. The stories read during the eye-tracking experiment were reconstructed following the reading order of the participants (Scanpaths). Gaze measures were extracted from all trials, discretized in ten bins for each individual, and a global average for each word was computed. These values were then predicted from the word embeddings as the output of a fully connected layer."
      }
    ],
    "acl_id": "2025.cmcl-1.9"
  },
  {
    "id": 45,
    "folder_name": "FGDGNN_Fine-Grained_Dynamic_Graph_Neural_Network_for_Rumor_Detection_on_Social_Media_2025.findings-acl.296",
    "title": "FGDGNN Fine-Grained Dynamic Graph Neural Network for Rumor Detection on Social Media",
    "images": [
      {
        "filename": "FGDGNN_Fine-Grained_Dynamic_Graph_Neural_Network_for_Rumor_Detection_on_Social_Media_2025.findings-acl.296__p0__score0.90.png",
        "path": "human_eval_dataset_dirs/FGDGNN_Fine-Grained_Dynamic_Graph_Neural_Network_for_Rumor_Detection_on_Social_Media_2025.findings-acl.296/FGDGNN_Fine-Grained_Dynamic_Graph_Neural_Network_for_Rumor_Detection_on_Social_Media_2025.findings-acl.296__p0__score0.90.png",
        "caption": "Figure 1: An example of event propagation graph on social media. (a) Static graph. Each node represents a post and each edge represents the response relationship without temporal information. (b) Dynamic graph. Each node represents a post and each edge represents the response relationship with an associated temporal information. The dynamic propagation process in the example is divided into S snapshots."
      },
      {
        "filename": "FGDGNN_Fine-Grained_Dynamic_Graph_Neural_Network_for_Rumor_Detection_on_Social_Media_2025.findings-acl.296__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/FGDGNN_Fine-Grained_Dynamic_Graph_Neural_Network_for_Rumor_Detection_on_Social_Media_2025.findings-acl.296/FGDGNN_Fine-Grained_Dynamic_Graph_Neural_Network_for_Rumor_Detection_on_Social_Media_2025.findings-acl.296__p3__score1.00.png",
        "caption": "Figure 2: Overview of the proposed FGDGNN framework. The Embedding Transformation (ET) Layer represents the transformation of the node embedding dimensions."
      }
    ],
    "acl_id": "2025.findings-acl.296"
  },
  {
    "id": 46,
    "folder_name": "FLEURS-ASL_Including_American_Sign_Language_in_Massively_Multilingual_Multitask_Evaluation_2025.naacl-long.314",
    "title": "FLEURS-ASL Including American Sign Language in Massively Multilingual Multitask Evaluation",
    "images": [
      {
        "filename": "FLEURS-ASL_Including_American_Sign_Language_in_Massively_Multilingual_Multitask_Evaluation_2025.naacl-long.314__p3__score0.90.png",
        "path": "human_eval_dataset_dirs/FLEURS-ASL_Including_American_Sign_Language_in_Massively_Multilingual_Multitask_Evaluation_2025.naacl-long.314/FLEURS-ASL_Including_American_Sign_Language_in_Massively_Multilingual_Multitask_Evaluation_2025.naacl-long.314__p3__score0.90.png",
        "caption": "Figure 1: FLEURS-ASL dataset splits. The sentences are divided among 5 interpreters, and 3 sets of splits: zeroshot (“zs”), signer-independent finetuning (“si”), and signer-dependent finetuning (“sd”). We blur the interpreters’ faces in this paper for privacy, but the underlying dataset is unblurred because facial expressions are an essential component of the grammar of sign languages."
      },
      {
        "filename": "FLEURS-ASL_Including_American_Sign_Language_in_Massively_Multilingual_Multitask_Evaluation_2025.naacl-long.314__p5__score1.00.png",
        "path": "human_eval_dataset_dirs/FLEURS-ASL_Including_American_Sign_Language_in_Massively_Multilingual_Multitask_Evaluation_2025.naacl-long.314/FLEURS-ASL_Including_American_Sign_Language_in_Massively_Multilingual_Multitask_Evaluation_2025.naacl-long.314__p5__score1.00.png",
        "caption": "Figure 2: Unified multitask document-level sign to text training; see Figure 3 in Appendix D for a complete exposition. Our baselines train on YouTube-ASL and evaluate on FLEURS-ASL."
      }
    ],
    "acl_id": "2025.naacl-long.314"
  },
  {
    "id": 47,
    "folder_name": "FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48",
    "title": "FinDABench Benchmarking Financial Data Analysis Ability of Large Language Models",
    "images": [
      {
        "filename": "FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48/FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48__p0__score0.95.png",
        "caption": "Figure 1: The job skills and their corresponding task names required for financial analysts to manage daily work. Text highlighted in green denotes the standard capabilities of financial analysts."
      },
      {
        "filename": "FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48__p1__score0.95.png",
        "path": "human_eval_dataset_dirs/FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48/FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48__p1__score0.95.png",
        "caption": "Figure 2: FinDABench aims to provide a multi-faceted evaluation framework that mirrors the multifarious nature of financial data analysis tasks."
      },
      {
        "filename": "FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48__p3__score0.90.png",
        "path": "human_eval_dataset_dirs/FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48/FinDABench_Benchmarking_Financial_Data_Analysis_Ability_of_Large_Language_Models_2025.coling-main.48__p3__score0.90.png",
        "caption": "Figure 3: Data examples for the six sub-tasks of FinDABench, each including questions and answers with a unique identifier to facilitate differentiation. For the Chinese version, please see the Appendix A."
      }
    ],
    "acl_id": "2025.coling-main.48"
  },
  {
    "id": 48,
    "folder_name": "Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330",
    "title": "Find the Intention of Instruction Comprehensive Evaluation of Instruction Understanding for Large Language Models",
    "images": [
      {
        "filename": "Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330__p0__score0.90.png",
        "path": "human_eval_dataset_dirs/Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330/Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330__p0__score0.90.png",
        "caption": "Figure 1: Simplified example of IOINST. We compose a benchmark designed to comprehend and select the appropriate instruction that derives given response. Potential error cases include misunderstanding prerequisites of context and responding to any candidate instruction."
      },
      {
        "filename": "Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330/Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330__p3__score1.00.png",
        "caption": "Figure 2: Construction of pool-based contrastive instructions. From the pre-processed data point obtained by the data curation, we establish our instruction candidates."
      },
      {
        "filename": "Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330__p4__score1.00.png",
        "path": "human_eval_dataset_dirs/Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330/Find_the_Intention_of_Instruction_Comprehensive_Evaluation_of_Instruction_Understanding_for_Large_Language_Models_2025.findings-naacl.330__p4__score1.00.png",
        "caption": "Figure 3: Construction of Anti-Attribute contrastive instructions. From the pre-processed data point obtained by the data curation, we establish our instruction candidates."
      }
    ],
    "acl_id": "2025.findings-naacl.330"
  },
  {
    "id": 49,
    "folder_name": "FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6",
    "title": "FlagEvalMM A Flexible Framework for Comprehensive Multimodal Model Evaluation",
    "images": [
      {
        "filename": "FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6/FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6__p0__score1.00.png",
        "caption": "Figure 1: Framework of FlagEvalMM"
      },
      {
        "filename": "FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6/FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6__p2__score1.00.png",
        "caption": "Figure 2: Components and workflow of the evaluation server"
      },
      {
        "filename": "FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6/FlagEvalMM_A_Flexible_Framework_for_Comprehensive_Multimodal_Model_Evaluation_2025.acl-demo.6__p3__score1.00.png",
        "caption": "Figure 3: Communication protocol between evaluation server and model runner"
      }
    ],
    "acl_id": "2025.acl-demo.6"
  },
  {
    "id": 50,
    "folder_name": "From_English_to_Second_Language_Mastery_Enhancing_LLMs_with_Cross-Lingual_Continued_Instruction_Tuning_2025.acl-long.1121",
    "title": "From English to Second Language Mastery Enhancing LLMs with Cross-Lingual Continued Instruction Tuning",
    "images": [
      {
        "filename": "From_English_to_Second_Language_Mastery_Enhancing_LLMs_with_Cross-Lingual_Continued_Instruction_Tuning_2025.acl-long.1121__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/From_English_to_Second_Language_Mastery_Enhancing_LLMs_with_Cross-Lingual_Continued_Instruction_Tuning_2025.acl-long.1121/From_English_to_Second_Language_Mastery_Enhancing_LLMs_with_Cross-Lingual_Continued_Instruction_Tuning_2025.acl-long.1121__p2__score1.00.png",
        "caption": "Figure 1: The pipeline of our Cross-lingual Continued Instruction Tuning (X-CIT) method. Guided by Chomsky’s Principles: 1⃝ SFT the base LLM with English instruction data to establish foundational capabilities; 2⃝ continue training with the target language and customized chat-instruction data to adjust language-specific parameters. Self-paced learning (SPL) is introduced to further mimic the human learning process, moving from simple to complex tasks. For clarity, the method using SPL is referred to as X-CIT+spl."
      }
    ],
    "acl_id": "2025.acl-long.1121"
  },
  {
    "id": 51,
    "folder_name": "From_Priest_to_Doctor_Domain_Adaptation_for_Low-Resource_Neural_Machine_Translation_2025.coling-main.472",
    "title": "From Priest to Doctor Domain Adaptation for Low-Resource Neural Machine Translation",
    "images": [
      {
        "filename": "From_Priest_to_Doctor_Domain_Adaptation_for_Low-Resource_Neural_Machine_Translation_2025.coling-main.472__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/From_Priest_to_Doctor_Domain_Adaptation_for_Low-Resource_Neural_Machine_Translation_2025.coling-main.472/From_Priest_to_Doctor_Domain_Adaptation_for_Low-Resource_Neural_Machine_Translation_2025.coling-main.472__p0__score0.95.png",
        "caption": "Figure 1: In our work, which looks at the (previously neglected) intersection of low-Resource NMT and domain adaptation in NMT, we consider only these commonly accessible resources."
      }
    ],
    "acl_id": "2025.coling-main.472"
  },
  {
    "id": 52,
    "folder_name": "GEMINI_Controlling_The_Sentence-Level_Summary_Style_in_Abstractive_Text_Summarization_2023.emnlp-main.53",
    "title": "GEMINI Controlling The Sentence-Level Summary Style in Abstractive Text Summarization",
    "images": [
      {
        "filename": "GEMINI_Controlling_The_Sentence-Level_Summary_Style_in_Abstractive_Text_Summarization_2023.emnlp-main.53__p0__score0.90.png",
        "path": "human_eval_dataset_dirs/GEMINI_Controlling_The_Sentence-Level_Summary_Style_in_Abstractive_Text_Summarization_2023.emnlp-main.53/GEMINI_Controlling_The_Sentence-Level_Summary_Style_in_Abstractive_Text_Summarization_2023.emnlp-main.53__p0__score0.90.png",
        "caption": "Figure 1: Example of human summary from CNN/DM."
      },
      {
        "filename": "GEMINI_Controlling_The_Sentence-Level_Summary_Style_in_Abstractive_Text_Summarization_2023.emnlp-main.53__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/GEMINI_Controlling_The_Sentence-Level_Summary_Style_in_Abstractive_Text_Summarization_2023.emnlp-main.53/GEMINI_Controlling_The_Sentence-Level_Summary_Style_in_Abstractive_Text_Summarization_2023.emnlp-main.53__p3__score1.00.png",
        "caption": "Figure 2: GEMINI uses a controller to decide ext/abs styles, and further switch the decoder accordingly between a rewriter and a generator. We express the identifier tokens “<S>” and “<Sk>” in the concise form “S” and “Sk”, and we use “w1 w2” to represent the tokens in the first sentence and “w3 w4” the tokens in the second sentence. Group tags are converted into embeddings and added to the input token embeddings for both the encoder and decoder. The decoder predicts an identifier token to determine the group tag of the following timesteps, for example “<S2>” to start the group tag “2” until the end of the sentence “</S>”."
      }
    ],
    "acl_id": "2023.emnlp-main.53"
  },
  {
    "id": 53,
    "folder_name": "Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458",
    "title": "Gradient-guided Attention Map Editing Towards Efficient Contextual Hallucination Mitigation",
    "images": [
      {
        "filename": "Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p1__score1.00.png",
        "caption": "Figure 1: Illustration of a decoder-only Transformer featuring a multi-head attention mechanism. Each row in an attention map represents a weight vector that sums to one, reflecting the current token’s relationship with preceding tokens. A deeper color indicates a higher attention weight."
      },
      {
        "filename": "Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p2__score0.95.png",
        "path": "human_eval_dataset_dirs/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p2__score0.95.png",
        "caption": "Figure 2: Derivation of the LR at the tth decoding step."
      },
      {
        "filename": "Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p3__score0.92.png",
        "path": "human_eval_dataset_dirs/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p3__score0.92.png",
        "caption": "Figure 3: Utilization of the positional-based decay prior attention bias."
      },
      {
        "filename": "Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p4__score0.80.png",
        "path": "human_eval_dataset_dirs/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p4__score0.80.png",
        "caption": "Figure 6: Illustrated example for the combination of prior attention bias and edit direction to perform attention editing."
      },
      {
        "filename": "Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p4__score1.00.png",
        "path": "human_eval_dataset_dirs/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p4__score1.00.png",
        "caption": "Figure 4: Illustrated example on the generation of one chunk of output in GAME. Step ①: the LLM predicts the next chunk (Y2) and calculates the chunk attention feature (v̄2) without any attention editing. Step ②: the classifier (F ) predicts the hallucination score (c) for the generated chunk with the corresponding feature. If the score exceeds a predefined threshold, the chunk will be accepted. Otherwise, attention editing will be applied to regenerate the chunk. Step ③: the attention edit signal for each head is computed with the prior bias and the edit direction ∆ derived from the gradient of the score. Step ④: a new chunk is generated with the calculated attention editing signal and re-evaluated with the classifier. If no qualified chunk is accepted with number of regeneration attempts, the chunk with the highest score during the generation process will be accepted."
      },
      {
        "filename": "Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p4__score1.00__1.png",
        "path": "human_eval_dataset_dirs/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458__p4__score1.00__1.png",
        "caption": "Figure 5: The training data construction and training process of lookback lens."
      }
    ],
    "acl_id": "2025.findings-naacl.458"
  },
  {
    "id": 54,
    "folder_name": "HaluEval_A_Large-Scale_Hallucination_Evaluation_Benchmark_for_Large_Language_Models_2023.emnlp-main.397",
    "title": "HaluEval A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "images": [
      {
        "filename": "HaluEval_A_Large-Scale_Hallucination_Evaluation_Benchmark_for_Large_Language_Models_2023.emnlp-main.397__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/HaluEval_A_Large-Scale_Hallucination_Evaluation_Benchmark_for_Large_Language_Models_2023.emnlp-main.397/HaluEval_A_Large-Scale_Hallucination_Evaluation_Benchmark_for_Large_Language_Models_2023.emnlp-main.397__p1__score1.00.png",
        "caption": "Figure 1: Construction pipeline of HaluEval, including automatic generation (top) and human annotation (bottom)."
      },
      {
        "filename": "HaluEval_A_Large-Scale_Hallucination_Evaluation_Benchmark_for_Large_Language_Models_2023.emnlp-main.397__p3__score0.90.png",
        "path": "human_eval_dataset_dirs/HaluEval_A_Large-Scale_Hallucination_Evaluation_Benchmark_for_Large_Language_Models_2023.emnlp-main.397/HaluEval_A_Large-Scale_Hallucination_Evaluation_Benchmark_for_Large_Language_Models_2023.emnlp-main.397__p3__score0.90.png",
        "caption": "Table 3: Instruction of hallucination filtering for question answering."
      }
    ],
    "acl_id": "2023.emnlp-main.397"
  },
  {
    "id": 55,
    "folder_name": "Harnessing_NLP_for_Indigenous_Language_Education_Fine-Tuning_Large_Language_Models_for_Sentence_Transformation_2025.americasnlp-1.14",
    "title": "Harnessing NLP for Indigenous Language Education Fine-Tuning Large Language Models for Sentence Transformation",
    "images": [
      {
        "filename": "Harnessing_NLP_for_Indigenous_Language_Education_Fine-Tuning_Large_Language_Models_for_Sentence_Transformation_2025.americasnlp-1.14__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Harnessing_NLP_for_Indigenous_Language_Education_Fine-Tuning_Large_Language_Models_for_Sentence_Transformation_2025.americasnlp-1.14/Harnessing_NLP_for_Indigenous_Language_Education_Fine-Tuning_Large_Language_Models_for_Sentence_Transformation_2025.americasnlp-1.14__p3__score1.00.png",
        "caption": "Figure 1: Methodological Workflow for Sentence Transformation in Indigenous Language Education Using Large Language Models"
      }
    ],
    "acl_id": "2025.americasnlp-1.14"
  },
  {
    "id": 56,
    "folder_name": "Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3",
    "title": "Health Sentinel An AI Pipeline For Real-time Disease Outbreak Detection",
    "images": [
      {
        "filename": "Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3/Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3__p0__score0.95.png",
        "caption": "Figure 1: Health Sentinel extracts structured information from online articles reporting unusual health events. The given example shows how our pipeline extracts multiple events from a single news article."
      },
      {
        "filename": "Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3/Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3__p3__score1.00.png",
        "caption": "Figure 2: System Overview of Health Sentinel. Health Sentinel combines rule based and ML techniques alongside a human-in-the-loop system to ensure a high level of reliance and efficiency. Its data ingestion pipeline continuously collects news articles from the web and stores them in a database. The article processing pipeline retrieves these articles, filters out irrelevant data, and extracts health events. The extracted events are then sent for expert review before publication for ground-level action."
      },
      {
        "filename": "Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3__p6__score0.80.png",
        "path": "human_eval_dataset_dirs/Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3/Health_Sentinel_An_AI_Pipeline_For_Real-time_Disease_Outbreak_Detection_2025.nlp4pi-1.3__p6__score0.80.png",
        "caption": "Table 3: Qualitative comparison of event extraction by GPT-4o-Mini and QA+NLI pipelines. In example 1, the LLM-based pipeline identifies a disease missed by QA+NLI. In Example 2, it filters out an irrelevant international event mistakenly extracted by QA+NLI. Example 3 shows the LLM capturing an illness caused by contaminated water, which QA+NLI misses. In the final example, it excludes the article lacking an infectious disease component, unlike QA+NLI. Overall, LLM’s inherent knowledge enables more accurate event extraction and contextual filtering of articles."
      }
    ],
    "acl_id": "2025.nlp4pi-1.3"
  },
  {
    "id": 57,
    "folder_name": "Howard_University-AI4PC_at_SemEval-2025_Task_9_Using_Open-weight_BART-MNLI_for_Zero_Shot_Classification_of_Food_Recall_D_2025.semeval-1.250",
    "title": "Howard University-AI4PC at SemEval-2025 Task 9 Using Open-weight BART-MNLI for Zero Shot Classification of Food Recall D",
    "images": [
      {
        "filename": "Howard_University-AI4PC_at_SemEval-2025_Task_9_Using_Open-weight_BART-MNLI_for_Zero_Shot_Classification_of_Food_Recall_D_2025.semeval-1.250__p1__score0.80.png",
        "path": "human_eval_dataset_dirs/Howard_University-AI4PC_at_SemEval-2025_Task_9_Using_Open-weight_BART-MNLI_for_Zero_Shot_Classification_of_Food_Recall_D_2025.semeval-1.250/Howard_University-AI4PC_at_SemEval-2025_Task_9_Using_Open-weight_BART-MNLI_for_Zero_Shot_Classification_of_Food_Recall_D_2025.semeval-1.250__p1__score0.80.png",
        "caption": "Figure 1: Model inputs in Blue and ground truth in Orange."
      },
      {
        "filename": "Howard_University-AI4PC_at_SemEval-2025_Task_9_Using_Open-weight_BART-MNLI_for_Zero_Shot_Classification_of_Food_Recall_D_2025.semeval-1.250__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Howard_University-AI4PC_at_SemEval-2025_Task_9_Using_Open-weight_BART-MNLI_for_Zero_Shot_Classification_of_Food_Recall_D_2025.semeval-1.250/Howard_University-AI4PC_at_SemEval-2025_Task_9_Using_Open-weight_BART-MNLI_for_Zero_Shot_Classification_of_Food_Recall_D_2025.semeval-1.250__p2__score1.00.png",
        "caption": "Figure 2: Proposed model approach"
      }
    ],
    "acl_id": "2025.semeval-1.250"
  },
  {
    "id": 58,
    "folder_name": "In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109",
    "title": "In-Context Analogical Reasoning with Pre-Trained Language Models",
    "images": [
      {
        "filename": "In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p0__score1.00.png",
        "caption": "Figure 1: Raven’s Progressive Matrices (Raven and Court, 1938; Zhang et al., 2019a) are an analogy-making task where one must infer the missing matrix item based on abstract rules instantiated in the first two rows. To demonstrate the potential analogical reasoning skills in pre-trained language models, we develop languagebased abstractions over their key perceptual features, then prompt them to select the completion of the matrix."
      },
      {
        "filename": "In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p2__score1.00.png",
        "caption": "Figure 2: Illustration of the compositional nature of entities, layouts, and component structures in RAVEN, and their unique attributes. We provide example items from sub-tasks each item type appears in."
      },
      {
        "filename": "In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p3__score0.95.png",
        "path": "human_eval_dataset_dirs/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p3__score0.95.png",
        "caption": "Figure 4: Example of generated entity layout encodings when abstracting position and number, and summarizing redundant entity attributes within the layout."
      },
      {
        "filename": "In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p3__score1.00.png",
        "caption": "Figure 3: Example generated prompts for a complete RPM under entity attribute naming (left) and decomposition (right) abstractions in the Center sub-task."
      },
      {
        "filename": "In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p5__score0.98.png",
        "path": "human_eval_dataset_dirs/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109/In-Context_Analogical_Reasoning_with_Pre-Trained_Language_Models_2023.acl-long.109__p5__score0.98.png",
        "caption": "Figure 5: Quasi-image abstractions for a triangle and pentagon of different size and color."
      }
    ],
    "acl_id": "2023.acl-long.109"
  },
  {
    "id": 59,
    "folder_name": "Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159",
    "title": "Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding",
    "images": [
      {
        "filename": "Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159/Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159__p0__score1.00.png",
        "caption": "Figure 1: The blue section on the left signifies the conversational dense retrieval, and the green section on the right provides an overview of CONVINV."
      },
      {
        "filename": "Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159/Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159__p3__score1.00.png",
        "caption": "Figure 2: Architecture of our proposed CONVINV."
      },
      {
        "filename": "Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159__p4__score1.00.png",
        "path": "human_eval_dataset_dirs/Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159/Interpreting_Conversational_Dense_Retrieval_by_Rewriting-Enhanced_Inversion_of_Session_Embedding_2024.acl-long.159__p4__score1.00.png",
        "caption": "Figure 3: The workflow of UniCRR (Unifying Conversational Dense Retrieval and Query Rewriting)."
      }
    ],
    "acl_id": "2024.acl-long.159"
  },
  {
    "id": 60,
    "folder_name": "Investigating_Bias_in_Multilingual_Language_Models_Cross-Lingual_Transfer_of_Debiasing_Techniques_2023.emnlp-main.175",
    "title": "Investigating Bias in Multilingual Language Models Cross-Lingual Transfer of Debiasing Techniques",
    "images": [
      {
        "filename": "Investigating_Bias_in_Multilingual_Language_Models_Cross-Lingual_Transfer_of_Debiasing_Techniques_2023.emnlp-main.175__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Investigating_Bias_in_Multilingual_Language_Models_Cross-Lingual_Transfer_of_Debiasing_Techniques_2023.emnlp-main.175/Investigating_Bias_in_Multilingual_Language_Models_Cross-Lingual_Transfer_of_Debiasing_Techniques_2023.emnlp-main.175__p0__score0.95.png",
        "caption": "Figure 1: The example of the English CrowS-Pairs dataset illustrates sentence probabilities after debiasing mBERT with SentenceDebias in English, French, German, and Dutch."
      }
    ],
    "acl_id": "2023.emnlp-main.175"
  },
  {
    "id": 61,
    "folder_name": "Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136",
    "title": "Iterative Document-level Information Extraction via Imitation Learning",
    "images": [
      {
        "filename": "Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136__p0__score0.80.png",
        "path": "human_eval_dataset_dirs/Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136/Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136__p0__score0.80.png",
        "caption": "Figure 1: An example of multi-template extraction on a document (an NLP paper; Lei et al. (2018)) from the SCIREX dataset. An agent reads the entire paper and iteratively generates templates, each consisting of slots for Task, Method, Dataset, and Metric."
      },
      {
        "filename": "Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136__p1__score0.95.png",
        "path": "human_eval_dataset_dirs/Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136/Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136__p1__score0.95.png",
        "caption": "Figure 2: Template examples from MUC-4 (left) and BETTER Granular (right) datasets. Event triggers (e.g. burned above) are not annotated in MUC-4 and are highlighted here only for clarity."
      },
      {
        "filename": "Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136/Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136__p2__score1.00.png",
        "caption": "Figure 3: The basic iteration step of ITERX (left box), and an unrolled version on SCIREX 4-ary relation extraction (extraction of templates in the form {Task, Method, Dataset, Metric}) executed on the NLP paper Bidirectional Attention Flow for Machine Comprehension (Seo et al., 2017). Span embeddings (X(0) ) are passed as input to the first step, where the model extracts the template {Task: Machine comprehension, Question answering; Method: BiDAF; Dataset: SQuAD; Metric: Exact match}. This information is propagated via our memory mechanism to the second step, and informs prediction of the next template: {Task: Machine comprehension; Method: BiDAF; Dataset: CNN/DailyMail; Metric: Accuracy}. The third step assigns the null slot type Y to all spans, indicating that the model is unable to find any further templates, thus stopping the generation process."
      },
      {
        "filename": "Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136__p5__score0.60.png",
        "path": "human_eval_dataset_dirs/Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136/Iterative_Document-level_Information_Extraction_via_Imitation_Learning_2023.eacl-main.136__p5__score0.60.png",
        "caption": "Figure 4: A comparison of the metrics discussed. Features in blue are “desired” for the evaluation of our task."
      }
    ],
    "acl_id": "2023.eacl-main.136"
  },
  {
    "id": 62,
    "folder_name": "JU-CSE-NLPs_Cascaded_Speech_to_Text_Translation_Systems_for_IWSLT_2025_in_Indic_Track_2025.iwslt-1.18",
    "title": "JU-CSE-NLPs Cascaded Speech to Text Translation Systems for IWSLT 2025 in Indic Track",
    "images": [
      {
        "filename": "JU-CSE-NLPs_Cascaded_Speech_to_Text_Translation_Systems_for_IWSLT_2025_in_Indic_Track_2025.iwslt-1.18__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/JU-CSE-NLPs_Cascaded_Speech_to_Text_Translation_Systems_for_IWSLT_2025_in_Indic_Track_2025.iwslt-1.18/JU-CSE-NLPs_Cascaded_Speech_to_Text_Translation_Systems_for_IWSLT_2025_in_Indic_Track_2025.iwslt-1.18__p2__score1.00.png",
        "caption": "Figure 1: Overview of the proposed Multilingual Speech Translation Pipeline: (a) English-to-Indic flow using Whisper and finetuned NLLB-200; (b) Indic-to-English flow using IndicConformer and finetuned IndicTrans2."
      }
    ],
    "acl_id": "2025.iwslt-1.18"
  },
  {
    "id": 63,
    "folder_name": "KEC_AI_DATA_DRIFTERSDravidianLangTech_2025_Fake_News_Detection_in_Dravidian_Languages_2025.dravidianlangtech-1.29",
    "title": "KEC AI DATA DRIFTERSDravidianLangTech 2025 Fake News Detection in Dravidian Languages",
    "images": [
      {
        "filename": "KEC_AI_DATA_DRIFTERSDravidianLangTech_2025_Fake_News_Detection_in_Dravidian_Languages_2025.dravidianlangtech-1.29__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/KEC_AI_DATA_DRIFTERSDravidianLangTech_2025_Fake_News_Detection_in_Dravidian_Languages_2025.dravidianlangtech-1.29/KEC_AI_DATA_DRIFTERSDravidianLangTech_2025_Fake_News_Detection_in_Dravidian_Languages_2025.dravidianlangtech-1.29__p2__score1.00.png",
        "caption": "Figure 1: Proposed System Workflow"
      }
    ],
    "acl_id": "2025.dravidianlangtech-1.29"
  },
  {
    "id": 64,
    "folder_name": "LLM-Assisted_Iterative_Curriculum_Writing_A_Human-Centered_AI_Approach_in_Finnish_Higher_Education_2025.bea-1.76",
    "title": "LLM-Assisted, Iterative Curriculum Writing A Human-Centered AI Approach in Finnish Higher Education",
    "images": [
      {
        "filename": "LLM-Assisted_Iterative_Curriculum_Writing_A_Human-Centered_AI_Approach_in_Finnish_Higher_Education_2025.bea-1.76__p4__score0.80.png",
        "path": "human_eval_dataset_dirs/LLM-Assisted_Iterative_Curriculum_Writing_A_Human-Centered_AI_Approach_in_Finnish_Higher_Education_2025.bea-1.76/LLM-Assisted_Iterative_Curriculum_Writing_A_Human-Centered_AI_Approach_in_Finnish_Higher_Education_2025.bea-1.76__p4__score0.80.png",
        "caption": "Figure 2: Screenshot of the prototype interface. The left panel shows course topics and learning outcomes; colour highlights indicate segments automatically matched by the LLM. The right panel lists the UN Sustainable Development Goals (SDGs) with the corresponding curriculum fragments, illustrating the tool’s alignment-analysis feature."
      }
    ],
    "acl_id": "2025.bea-1.76"
  },
  {
    "id": 65,
    "folder_name": "LLMCrit_Teaching_Large_Language_Models_to_Use_Criteria_2024.findings-acl.472",
    "title": "LLMCrit Teaching Large Language Models to Use Criteria",
    "images": [
      {
        "filename": "LLMCrit_Teaching_Large_Language_Models_to_Use_Criteria_2024.findings-acl.472__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/LLMCrit_Teaching_Large_Language_Models_to_Use_Criteria_2024.findings-acl.472/LLMCrit_Teaching_Large_Language_Models_to_Use_Criteria_2024.findings-acl.472__p0__score0.95.png",
        "caption": "Figure 1: Illustration of teaching LLMs to use criteria."
      }
    ],
    "acl_id": "2024.findings-acl.472"
  },
  {
    "id": 66,
    "folder_name": "LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207",
    "title": "LLMs know their vulnerabilities Uncover Safety Gaps through Natural Distribution Shifts",
    "images": [
      {
        "filename": "LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207/LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207__p1__score1.00.png",
        "caption": "Figure 1: (a): A real-world example of our multi-turn attack compared with the single-turn toxic query. (b): the schematic description of our method. Each triangle box represents an actor, semantically related to the harmful target, as a hint for our multi-turn attack. The series of white circles represent a sequence of thoughts about how to finish our multi-turn attack step by step."
      },
      {
        "filename": "LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207/LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207__p3__score1.00.png",
        "caption": "Figure 2: Druing the pre-attack stage, ActorBreaker first leverages the knowledge of LLMs to instantiate our conceptual network Gconcept as Ginst as a two-layer tree. The leaf nodes of Ginst are specific actor names. ActorBreaker then samples actors and their relationships with the harmful target as our attack clues."
      },
      {
        "filename": "LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207__p4__score1.00.png",
        "path": "human_eval_dataset_dirs/LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207/LLMs_know_their_vulnerabilities_Uncover_Safety_Gaps_through_Natural_Distribution_Shifts_2025.acl-long.1207__p4__score1.00.png",
        "caption": "Figure 3: Our in-attack process consists of three steps: (a) infer the attack chain about how to perform our attack step by step, based on the attack clue; (b) follow the attack chain to generate the initial attack path via self-talk, i.e., self-ask and self-answer; (c) dynamic modify the initial attack path by exploiting responses from the victim model, using a GPT4-Judge, to enhance effectiveness."
      }
    ],
    "acl_id": "2025.acl-long.1207"
  },
  {
    "id": 67,
    "folder_name": "Language_Pivoting_from_Parallel_Corpora_for_Word_Sense_Disambiguation_of_Historical_Languages_A_Case_Study_on_Latin_2024.lrec-main.880",
    "title": "Language Pivoting from Parallel Corpora for Word Sense Disambiguation of Historical Languages A Case Study on Latin",
    "images": [
      {
        "filename": "Language_Pivoting_from_Parallel_Corpora_for_Word_Sense_Disambiguation_of_Historical_Languages_A_Case_Study_on_Latin_2024.lrec-main.880__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Language_Pivoting_from_Parallel_Corpora_for_Word_Sense_Disambiguation_of_Historical_Languages_A_Case_Study_on_Latin_2024.lrec-main.880/Language_Pivoting_from_Parallel_Corpora_for_Word_Sense_Disambiguation_of_Historical_Languages_A_Case_Study_on_Latin_2024.lrec-main.880__p3__score1.00.png",
        "caption": "Figure 1: An example of the two annotation propagation methods described. Two parallel Latin-English sentences are used so that the English sentence is disambiguated by a state-of-the-art WSD system, which obtains the WordNet synset identifiers for each lemma in the English sentence. At this point the synset for the target Latin word (i.e. sanctam) is chosen in one of the two methods: a) Propagationw/inter: we take the union of the English synsets w and the possible synsets for sanctam v. b) Propagationw/align: we use a word alignment module to obtain the English word associated with the target word sanctam (i.e. holy) and we assign the English synset w6 as the synset of the target word."
      }
    ],
    "acl_id": "2024.lrec-main.880"
  },
  {
    "id": 68,
    "folder_name": "Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176",
    "title": "Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools",
    "images": [
      {
        "filename": "Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176/Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176__p2__score1.00.png",
        "caption": "Figure 1: An overview of the framework. The blue region represents LLM. Given a natural language query, LLM 1) generates steps to formulate it as an SMT problem, 2) generates corresponding codes that encode the problem and call the solver. If the solver is not able to find the solution, LLM receives unsatisfiable reasons from the solver, collects information, analyzes the current situation, and offers suggestions to modify the query interactively. LLM then updates the code based on suggestions and calls the solver again to find a feasible plan."
      },
      {
        "filename": "Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176__p3__score0.70.png",
        "path": "human_eval_dataset_dirs/Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176/Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176__p3__score0.70.png",
        "caption": "Figure 2: Step to Code translation example."
      },
      {
        "filename": "Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176__p5__score0.70.png",
        "path": "human_eval_dataset_dirs/Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176/Large_Language_Models_Can_Solve_Real-World_Planning_Rigorously_with_Formal_Verification_Tools_2025.naacl-long.176__p5__score0.70.png",
        "caption": "Figure 3: Example of how JSON-Step prompt generalizes to unseen constraints. Yellow: unseen constraint types. Green: corresponding generated steps."
      }
    ],
    "acl_id": "2025.naacl-long.176"
  },
  {
    "id": 69,
    "folder_name": "Leveraging_Domain_Knowledge_at_Inference_Time_for_LLM_Translation_Retrieval_versus_Generation_2025.knowledgenlp-1.7",
    "title": "Leveraging Domain Knowledge at Inference Time for LLM Translation Retrieval versus Generation",
    "images": [
      {
        "filename": "Leveraging_Domain_Knowledge_at_Inference_Time_for_LLM_Translation_Retrieval_versus_Generation_2025.knowledgenlp-1.7__p1__score0.98.png",
        "path": "human_eval_dataset_dirs/Leveraging_Domain_Knowledge_at_Inference_Time_for_LLM_Translation_Retrieval_versus_Generation_2025.knowledgenlp-1.7/Leveraging_Domain_Knowledge_at_Inference_Time_for_LLM_Translation_Retrieval_versus_Generation_2025.knowledgenlp-1.7__p1__score0.98.png",
        "caption": "Figure 1: Illustration of the main MT settings, for an example source text in German. The two knowledge strategies are demonstrations vs. terminology; the two sources are retrieval vs. generation. This gives 4 settings for comparison. Within a strategy, we use the same prompts, varying only the provided information."
      },
      {
        "filename": "Leveraging_Domain_Knowledge_at_Inference_Time_for_LLM_Translation_Retrieval_versus_Generation_2025.knowledgenlp-1.7__p5__score1.00.png",
        "path": "human_eval_dataset_dirs/Leveraging_Domain_Knowledge_at_Inference_Time_for_LLM_Translation_Retrieval_versus_Generation_2025.knowledgenlp-1.7/Leveraging_Domain_Knowledge_at_Inference_Time_for_LLM_Translation_Retrieval_versus_Generation_2025.knowledgenlp-1.7__p5__score1.00.png",
        "caption": "Figure 2: Illustration of our process to decompose the contributions of retrieved demonstrations into style and terminology. We first extract the source-target term pairs using a simple function, and aggregate them into a local terminology. Then, the remaining tokens are the style templates, with the terms masked. Note that in the actual data, we use <MASK> instead of []."
      }
    ],
    "acl_id": "2025.knowledgenlp-1.7"
  },
  {
    "id": 70,
    "folder_name": "Leveraging_LLM_For_Synchronizing_Information_Across_Multilingual_Tables_2025.naacl-long.329",
    "title": "Leveraging LLM For Synchronizing Information Across Multilingual Tables",
    "images": [
      {
        "filename": "Leveraging_LLM_For_Synchronizing_Information_Across_Multilingual_Tables_2025.naacl-long.329__p1__score0.95.png",
        "path": "human_eval_dataset_dirs/Leveraging_LLM_For_Synchronizing_Information_Across_Multilingual_Tables_2025.naacl-long.329/Leveraging_LLM_For_Synchronizing_Information_Across_Multilingual_Tables_2025.naacl-long.329__p1__score0.95.png",
        "caption": "Figure 1: Example of information synchronization across multilingual tables. A reference table in a high-resource language is used to update outdated input tables in a low-resource language, resulting in an updated output table in the low-resource language."
      },
      {
        "filename": "Leveraging_LLM_For_Synchronizing_Information_Across_Multilingual_Tables_2025.naacl-long.329__p3__score0.95.png",
        "path": "human_eval_dataset_dirs/Leveraging_LLM_For_Synchronizing_Information_Across_Multilingual_Tables_2025.naacl-long.329/Leveraging_LLM_For_Synchronizing_Information_Across_Multilingual_Tables_2025.naacl-long.329__p3__score0.95.png",
        "caption": "Figure 2: Alignment Groups For Information Alignment. All rows highlighted in blue and connected by blue lines in the Source, Gold, and Output tables are tri-aligned, meaning they contain the same information across all three tables. Rows highlighted in red or green are bi-aligned, indicating that the information is consistent either between the Input and Gold tables or the Gold and Output tables. The remaining rows are unaligned, containing differing information."
      }
    ],
    "acl_id": "2025.naacl-long.329"
  },
  {
    "id": 71,
    "folder_name": "Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5",
    "title": "Linguistically Informed Transformers for Text to American Sign Language Translation",
    "images": [
      {
        "filename": "Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5/Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5__p0__score0.95.png",
        "caption": "Figure 1: Illustration of text to American Sign Language (ASL) translation using glosses as intermediate step."
      },
      {
        "filename": "Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5__p1__score0.80.png",
        "path": "human_eval_dataset_dirs/Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5/Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5__p1__score0.80.png",
        "caption": "Figure 2: Classification of signing space into horizontal, vertical, and lateral regions."
      },
      {
        "filename": "Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5__p1__score0.90.png",
        "path": "human_eval_dataset_dirs/Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5/Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5__p1__score0.90.png",
        "caption": "Figure 3: Illustration of Topic-Comment Structure"
      },
      {
        "filename": "Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5__p5__score1.00.png",
        "path": "human_eval_dataset_dirs/Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5/Linguistically_Informed_Transformers_for_Text_to_American_Sign_Language_Translation_2024.loresmt-1.5__p5__score1.00.png",
        "caption": "Figure 4: Video Retrieval Process flow"
      }
    ],
    "acl_id": "2024.loresmt-1.5"
  },
  {
    "id": 72,
    "folder_name": "MISGENDERED_Limits_of_Large_Language_Models_in_Understanding_Pronouns_2023.acl-long.293",
    "title": "MISGENDERED Limits of Large Language Models in Understanding Pronouns",
    "images": [
      {
        "filename": "MISGENDERED_Limits_of_Large_Language_Models_in_Understanding_Pronouns_2023.acl-long.293__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/MISGENDERED_Limits_of_Large_Language_Models_in_Understanding_Pronouns_2023.acl-long.293/MISGENDERED_Limits_of_Large_Language_Models_in_Understanding_Pronouns_2023.acl-long.293__p2__score1.00.png",
        "caption": "Figure 2: MISGENDERED Framework: We create a dataset to evaluate the ability of large language models to correctly ‘gender’ individuals. We manually write templates, each referring to an individual and containing a blank space for a pronoun to be filled-in. We populate the templates with names (unisex, female, and male) and pronouns (binary, gender-neutral, and non-binary), and declare two to five pronoun forms are for each individual either explicitly or parenthetically. We then use masked and auto-regressive LMs to predict missing pronouns in each instance utilizing a unified constrained decoding method."
      }
    ],
    "acl_id": "2023.acl-long.293"
  },
  {
    "id": 73,
    "folder_name": "Merging_Generated_and_Retrieved_Knowledge_for_Open-Domain_QA_2023.emnlp-main.286",
    "title": "Merging Generated and Retrieved Knowledge for Open-Domain QA",
    "images": [
      {
        "filename": "Merging_Generated_and_Retrieved_Knowledge_for_Open-Domain_QA_2023.emnlp-main.286__p3__score0.95.png",
        "path": "human_eval_dataset_dirs/Merging_Generated_and_Retrieved_Knowledge_for_Open-Domain_QA_2023.emnlp-main.286/Merging_Generated_and_Retrieved_Knowledge_for_Open-Domain_QA_2023.emnlp-main.286__p3__score0.95.png",
        "caption": "Figure 3: Example for demonstrating the construction of silver consistent labels to train the consistency discriminator DC ."
      },
      {
        "filename": "Merging_Generated_and_Retrieved_Knowledge_for_Open-Domain_QA_2023.emnlp-main.286__p6__score1.00.png",
        "path": "human_eval_dataset_dirs/Merging_Generated_and_Retrieved_Knowledge_for_Open-Domain_QA_2023.emnlp-main.286/Merging_Generated_and_Retrieved_Knowledge_for_Open-Domain_QA_2023.emnlp-main.286__p6__score1.00.png",
        "caption": "Figure 4: Illustrations of the input formats for two of the ablation experiments in Table 2."
      }
    ],
    "acl_id": "2023.emnlp-main.286"
  },
  {
    "id": 74,
    "folder_name": "Movie101v2_Improved_Movie_Narration_Benchmark_2025.acl-long.836",
    "title": "Movie101v2 Improved Movie Narration Benchmark",
    "images": [
      {
        "filename": "Movie101v2_Improved_Movie_Narration_Benchmark_2025.acl-long.836__p1__score0.85.png",
        "path": "human_eval_dataset_dirs/Movie101v2_Improved_Movie_Narration_Benchmark_2025.acl-long.836/Movie101v2_Improved_Movie_Narration_Benchmark_2025.acl-long.836__p1__score0.85.png",
        "caption": "Figure 1: Examples from other datasets (left) and Movie101v2 (right) where cases are from Goodbye Mr. Loser."
      }
    ],
    "acl_id": "2025.acl-long.836"
  },
  {
    "id": 75,
    "folder_name": "Multi-Strategy_Named_Entity_Recognition_System_for_Ancient_Chinese_2025.alp-1.28",
    "title": "Multi-Strategy Named Entity Recognition System for Ancient Chinese",
    "images": [
      {
        "filename": "Multi-Strategy_Named_Entity_Recognition_System_for_Ancient_Chinese_2025.alp-1.28__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Multi-Strategy_Named_Entity_Recognition_System_for_Ancient_Chinese_2025.alp-1.28/Multi-Strategy_Named_Entity_Recognition_System_for_Ancient_Chinese_2025.alp-1.28__p1__score1.00.png",
        "caption": "Figure 1: Architecture of the Multi-Strategy NER System. The system employs GujiRoBERTa_jian_fan as the PLM, paired with CRF for Tasks A and C (six entity types) and Softmax for Task B (three entity types)."
      }
    ],
    "acl_id": "2025.alp-1.28"
  },
  {
    "id": 76,
    "folder_name": "NACL_A_General_and_Effective_KV_Cache_Eviction_Framework_for_LLM_at_Inference_Time_2024.acl-long.428",
    "title": "NACL A General and Effective KV Cache Eviction Framework for LLM at Inference Time",
    "images": [
      {
        "filename": "NACL_A_General_and_Effective_KV_Cache_Eviction_Framework_for_LLM_at_Inference_Time_2024.acl-long.428__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/NACL_A_General_and_Effective_KV_Cache_Eviction_Framework_for_LLM_at_Inference_Time_2024.acl-long.428/NACL_A_General_and_Effective_KV_Cache_Eviction_Framework_for_LLM_at_Inference_Time_2024.acl-long.428__p0__score0.95.png",
        "caption": "Figure 1: Traditional eviction algorithms perform stepby-step greedy search for tokens for eviction. Our framework searches globally for tokens within a chunk and then performs one single eviction."
      },
      {
        "filename": "NACL_A_General_and_Effective_KV_Cache_Eviction_Framework_for_LLM_at_Inference_Time_2024.acl-long.428__p4__score0.95.png",
        "path": "human_eval_dataset_dirs/NACL_A_General_and_Effective_KV_Cache_Eviction_Framework_for_LLM_at_Inference_Time_2024.acl-long.428/NACL_A_General_and_Effective_KV_Cache_Eviction_Framework_for_LLM_at_Inference_Time_2024.acl-long.428__p4__score0.95.png",
        "caption": "Figure 3: NACL consists of a hybrid eviction policy by incorporating RANDOM EVICTION into PROXY-TOKENS EVICTION. PROXY-TOKENS EVICTION utilizes proxy tokens for more accurate eviction, while RANDOM EVICTION performs head-wise sampling from the scoring function of PROXY-TOKENS EVICTION to enhance the robustness."
      }
    ],
    "acl_id": "2024.acl-long.428"
  },
  {
    "id": 77,
    "folder_name": "No_Train_but_Gain_Language_Arithmetic_for_training-free_Language_Adapters_enhancement_2025.coling-main.737",
    "title": "No Train but Gain Language Arithmetic for training-free Language Adapters enhancement",
    "images": [
      {
        "filename": "No_Train_but_Gain_Language_Arithmetic_for_training-free_Language_Adapters_enhancement_2025.coling-main.737__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/No_Train_but_Gain_Language_Arithmetic_for_training-free_Language_Adapters_enhancement_2025.coling-main.737/No_Train_but_Gain_Language_Arithmetic_for_training-free_Language_Adapters_enhancement_2025.coling-main.737__p3__score1.00.png",
        "caption": "Figure 1: Language arithmetic as an extension of the MAD-X framework. Given language and task adapters (left), language arithmetic (right) enables post-processing, training-free improvement in two use-cases: (i) zero-shot where a language adapter for a target language was not trained (presented in the figure as Spanish, which was not part of existing language adapters pool, LAes(en, fr)) or (ii) to improve existing language adapters via arithmetic with either related language or a language on which task adapter was trained (e.g. LAfr(en, fr))."
      }
    ],
    "acl_id": "2025.coling-main.737"
  },
  {
    "id": 78,
    "folder_name": "Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314",
    "title": "Non-autoregressive Streaming Transformer for Simultaneous Translation",
    "images": [
      {
        "filename": "Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314__p1__score0.98.png",
        "path": "human_eval_dataset_dirs/Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314/Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314__p1__score0.98.png",
        "caption": "Figure 1: Illustration of the non-monotonicity problem and the source-info leakage bias in the training of autoregressive SiMT models. In this case, the AR SiMT model learns to predict at the third time step based on the source contexts \"布什 (Bush)\", \"与 (and)\", \"沙龙 (Sharon)\", and the ground truth contexts \"Bush\", \"held\". Although the source token \"举行 (hold)\" has not been read yet, it is exposed to the AR SiMT model through its corresponding token \"held\" in the ground truth context."
      },
      {
        "filename": "Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314/Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314__p3__score1.00.png",
        "caption": "Figure 2: Overview of the proposed non-autoregressive streaming Transformer (NAST). Upon receiving a source token, NAST upsamples it λ times and feeds them to the decoder as a chunk. NAST can generate blank token ϵ or repetitive tokens (both highlighted in gray) to find reasonable READ/WRITE paths adaptively. We train NAST using the non-monotonic latent alignment loss (Shao and Feng, 2022) with the alignment-based latency loss to achieve translation of high quality while maintaining low latency."
      },
      {
        "filename": "Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314__p4__score0.70.png",
        "path": "human_eval_dataset_dirs/Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314/Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314__p4__score0.70.png",
        "caption": "Figure 3: Illustration of cross-attention with different chunk wait-k strategies."
      }
    ],
    "acl_id": "2023.emnlp-main.314"
  },
  {
    "id": 79,
    "folder_name": "ORMind_A_Cognitive-Inspired_End-to-End_Reasoning_Framework_for_Operations_Research_2025.acl-industry.10",
    "title": "ORMind A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research",
    "images": [
      {
        "filename": "ORMind_A_Cognitive-Inspired_End-to-End_Reasoning_Framework_for_Operations_Research_2025.acl-industry.10__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/ORMind_A_Cognitive-Inspired_End-to-End_Reasoning_Framework_for_Operations_Research_2025.acl-industry.10/ORMind_A_Cognitive-Inspired_End-to-End_Reasoning_Framework_for_Operations_Research_2025.acl-industry.10__p1__score1.00.png",
        "caption": "Figure 1: Current frameworks rely on complex agent orchestration with unpredictable execution paths, dramatically increasing API calls and computation time. Their focus on code syntax rather than mathematical accuracy results in costly errors that can propagate through business operations undetected. This excessive coordination overhead makes these systems impractical for time-sensitive business applications. Compared to traditional methods, ORMind employs a streamlined end-to-end workflow with counterfactual reasoning, significantly enhancing solution reliability."
      },
      {
        "filename": "ORMind_A_Cognitive-Inspired_End-to-End_Reasoning_Framework_for_Operations_Research_2025.acl-industry.10__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/ORMind_A_Cognitive-Inspired_End-to-End_Reasoning_Framework_for_Operations_Research_2025.acl-industry.10/ORMind_A_Cognitive-Inspired_End-to-End_Reasoning_Framework_for_Operations_Research_2025.acl-industry.10__p2__score1.00.png",
        "caption": "Figure 2: Our approach is grounded in established cognitive science theories, particularly dual-process framework(Kahneman, 2011) and tripartite model of cognition(Stanovich, 2009). The Semantic Encoder and Formalization Thinking modules correspond to Type 1 (intuitive) processing, while the System 2 Reasoner implements Type 2 (analytical) processing. The Metacognitive Supervisor embodies the reflective mind, monitoring and coordinating between these systems."
      }
    ],
    "acl_id": "2025.acl-industry.10"
  },
  {
    "id": 80,
    "folder_name": "On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29",
    "title": "On Behalf of the Stakeholders Trends in NLP Model Interpretability in the Era of LLMs",
    "images": [
      {
        "filename": "On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29__p2__score0.95.png",
        "path": "human_eval_dataset_dirs/On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29/On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29__p2__score0.95.png",
        "caption": "Figure 2: Overview of four perspectives on the need for interpretability proposed in this paper."
      },
      {
        "filename": "On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29__p4__score0.70.png",
        "path": "human_eval_dataset_dirs/On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29/On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29__p4__score0.70.png",
        "caption": "Table 1: Overview of the interpretability paradigms discussed in this paper, categorised by their what and how properties (§4). A detailed survey of these paradigms is provided in §B. In bold, methods (SHAP, LIME, Clustering, Adversarial Attacks, Classic ML) that were analyzed separately of their paradigm in our trend analysis in §5."
      },
      {
        "filename": "On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29__p5__score0.98.png",
        "path": "human_eval_dataset_dirs/On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29/On_Behalf_of_the_Stakeholders_Trends_in_NLP_Model_Interpretability_in_the_Era_of_LLMs_2025.naacl-long.29__p5__score0.98.png",
        "caption": "Figure 3: An illustration of our five-stage procedure for annotating NLP interpretability papers, with the stages fully detailed in Appendix §D."
      }
    ],
    "acl_id": "2025.naacl-long.29"
  },
  {
    "id": 81,
    "folder_name": "On_the_Fragility_of_Active_Learners_for_Text_Classification_2024.emnlp-main.1240",
    "title": "On the Fragility of Active Learners for Text Classification",
    "images": [
      {
        "filename": "On_the_Fragility_of_Active_Learners_for_Text_Classification_2024.emnlp-main.1240__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/On_the_Fragility_of_Active_Learners_for_Text_Classification_2024.emnlp-main.1240/On_the_Fragility_of_Active_Learners_for_Text_Classification_2024.emnlp-main.1240__p2__score1.00.png",
        "caption": "Figure 1: The space of experiments is shown. See §4.1 for description. All representations are produced by pre-trained models, which are ubiquitous in practice today. The lines between the boxes “Representation” and “Classifier” denote combinations that constitute our prediction pipelines. Note that RoBERTa is an end-to-end predictor, where there are no separate representation and classification steps. Also note that the popular Transformer architecture (Vaswani et al., 2017) is represented by RoBERTa and MPNet here."
      }
    ],
    "acl_id": "2024.emnlp-main.1240"
  },
  {
    "id": 82,
    "folder_name": "One_by_zero_NLU_of_Devanagari_Script_Languages_2025_Target_Identification_for_Hate_Speech_Leveraging_Transformer-based_A_2025.chipsal-1.38",
    "title": "One by zero NLU of Devanagari Script Languages 2025 Target Identification for Hate Speech Leveraging Transformer-based A",
    "images": [
      {
        "filename": "One_by_zero_NLU_of_Devanagari_Script_Languages_2025_Target_Identification_for_Hate_Speech_Leveraging_Transformer-based_A_2025.chipsal-1.38__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/One_by_zero_NLU_of_Devanagari_Script_Languages_2025_Target_Identification_for_Hate_Speech_Leveraging_Transformer-based_A_2025.chipsal-1.38/One_by_zero_NLU_of_Devanagari_Script_Languages_2025_Target_Identification_for_Hate_Speech_Leveraging_Transformer-based_A_2025.chipsal-1.38__p2__score1.00.png",
        "caption": "Figure 1: Schematic process of the target identification for hate speech."
      }
    ],
    "acl_id": "2025.chipsal-1.38"
  },
  {
    "id": 83,
    "folder_name": "PeerQA_A_Scientific_Question_Answering_Dataset_from_Peer_Reviews_2025.naacl-long.22",
    "title": "PeerQA A Scientific Question Answering Dataset from Peer Reviews",
    "images": [
      {
        "filename": "PeerQA_A_Scientific_Question_Answering_Dataset_from_Peer_Reviews_2025.naacl-long.22__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/PeerQA_A_Scientific_Question_Answering_Dataset_from_Peer_Reviews_2025.naacl-long.22/PeerQA_A_Scientific_Question_Answering_Dataset_from_Peer_Reviews_2025.naacl-long.22__p0__score1.00.png",
        "caption": "Figure 1: Overview of the PeerQA data collection process. From the peer review process (in green), we extract and process questions from the reviews. Given the published version of the article and a question, an expert (in our case, the original paper authors) (1) checks the question and modifies or discards it, (2) annotates whether it is answerable or not (i.e. if there is sufficient information in the paper), and if so (3) highlights the evidence to answer the question and finally (4) provides a free-form answer to the question."
      }
    ],
    "acl_id": "2025.naacl-long.22"
  },
  {
    "id": 84,
    "folder_name": "Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120",
    "title": "Pluggable Neural Machine Translation Models via Memory-augmented Adapters",
    "images": [
      {
        "filename": "Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120/Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120__p0__score0.95.png",
        "caption": "Figure 1: A frozen and pluggable NMT model using memory-augmented plugins. For each user group with special requirements, we can develop a plugin for them without affecting other users."
      },
      {
        "filename": "Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120/Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120__p3__score1.00.png",
        "caption": "Figure 2: Construct and integrate memories. (a) We leverage parse trees to obtain multi-granular phrases. Each monolingual phrase is then translated by NMT models. (b) For each phrase pair, we perform a forward computation in the teacher-forcing manner and record some intermediate representations into the memory. (c) Illustration of adapter integration. The adapter retrieve and leverage the memories."
      },
      {
        "filename": "Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120__p3__score1.00__1.png",
        "path": "human_eval_dataset_dirs/Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120/Pluggable_Neural_Machine_Translation_Models_via_Memory-augmented_Adapters_2024.lrec-main.1120__p3__score1.00__1.png",
        "caption": "Figure 3: Memory-augmented adapter architecture."
      }
    ],
    "acl_id": "2024.lrec-main.1120"
  },
  {
    "id": 85,
    "folder_name": "PostMark_A_Robust_Blackbox_Watermark_for_Large_Language_Models_2024.emnlp-main.506",
    "title": "PostMark A Robust Blackbox Watermark for Large Language Models",
    "images": [
      {
        "filename": "PostMark_A_Robust_Blackbox_Watermark_for_Large_Language_Models_2024.emnlp-main.506__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/PostMark_A_Robust_Blackbox_Watermark_for_Large_Language_Models_2024.emnlp-main.506/PostMark_A_Robust_Blackbox_Watermark_for_Large_Language_Models_2024.emnlp-main.506__p1__score1.00.png",
        "caption": "Figure 1: The POSTMARK watermarking and detection procedure. Given some unwatermarked input text, we generate its embedding using the EMBEDDER and compute its cosine similarity with all word embeddings in the SECTABLE, performing top-k selection and additional semantic similarity filtering to choose a list of words. Then, we instruct the INSERTER to watermark the text by rewriting it to incorporate all selected words. During detection, we similarly obtain a watermark word list and check how many of these words are present in the input text."
      }
    ],
    "acl_id": "2024.emnlp-main.506"
  },
  {
    "id": 86,
    "folder_name": "Pre-Trained_Language_Models_Augmented_with_Synthetic_Scanpaths_for_Natural_Language_Understanding_2023.emnlp-main.400",
    "title": "Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding",
    "images": [
      {
        "filename": "Pre-Trained_Language_Models_Augmented_with_Synthetic_Scanpaths_for_Natural_Language_Understanding_2023.emnlp-main.400__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/Pre-Trained_Language_Models_Augmented_with_Synthetic_Scanpaths_for_Natural_Language_Understanding_2023.emnlp-main.400/Pre-Trained_Language_Models_Augmented_with_Synthetic_Scanpaths_for_Natural_Language_Understanding_2023.emnlp-main.400__p0__score1.00.png",
        "caption": "Figure 1: Synthetic scanpath-augmented language model: the Scanpath Generation Model predicts a sequence of fixations for an input sentence; token embeddings are rearranged according to the order of fixations."
      }
    ],
    "acl_id": "2023.emnlp-main.400"
  },
  {
    "id": 87,
    "folder_name": "ProBench_Judging_Multimodal_Foundation_Models_on_Open-ended_Multi-domain_Expert_Tasks_2025.findings-acl.568",
    "title": "ProBench Judging Multimodal Foundation Models on Open-ended Multi-domain Expert Tasks",
    "images": [
      {
        "filename": "ProBench_Judging_Multimodal_Foundation_Models_on_Open-ended_Multi-domain_Expert_Tasks_2025.findings-acl.568__p1__score0.80.png",
        "path": "human_eval_dataset_dirs/ProBench_Judging_Multimodal_Foundation_Models_on_Open-ended_Multi-domain_Expert_Tasks_2025.findings-acl.568/ProBench_Judging_Multimodal_Foundation_Models_on_Open-ended_Multi-domain_Expert_Tasks_2025.findings-acl.568__p1__score0.80.png",
        "caption": "Figure 3: ProBench overview. Distributions of (a) task fields on the single-round track, (b) languages on the multi-linguistic track, and (c) conversation rounds on the multi-round tracks."
      },
      {
        "filename": "ProBench_Judging_Multimodal_Foundation_Models_on_Open-ended_Multi-domain_Expert_Tasks_2025.findings-acl.568__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/ProBench_Judging_Multimodal_Foundation_Models_on_Open-ended_Multi-domain_Expert_Tasks_2025.findings-acl.568/ProBench_Judging_Multimodal_Foundation_Models_on_Open-ended_Multi-domain_Expert_Tasks_2025.findings-acl.568__p2__score1.00.png",
        "caption": "Figure 4: Framework of ProBench. Starting with 100K crowdsourced conversations, we identify high-quality user queries to curate single-round, multi-linguistic, and multi-round tracks. Using MLLM-as-a-Judge, we benchmark and rank 24 state-of-theart MLLMs with ELO ratings. To ensure fairness, the ELO ratings are de-biased to remove confounder effects (e.g., MLLM response formats), resulting in the final ProBench leaderboard. Icons in the figure are sourced from (Freepik et al., 2025)."
      }
    ],
    "acl_id": "2025.findings-acl.568"
  },
  {
    "id": 88,
    "folder_name": "Prompting_LLMs_Length_Control_for_Isometric_Machine_Translation_2025.iwslt-1.11",
    "title": "Prompting LLMs Length Control for Isometric Machine Translation",
    "images": [
      {
        "filename": "Prompting_LLMs_Length_Control_for_Isometric_Machine_Translation_2025.iwslt-1.11__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Prompting_LLMs_Length_Control_for_Isometric_Machine_Translation_2025.iwslt-1.11/Prompting_LLMs_Length_Control_for_Isometric_Machine_Translation_2025.iwslt-1.11__p0__score0.95.png",
        "caption": "Figure 1: Overview of our experiment with prompts asking for different length constraints for the desired translation, complemented with few-shot examples demonstrating the given constraint (match) or not (no-match). Strong enough control to reach isometric translation needs matching instructions and preferably Tiny or Short demonstrations. The construction of demonstration sets is described in Section 3 and the prompt content is presented in Table 6 in Appendix B.2."
      }
    ],
    "acl_id": "2025.iwslt-1.11"
  },
  {
    "id": 89,
    "folder_name": "Prompting_Large_Language_Models_for_Italian_Clinical_Reports_A_Benchmark_Study_2025.bionlp-1.17",
    "title": "Prompting Large Language Models for Italian Clinical Reports A Benchmark Study",
    "images": [
      {
        "filename": "Prompting_Large_Language_Models_for_Italian_Clinical_Reports_A_Benchmark_Study_2025.bionlp-1.17__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Prompting_Large_Language_Models_for_Italian_Clinical_Reports_A_Benchmark_Study_2025.bionlp-1.17/Prompting_Large_Language_Models_for_Italian_Clinical_Reports_A_Benchmark_Study_2025.bionlp-1.17__p1__score1.00.png",
        "caption": "Figure 1: Study Framework: task-specific benchmark analysis comparing LLMs using zero-shot and in-context learning (ICL) strategies against fine-tuned BERT-based models, in an information extraction task."
      }
    ],
    "acl_id": "2025.bionlp-1.17"
  },
  {
    "id": 90,
    "folder_name": "Propagation_and_Pitfalls_Reasoning-based_Assessment_of_Knowledge_Editing_through_Counterfactual_Tasks_2024.findings-acl.743",
    "title": "Propagation and Pitfalls Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks",
    "images": [
      {
        "filename": "Propagation_and_Pitfalls_Reasoning-based_Assessment_of_Knowledge_Editing_through_Counterfactual_Tasks_2024.findings-acl.743__p0__score0.85.png",
        "path": "human_eval_dataset_dirs/Propagation_and_Pitfalls_Reasoning-based_Assessment_of_Knowledge_Editing_through_Counterfactual_Tasks_2024.findings-acl.743/Propagation_and_Pitfalls_Reasoning-based_Assessment_of_Knowledge_Editing_through_Counterfactual_Tasks_2024.findings-acl.743__p0__score0.85.png",
        "caption": "Figure 1: An example of reasoning-based assessment for knowledge editing. Existing methods perform well at answering the question of the edited fact, but fail on reasoning with the edited fact."
      },
      {
        "filename": "Propagation_and_Pitfalls_Reasoning-based_Assessment_of_Knowledge_Editing_through_Counterfactual_Tasks_2024.findings-acl.743__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Propagation_and_Pitfalls_Reasoning-based_Assessment_of_Knowledge_Editing_through_Counterfactual_Tasks_2024.findings-acl.743/Propagation_and_Pitfalls_Reasoning-based_Assessment_of_Knowledge_Editing_through_Counterfactual_Tasks_2024.findings-acl.743__p2__score1.00.png",
        "caption": "Figure 2: Demonstration of construction process of ReCoE. Straight lines represent data sourced from existing datasets; Dashed lines denote data derived from Claude-generation; Zigzag lines denote data obtained through the corruption of other data. Group 1 includes superlative, comparative, and sorting questions, where we use “swapping” to create counterfactual facts. Group 2 represents counting, aggregation, and subtraction questions, where we use “altering” to create counterfactual facts."
      }
    ],
    "acl_id": "2024.findings-acl.743"
  },
  {
    "id": 91,
    "folder_name": "RATHANDravidianLangTech_2025_Annaparavai_-_Separate_the_Authentic_Human_Reviews_from_AI-generated_one_2025.dravidianlangtech-1.66",
    "title": "RATHANDravidianLangTech 2025 Annaparavai - Separate the Authentic Human Reviews from AI-generated one",
    "images": [
      {
        "filename": "RATHANDravidianLangTech_2025_Annaparavai_-_Separate_the_Authentic_Human_Reviews_from_AI-generated_one_2025.dravidianlangtech-1.66__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/RATHANDravidianLangTech_2025_Annaparavai_-_Separate_the_Authentic_Human_Reviews_from_AI-generated_one_2025.dravidianlangtech-1.66/RATHANDravidianLangTech_2025_Annaparavai_-_Separate_the_Authentic_Human_Reviews_from_AI-generated_one_2025.dravidianlangtech-1.66__p2__score1.00.png",
        "caption": "Figure 3: Proposed DNN architecture."
      },
      {
        "filename": "RATHANDravidianLangTech_2025_Annaparavai_-_Separate_the_Authentic_Human_Reviews_from_AI-generated_one_2025.dravidianlangtech-1.66__p2__score1.00__1.png",
        "path": "human_eval_dataset_dirs/RATHANDravidianLangTech_2025_Annaparavai_-_Separate_the_Authentic_Human_Reviews_from_AI-generated_one_2025.dravidianlangtech-1.66/RATHANDravidianLangTech_2025_Annaparavai_-_Separate_the_Authentic_Human_Reviews_from_AI-generated_one_2025.dravidianlangtech-1.66__p2__score1.00__1.png",
        "caption": "Figure 4: Proposed methodology."
      }
    ],
    "acl_id": "2025.dravidianlangtech-1.66"
  },
  {
    "id": 92,
    "folder_name": "Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634",
    "title": "Ranking Over Scoring Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments",
    "images": [
      {
        "filename": "Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634/Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634__p0__score1.00.png",
        "caption": "Figure 1: Graphical abstract illustrating the key elements of our approach. Synthetic arguments are first generated by prompting multiple LLMs, which are then ranked alongside gold-standard arguments by both our trained LM evaluator and a human expert. Our results show the LM evaluator aligns with human preferences."
      },
      {
        "filename": "Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634__p5__score0.95.png",
        "path": "human_eval_dataset_dirs/Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634/Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634__p5__score0.95.png",
        "caption": "Figure 2: A graphical abstract illustrating the system’s main components and behavior. The proposed LM evaluator prioritizes ranking primary arguments first and placing control cases last."
      },
      {
        "filename": "Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634__p6__score0.70.png",
        "path": "human_eval_dataset_dirs/Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634/Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634__p6__score0.70.png",
        "caption": "Figure 3: Ranking of the Primary Arguments. Each row corresponds to a distinct evaluator: the first three rows correspond to our proposed Proxy Task evaluators based on discriminative classification models, while the last row reflects the human criteria, obtained by having experts directly rank the arguments."
      },
      {
        "filename": "Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634__p7__score0.80.png",
        "path": "human_eval_dataset_dirs/Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634/Ranking_Over_Scoring_Towards_Reliable_and_Robust_Automated_Evaluation_of_LLM-Generated_Medical_Explanatory_Arguments_2025.coling-main.634__p7__score0.80.png",
        "caption": "Figure 4: Ranking of the Primary Arguments and Control Cases by the Proxy Task evaluators for each Proxy Task. Each row represents a distinct evaluator, and the columns represent the evaluated arguments. This table highlights the evaluators’ ability to differentiate between proper and improper arguments."
      }
    ],
    "acl_id": "2025.coling-main.634"
  },
  {
    "id": 93,
    "folder_name": "Response_Wide_Shut_Surprising_Observations_in_Basic_Vision_Language_Model_Capabilities_2025.acl-long.1241",
    "title": "Response Wide Shut Surprising Observations in Basic Vision Language Model Capabilities",
    "images": [
      {
        "filename": "Response_Wide_Shut_Surprising_Observations_in_Basic_Vision_Language_Model_Capabilities_2025.acl-long.1241__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/Response_Wide_Shut_Surprising_Observations_in_Basic_Vision_Language_Model_Capabilities_2025.acl-long.1241/Response_Wide_Shut_Surprising_Observations_in_Basic_Vision_Language_Model_Capabilities_2025.acl-long.1241__p0__score1.00.png",
        "caption": "Figure 1: Overview of our VLM analysis. Going beyond existing efforts that analyze VLMs as a whole, we study performance of VLMs in terms of intermediate spaces that represent knowledge as it is processed through the VLM network. Specifically, we consider three spaces in VLMs: visual, VL projection and response space; to understand what aspects of visual information are captured (not captured) and where."
      },
      {
        "filename": "Response_Wide_Shut_Surprising_Observations_in_Basic_Vision_Language_Model_Capabilities_2025.acl-long.1241__p1__score0.90.png",
        "path": "human_eval_dataset_dirs/Response_Wide_Shut_Surprising_Observations_in_Basic_Vision_Language_Model_Capabilities_2025.acl-long.1241/Response_Wide_Shut_Surprising_Observations_in_Basic_Vision_Language_Model_Capabilities_2025.acl-long.1241__p1__score0.90.png",
        "caption": "Figure 2: Qualitative results supporting the findings of our analysis. We show prediction (correct vs incorrect) for three spaces i.e visual, VL projection and response. We notice correct predictions in intermediate spaces and incorrect predictions in response space for object recognition and counting task. Furthermore, we notice a reversal in trend for spatial understanding task, where the response space has more correct predictions compared to intermediate spaces."
      }
    ],
    "acl_id": "2025.acl-long.1241"
  },
  {
    "id": 94,
    "folder_name": "Retrieval-free_Knowledge_Injection_through_Multi-Document_Traversal_for_Dialogue_Models_2023.acl-long.364",
    "title": "Retrieval-free Knowledge Injection through Multi-Document Traversal for Dialogue Models",
    "images": [
      {
        "filename": "Retrieval-free_Knowledge_Injection_through_Multi-Document_Traversal_for_Dialogue_Models_2023.acl-long.364__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/Retrieval-free_Knowledge_Injection_through_Multi-Document_Traversal_for_Dialogue_Models_2023.acl-long.364/Retrieval-free_Knowledge_Injection_through_Multi-Document_Traversal_for_Dialogue_Models_2023.acl-long.364__p0__score0.95.png",
        "caption": "Figure 1: The structure of the document \"Taylor Swift\" and her album \"Speak Now\". A document usually concentrates on one topic. Sentences within the document describe different aspects of the document’s topic."
      },
      {
        "filename": "Retrieval-free_Knowledge_Injection_through_Multi-Document_Traversal_for_Dialogue_Models_2023.acl-long.364__p2__score0.98.png",
        "path": "human_eval_dataset_dirs/Retrieval-free_Knowledge_Injection_through_Multi-Document_Traversal_for_Dialogue_Models_2023.acl-long.364/Retrieval-free_Knowledge_Injection_through_Multi-Document_Traversal_for_Dialogue_Models_2023.acl-long.364__p2__score0.98.png",
        "caption": "Figure 2: Overview of KiDG. To construct in-depth and topic-diversified dialogues, KiDG (1) builds an AG for each document to distinguish aspects; (2) connects topic-related documents to construct ATG; (3) utilizes the traversal algorithm MDT to walk through ATG to sample a sequence of knowledge sentences and transform them to a simulated dialogue. Note that, the Representative Nodes are marked in the darkest color, black, purple and red arrows represent the topic relations derived from Web Hyperlink, Knowledge Graph and Word vector respectively."
      }
    ],
    "acl_id": "2023.acl-long.364"
  },
  {
    "id": 95,
    "folder_name": "Robust_and_Minimally_Invasive_Watermarking_for_EaaS_2025.findings-acl.112",
    "title": "Robust and Minimally Invasive Watermarking for EaaS",
    "images": [
      {
        "filename": "Robust_and_Minimally_Invasive_Watermarking_for_EaaS_2025.findings-acl.112__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Robust_and_Minimally_Invasive_Watermarking_for_EaaS_2025.findings-acl.112/Robust_and_Minimally_Invasive_Watermarking_for_EaaS_2025.findings-acl.112__p1__score1.00.png",
        "caption": "Figure 1: The framework of our ESpeW. The upper part presents an overview of watermark injection and model extraction. (1) The stealer queries the provider’s EaaS to obtain a dataset that maps texts to embeddings. During this process, the provider injects watermarks. (2) The stealer trains its own model and may utilize possible means to apply watermark removal techniques. (3) The provider queries the stealer’s EaaS for copyright verification. The lower part offers a detailed explanation of the key modules for watermark insertion and verification."
      },
      {
        "filename": "Robust_and_Minimally_Invasive_Watermarking_for_EaaS_2025.findings-acl.112__p3__score0.98.png",
        "path": "human_eval_dataset_dirs/Robust_and_Minimally_Invasive_Watermarking_for_EaaS_2025.findings-acl.112/Robust_and_Minimally_Invasive_Watermarking_for_EaaS_2025.findings-acl.112__p3__score0.98.png",
        "caption": "Figure 2: Illustration of motivation for embedding-specific watermark. Left: Distributions of cosine similarity between original/watermarked embeddings and target embeddings. Middle: Calculation processes of watermarking. Right: Shared components among all watermarked embeddings."
      }
    ],
    "acl_id": "2025.findings-acl.112"
  },
  {
    "id": 96,
    "folder_name": "SURVEYFORGE_On_the_Outline_Heuristics_Memory-Driven_Generation_and_Multi-dimensional_Evaluation_for_Automated_Survey_2025.acl-long.609",
    "title": "SURVEYFORGE  On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey ",
    "images": [
      {
        "filename": "SURVEYFORGE_On_the_Outline_Heuristics_Memory-Driven_Generation_and_Multi-dimensional_Evaluation_for_Automated_Survey_2025.acl-long.609__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/SURVEYFORGE_On_the_Outline_Heuristics_Memory-Driven_Generation_and_Multi-dimensional_Evaluation_for_Automated_Survey_2025.acl-long.609/SURVEYFORGE_On_the_Outline_Heuristics_Memory-Driven_Generation_and_Multi-dimensional_Evaluation_for_Automated_Survey_2025.acl-long.609__p0__score1.00.png",
        "caption": "Figure 1: Compared to human-written surveys, AIgenerated surveys face two primary challenges. First, regarding the outline, these papers may often lack coherent logic and well-structured organization. Second, with respect to references, they frequently fail to include truly relevant and influential literature."
      },
      {
        "filename": "SURVEYFORGE_On_the_Outline_Heuristics_Memory-Driven_Generation_and_Multi-dimensional_Evaluation_for_Automated_Survey_2025.acl-long.609__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/SURVEYFORGE_On_the_Outline_Heuristics_Memory-Driven_Generation_and_Multi-dimensional_Evaluation_for_Automated_Survey_2025.acl-long.609/SURVEYFORGE_On_the_Outline_Heuristics_Memory-Driven_Generation_and_Multi-dimensional_Evaluation_for_Automated_Survey_2025.acl-long.609__p2__score1.00.png",
        "caption": "Figure 2: The overview of SURVEYFORGE. The framework consists of two main stages: Outline Generation and Content Writing. In the Outline Generation stage, SURVEYFORGE utilizes heuristic learning to generate well-structured outlines by leveraging topic-relevant literature and structural patterns from existing surveys. In the Content Writing stage, a memory-driven Scholar Navigation Agent (SANA) retrieves high-quality literature for each subsection and LLM generates the content of each subsection. Finally, the content is synthesized and refined into a coherent and comprehensive survey."
      }
    ],
    "acl_id": "2025.acl-long.609"
  },
  {
    "id": 97,
    "folder_name": "ScottyPoseidon_at_SemEval-2025_Task_8_LLM-Driven_Code_Generation_for_Zero-Shot_Question_Answering_on_Tabular_Data_2025.semeval-1.285",
    "title": "ScottyPoseidon at SemEval-2025 Task 8 LLM-Driven Code Generation for Zero-Shot Question Answering on Tabular Data",
    "images": [
      {
        "filename": "ScottyPoseidon_at_SemEval-2025_Task_8_LLM-Driven_Code_Generation_for_Zero-Shot_Question_Answering_on_Tabular_Data_2025.semeval-1.285__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/ScottyPoseidon_at_SemEval-2025_Task_8_LLM-Driven_Code_Generation_for_Zero-Shot_Question_Answering_on_Tabular_Data_2025.semeval-1.285/ScottyPoseidon_at_SemEval-2025_Task_8_LLM-Driven_Code_Generation_for_Zero-Shot_Question_Answering_on_Tabular_Data_2025.semeval-1.285__p2__score1.00.png",
        "caption": "Figure 1: Flowchart illustrating the data preprocessing and model workflow for Subtasks 1 and 2 using a unified and an agentic approach. In the agentic setting the central ‘LLM’ turns into a ‘reasoner LLM’ which delineates steps for the ‘Code LLM’ to write code, which on execution feeds back the error codes to both the reasoner and code LLMs. This is illustrated by using red-dashed arrows in the figure."
      }
    ],
    "acl_id": "2025.semeval-1.285"
  },
  {
    "id": 98,
    "folder_name": "Strategies_for_political-statement_segmentation_and_labelling_in_unstructured_text_2025.nlp4dh-1.38",
    "title": "Strategies for political-statement segmentation and labelling in unstructured text",
    "images": [
      {
        "filename": "Strategies_for_political-statement_segmentation_and_labelling_in_unstructured_text_2025.nlp4dh-1.38__p3__score0.70.png",
        "path": "human_eval_dataset_dirs/Strategies_for_political-statement_segmentation_and_labelling_in_unstructured_text_2025.nlp4dh-1.38/Strategies_for_political-statement_segmentation_and_labelling_in_unstructured_text_2025.nlp4dh-1.38__p3__score0.70.png",
        "caption": "Figure 1: An example of an in-context learning prompt, comprising natural-language instructions, in-context learning examples, and the input text. The instructions are shown verbatim; in-context learning examples shown are real examples from the dataset but are truncated for space. The model’s response to this prompt, decoded with constraints, will constitute the prediction for the input text."
      }
    ],
    "acl_id": "2025.nlp4dh-1.38"
  },
  {
    "id": 99,
    "folder_name": "Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1",
    "title": "Streaming Sequence Transduction through Dynamic Compression",
    "images": [
      {
        "filename": "Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1__p0__score0.97.png",
        "path": "human_eval_dataset_dirs/Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1/Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1__p0__score0.97.png",
        "caption": "Figure 1: When YIELD is triggered, the current segment’s information is compressed into an anchor representation to generate the next output."
      },
      {
        "filename": "Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1/Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1__p2__score1.00.png",
        "caption": "Figure 2: Visualization for the training of the segmenter through feedback from the encoder-decoder’s crossattention."
      },
      {
        "filename": "Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1__p3__score0.95.png",
        "path": "human_eval_dataset_dirs/Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1/Streaming_Sequence_Transduction_through_Dynamic_Compression_2025.iwslt-1.1__p3__score0.95.png",
        "caption": "Figure 3: Visualization for the proposed “selection as compression” method. Input features are transformed by the encoder and we only select the encoding at the anchor position (where YIELD is triggered) as the compressed representation."
      }
    ],
    "acl_id": "2025.iwslt-1.1"
  },
  {
    "id": 100,
    "folder_name": "SuLoRA_Subspace_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning_2025.findings-acl.278",
    "title": "SuLoRA Subspace Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "images": [
      {
        "filename": "SuLoRA_Subspace_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning_2025.findings-acl.278__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/SuLoRA_Subspace_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning_2025.findings-acl.278/SuLoRA_Subspace_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning_2025.findings-acl.278__p0__score0.95.png",
        "caption": "Figure 1: (a) Comparison between LoRA (left) and our method SuLoRA (right). The core of our method is to partition the internal parameter space of LoRA and select different parameter spaces for different tasks, avoiding interference between tasks. (b) Activation of the internal parameter space in LoRA B. G and c denote the parameter subspace and task, respectively. Darker colors indicate higher levels of activation. Taking STS-B as an example, we find that the activation of the internal parameter space in LoRA varies across different tasks."
      },
      {
        "filename": "SuLoRA_Subspace_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning_2025.findings-acl.278__p3__score0.98.png",
        "path": "human_eval_dataset_dirs/SuLoRA_Subspace_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning_2025.findings-acl.278/SuLoRA_Subspace_Low-Rank_Adaptation_for_Parameter-Efficient_Fine-Tuning_2025.findings-acl.278__p3__score0.98.png",
        "caption": "Figure 2: Parameter subspace selection of LoRA B matrix in SuLoRA. SuLoRA treats different parameter subspaces as experts and allocates r parameter subspaces to each instance through a routing method based on hidden features."
      }
    ],
    "acl_id": "2025.findings-acl.278"
  },
  {
    "id": 101,
    "folder_name": "Synergistic_Weak-Strong_Collaboration_by_Aligning_Preferences_2025.acl-long.995",
    "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences",
    "images": [
      {
        "filename": "Synergistic_Weak-Strong_Collaboration_by_Aligning_Preferences_2025.acl-long.995__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/Synergistic_Weak-Strong_Collaboration_by_Aligning_Preferences_2025.acl-long.995/Synergistic_Weak-Strong_Collaboration_by_Aligning_Preferences_2025.acl-long.995__p0__score1.00.png",
        "caption": "Figure 1: Comparison of our method and the related"
      },
      {
        "filename": "Synergistic_Weak-Strong_Collaboration_by_Aligning_Preferences_2025.acl-long.995__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/Synergistic_Weak-Strong_Collaboration_by_Aligning_Preferences_2025.acl-long.995/Synergistic_Weak-Strong_Collaboration_by_Aligning_Preferences_2025.acl-long.995__p3__score1.00.png",
        "caption": "Figure 2: Aligning the Weak Model with Strong Model Feedback, including preference data construction and preference tuning."
      }
    ],
    "acl_id": "2025.acl-long.995"
  },
  {
    "id": 102,
    "folder_name": "Tarbiat_Modares_SemEval2025_Task11_MultiLabel_Emotion_TransferLearning_2025.semeval-1.154",
    "title": "Tarbiat Modares SemEval2025 Task11 MultiLabel Emotion TransferLearning",
    "images": [
      {
        "filename": "Tarbiat_Modares_SemEval2025_Task11_MultiLabel_Emotion_TransferLearning_2025.semeval-1.154__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Tarbiat_Modares_SemEval2025_Task11_MultiLabel_Emotion_TransferLearning_2025.semeval-1.154/Tarbiat_Modares_SemEval2025_Task11_MultiLabel_Emotion_TransferLearning_2025.semeval-1.154__p2__score1.00.png",
        "caption": "Figure 1: Schematic of our multi-label emotion detection model. The LaBSE encoder generates sentence embeddings, followed by dropout, a fully connected layer, and sigmoid activation for classification."
      }
    ],
    "acl_id": "2025.semeval-1.154"
  },
  {
    "id": 103,
    "folder_name": "TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2",
    "title": "TaxoCritic Exploring Credit Assignment in Taxonomy Induction with Multi-Critic Reinforcement Learning",
    "images": [
      {
        "filename": "TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2__p2__score0.95.png",
        "path": "human_eval_dataset_dirs/TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2/TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2__p2__score0.95.png",
        "caption": "Figure 1: Example of an action at = (Apple Tree, Plant), where roott = {Living}, Ut = {Living, Plant, Animal, Carnivore, Herbivore}, Vt = {Tree, Rabbit, Apple Tree, Horse}, and Et = {(Plant, Living), (Animal, Living), (Carnivore, Animal), (Herbivore, Animal)}. After the execution of this action, roott+1 = {Living}, Ut+1 = {Living, Plant, Animal, Carnivore, Herbivore, Apple Tree}, Vt+1 = {Tree, Rabbit, Horse}, and Et+1 = {(Plant, Living), (Animal, Living), (Carnivore, Animal), (Herbivore, Animal), (Apple Tree, Plant)}."
      },
      {
        "filename": "TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2/TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2__p3__score1.00.png",
        "caption": "Figure 2: An overview of the TaxoCritic method. fa, fc1 and fc2 represent the feature representations (vectors) of the inputs for the actor and two critics respectively. The actor (a two-layer fully connected feed-forward neural network) takes the encoding of a state as the input and outputs the policy π. Following this policy, the environment executes a sampled action which contains two sub-actions. As shown on the right side, the critic network features two sub-critics and a mixing layer. Considering the state and two sub-actions, one critic evaluates the child’s choice, while the other evaluates the parent’s choice. The mixing layer combines those results from both critics and produces the action value q."
      },
      {
        "filename": "TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2__p4__score0.70.png",
        "path": "human_eval_dataset_dirs/TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2/TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2__p4__score0.70.png",
        "caption": "Figure 3: The action feature vector at = (v, u) concatenates the word embeddings —using GloVe (Pennington et al., 2014)— for terms v and u, their dependency path from the corpus, and syntactic features into one vector."
      },
      {
        "filename": "TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2__p7__score0.95.png",
        "path": "human_eval_dataset_dirs/TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2/TaxoCritic_Exploring_Credit_Assignment_in_Taxonomy_Induction_with_Multi-Critic_Reinforcement_Learning_2024.dlnld-1.2__p7__score0.95.png",
        "caption": "Figure 5: A simple partially constructed bedroom hierarchy. Yellow and Blue nodes (top three rows) denote correctly placed terms. Green and Red nodes (bottom two rows) are yet to be placed, while indicating their intended positions."
      }
    ],
    "acl_id": "2024.dlnld-1.2"
  },
  {
    "id": 104,
    "folder_name": "Text2Cypher_Bridging_Natural_Language_and_Graph_Databases_2025.genaik-1.11",
    "title": "Text2Cypher Bridging Natural Language and Graph Databases",
    "images": [
      {
        "filename": "Text2Cypher_Bridging_Natural_Language_and_Graph_Databases_2025.genaik-1.11__p0__score1.00.png",
        "path": "human_eval_dataset_dirs/Text2Cypher_Bridging_Natural_Language_and_Graph_Databases_2025.genaik-1.11/Text2Cypher_Bridging_Natural_Language_and_Graph_Databases_2025.genaik-1.11__p0__score1.00.png",
        "caption": "Figure 1: User wants to write a Cypher query for ‘What are the movies of Tom Hanks‘. A Text2Cypher model translates the input natural language question into Cypher, i.e., ‘MATCH (actor:Person {name: \"Tom Hanks\"})-[:ACTED_IN]->(movie:Movie) RETURN movie.title AS movies‘"
      }
    ],
    "acl_id": "2025.genaik-1.11"
  },
  {
    "id": 105,
    "folder_name": "Threefold_model_for_AI_Readiness_A_Case_Study_with_Finnish_Healthcare_SMEs_2025.nlp4dh-1.41",
    "title": "Threefold model for AI Readiness A Case Study with Finnish Healthcare SMEs",
    "images": [
      {
        "filename": "Threefold_model_for_AI_Readiness_A_Case_Study_with_Finnish_Healthcare_SMEs_2025.nlp4dh-1.41__p6__score0.80.png",
        "path": "human_eval_dataset_dirs/Threefold_model_for_AI_Readiness_A_Case_Study_with_Finnish_Healthcare_SMEs_2025.nlp4dh-1.41/Threefold_model_for_AI_Readiness_A_Case_Study_with_Finnish_Healthcare_SMEs_2025.nlp4dh-1.41__p6__score0.80.png",
        "caption": "Figure 1: Threefold Model of AI in Business"
      },
      {
        "filename": "Threefold_model_for_AI_Readiness_A_Case_Study_with_Finnish_Healthcare_SMEs_2025.nlp4dh-1.41__p7__score1.00.png",
        "path": "human_eval_dataset_dirs/Threefold_model_for_AI_Readiness_A_Case_Study_with_Finnish_Healthcare_SMEs_2025.nlp4dh-1.41/Threefold_model_for_AI_Readiness_A_Case_Study_with_Finnish_Healthcare_SMEs_2025.nlp4dh-1.41__p7__score1.00.png",
        "caption": "Figure 2: Interdependence of Companies in Different Categories of Business AI"
      }
    ],
    "acl_id": "2025.nlp4dh-1.41"
  },
  {
    "id": 106,
    "folder_name": "Tooling_or_Not_Tooling_The_Impact_of_Tools_on_Language_Agents_for_Chemistry_Problem_Solving_2025.findings-naacl.424",
    "title": "Tooling or Not Tooling The Impact of Tools on Language Agents for Chemistry Problem Solving",
    "images": [
      {
        "filename": "Tooling_or_Not_Tooling_The_Impact_of_Tools_on_Language_Agents_for_Chemistry_Problem_Solving_2025.findings-naacl.424__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Tooling_or_Not_Tooling_The_Impact_of_Tools_on_Language_Agents_for_Chemistry_Problem_Solving_2025.findings-naacl.424/Tooling_or_Not_Tooling_The_Impact_of_Tools_on_Language_Agents_for_Chemistry_Problem_Solving_2025.findings-naacl.424__p1__score1.00.png",
        "caption": "Figure 1: Our ChemAgent framework. Upon receiving a user task, the agent iterates through a three-step ReAct process (Yao et al., 2023): (1) Thought generation, analyzing the current situation and planning subsequent steps; (2) Action determination, selecting the appropriate tool and its input based on the generated thought; and (3) Observation obtaining, executing a tool in the environment and obtaining the results or feedback. This iterative cycle continues until task completion or conclusion, and the final answer is returned to the user."
      }
    ],
    "acl_id": "2025.findings-naacl.424"
  },
  {
    "id": 107,
    "folder_name": "Toward_Human-Like_Evaluation_for_Natural_Language_Generation_with_Error_Analysis_2023.acl-long.324",
    "title": "Toward Human-Like Evaluation for Natural Language Generation with Error Analysis",
    "images": [
      {
        "filename": "Toward_Human-Like_Evaluation_for_Natural_Language_Generation_with_Error_Analysis_2023.acl-long.324__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Toward_Human-Like_Evaluation_for_Natural_Language_Generation_with_Error_Analysis_2023.acl-long.324/Toward_Human-Like_Evaluation_for_Natural_Language_Generation_with_Error_Analysis_2023.acl-long.324__p2__score1.00.png",
        "caption": "Figure 1: An analogy between MQM and BARTScore++. We show an evaluation example from machine translation (zh-en). Top: Source and reference sentence provided for evaluation. Medium: An annotation example using MQM framework. Errors in the hypothesis are assigned with Major and Minor. The MQM score is computed through the weighted sum of these errors. Bottom: BARTScore++. The hypothesis is first refined through an error analysis framework. The refined sentence is then used to obtain the distance of explicit/ implicit errors through vanilla BARTScore. Different weights are finally assigned to these errors to get a more accurate score."
      }
    ],
    "acl_id": "2023.acl-long.324"
  },
  {
    "id": 108,
    "folder_name": "Towards_Cross-Cultural_Machine_Translation_with_Retrieval-Augmented_Generation_from_Multilingual_Knowledge_Graphs_2024.emnlp-main.914",
    "title": "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs",
    "images": [
      {
        "filename": "Towards_Cross-Cultural_Machine_Translation_with_Retrieval-Augmented_Generation_from_Multilingual_Knowledge_Graphs_2024.emnlp-main.914__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Towards_Cross-Cultural_Machine_Translation_with_Retrieval-Augmented_Generation_from_Multilingual_Knowledge_Graphs_2024.emnlp-main.914/Towards_Cross-Cultural_Machine_Translation_with_Retrieval-Augmented_Generation_from_Multilingual_Knowledge_Graphs_2024.emnlp-main.914__p2__score1.00.png",
        "caption": "Figure 1: Overview of KG-MT, which leverages a knowledge retriever, i.e., a dense retrieval mechanism to retrieve the most relevant entities from a multilingual knowledge graph (see Section 3.1), to improve the translation. The retrieved entities are then integrated into the MT system in two ways: explicit knowledge integration, where the entity names are explicitly added to the source text (see Section 3.2), and implicit knowledge integration, where the entity embeddings are fused with the encoder hidden states (see Section 3.3)."
      }
    ],
    "acl_id": "2024.emnlp-main.914"
  },
  {
    "id": 109,
    "folder_name": "Transcending_Scaling_Laws_with_0.1_Extra_Compute_2023.emnlp-main.91",
    "title": "Transcending Scaling Laws with 0.1 Extra Compute",
    "images": [
      {
        "filename": "Transcending_Scaling_Laws_with_0.1_Extra_Compute_2023.emnlp-main.91__p6__score0.80.png",
        "path": "human_eval_dataset_dirs/Transcending_Scaling_Laws_with_0.1_Extra_Compute_2023.emnlp-main.91/Transcending_Scaling_Laws_with_0.1_Extra_Compute_2023.emnlp-main.91__p6__score0.80.png",
        "caption": "Figure 5: An example of a prompt that is improved by rephrasing to use U-PaLM’s infilling capabilities."
      },
      {
        "filename": "Transcending_Scaling_Laws_with_0.1_Extra_Compute_2023.emnlp-main.91__p7__score0.90.png",
        "path": "human_eval_dataset_dirs/Transcending_Scaling_Laws_with_0.1_Extra_Compute_2023.emnlp-main.91/Transcending_Scaling_Laws_with_0.1_Extra_Compute_2023.emnlp-main.91__p7__score0.90.png",
        "caption": "Figure 6: An example of a prompt that works only when querying a specific pretraining mode."
      }
    ],
    "acl_id": "2023.emnlp-main.91"
  },
  {
    "id": 110,
    "folder_name": "USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216",
    "title": "USDC A Dataset of User Stance and Dogmatism in Long Conversations",
    "images": [
      {
        "filename": "USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216__p1__score0.95.png",
        "path": "human_eval_dataset_dirs/USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216/USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216__p1__score0.95.png",
        "caption": "Figure 1: Sample Reddit conversation on “Capitalism vs. Socialism” with Stance (for every comment {ci}6i=1) and Dogmatism (for every author {aj}3j=1) labels from Mistral Large and GPT-4. The submission content favors socialism and examines how the authors position their opinions regarding socialism vs. capitalism."
      },
      {
        "filename": "USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216/USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216__p2__score1.00.png",
        "caption": "Figure 2: Generating annotations using LLMs: We pass the entire conversation for each Reddit thread as JSON. The JSON includes top two authors who posted most comments, alongside annotation guidelines for stance and dogmatism labels in system prompt."
      },
      {
        "filename": "USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216__p4__score0.95.png",
        "path": "human_eval_dataset_dirs/USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216/USDC_A_Dataset_of_User_Stance_and_Dogmatism_in_Long_Conversations_2025.findings-acl.1216__p4__score0.95.png",
        "caption": "Figure 3: Failure cases of LLMs: Mistral Large few-shot output (left), the ids (“f6mmzx1”,“f6mna88”) were mismatched with generated ids (“f9mmzx1”,“f9mna88”), GPT-4 zero-shot output (right), the key “label” was mismatched with generated key “body”."
      }
    ],
    "acl_id": "2025.findings-acl.1216"
  },
  {
    "id": 111,
    "folder_name": "Unveiling_Fake_News_with_Adversarial_Arguments_Generated_by_Multimodal_Large_Language_Models_2025.coling-main.526",
    "title": "Unveiling Fake News with Adversarial Arguments Generated by Multimodal Large Language Models",
    "images": [
      {
        "filename": "Unveiling_Fake_News_with_Adversarial_Arguments_Generated_by_Multimodal_Large_Language_Models_2025.coling-main.526__p2__score0.98.png",
        "path": "human_eval_dataset_dirs/Unveiling_Fake_News_with_Adversarial_Arguments_Generated_by_Multimodal_Large_Language_Models_2025.coling-main.526/Unveiling_Fake_News_with_Adversarial_Arguments_Generated_by_Multimodal_Large_Language_Models_2025.coling-main.526__p2__score0.98.png",
        "caption": "Figure 1: Illustration of AAR model."
      }
    ],
    "acl_id": "2025.coling-main.526"
  },
  {
    "id": 112,
    "folder_name": "VECHR_A_Dataset_for_Explainable_and_Robust_Classification_of_Vulnerability_Type_in_the_European_Court_of_Human_Rights_2023.emnlp-main.718",
    "title": "VECHR A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights",
    "images": [
      {
        "filename": "VECHR_A_Dataset_for_Explainable_and_Robust_Classification_of_Vulnerability_Type_in_the_European_Court_of_Human_Rights_2023.emnlp-main.718__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/VECHR_A_Dataset_for_Explainable_and_Robust_Classification_of_Vulnerability_Type_in_the_European_Court_of_Human_Rights_2023.emnlp-main.718/VECHR_A_Dataset_for_Explainable_and_Robust_Classification_of_Vulnerability_Type_in_the_European_Court_of_Human_Rights_2023.emnlp-main.718__p3__score1.00.png",
        "caption": "Figure 2: Visualization of Hierarchical and Concept-aware Hierarchical Model architectures."
      }
    ],
    "acl_id": "2023.emnlp-main.718"
  },
  {
    "id": 113,
    "folder_name": "Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486",
    "title": "Value FULCRA Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Value",
    "images": [
      {
        "filename": "Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486/Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486__p1__score1.00.png",
        "caption": "Figure 1: Illustration of (a) downstream safety risks in existing value alignment datasets and (b) our proposed basic value paradigm with an instantiated 10-dim value space spanned by Schwartz’s Theory for evaluation and alignment."
      },
      {
        "filename": "Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486__p3__score0.90.png",
        "path": "human_eval_dataset_dirs/Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486/Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486__p3__score0.90.png",
        "caption": "Figure 2: Demonstration of FULCRA dataset, including dataset composition, statistical information and case study."
      },
      {
        "filename": "Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486__p4__score0.98.png",
        "path": "human_eval_dataset_dirs/Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486/Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486__p4__score0.98.png",
        "caption": "Figure 3: The workflow of Human-GPT collaborative annotation, including three primary steps."
      },
      {
        "filename": "Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486__p5__score0.85.png",
        "path": "human_eval_dataset_dirs/Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486/Value_FULCRA_Mapping_Large_Language_Models_to_the_Multidimensional_Spectrum_of_Basic_Human_Value_2024.naacl-long.486__p5__score0.85.png",
        "caption": "Figure 4: (a) Visualization of LLM outputs in the value space. We observe that 1) basic values effectively distinguish safe and unsafe behaviors; 2) different safety risks are well clarified in the value space; and 3) basic values can help identify new types of risks. (b) Correlation between basic value dimensions and specific safety risks."
      }
    ],
    "acl_id": "2024.naacl-long.486"
  },
  {
    "id": 114,
    "folder_name": "WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508",
    "title": "WebWalker Benchmarking LLMs in Web Traversal",
    "images": [
      {
        "filename": "WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508/WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508__p0__score0.95.png",
        "caption": "Figure 1: A multi-source QA2example from WebWalkerQA that requires traversing web pages to gather information for answering the given question."
      },
      {
        "filename": "WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508/WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508__p3__score1.00.png",
        "caption": "Figure 2: Data Generation Pipeline for WebWalkerQA. We first collect root official websites across conference, organization, education, and game domains. Then we mimic human behavior by systematically clicking and collecting subpages accessible through sublinks on the root page. Using predefined rules, we leverage GPT4o to generate synthetic QA-pairs based on the gathered information, followed by manual verification to ensure accuracy and relevance."
      },
      {
        "filename": "WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508__p4__score1.00.png",
        "path": "human_eval_dataset_dirs/WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508/WebWalker_Benchmarking_LLMs_in_Web_Traversal_2025.acl-long.508__p4__score1.00.png",
        "caption": "Figure 4: The overall framework of WebWalker."
      }
    ],
    "acl_id": "2025.acl-long.508"
  },
  {
    "id": 115,
    "folder_name": "What_Factors_Influence_LLMs_Judgments_A_Case_Study_on_Question_Answering_2024.lrec-main.1519",
    "title": "What Factors Influence LLMs Judgments A Case Study on Question Answering",
    "images": [
      {
        "filename": "What_Factors_Influence_LLMs_Judgments_A_Case_Study_on_Question_Answering_2024.lrec-main.1519__p0__score0.95.png",
        "path": "human_eval_dataset_dirs/What_Factors_Influence_LLMs_Judgments_A_Case_Study_on_Question_Answering_2024.lrec-main.1519/What_Factors_Influence_LLMs_Judgments_A_Case_Study_on_Question_Answering_2024.lrec-main.1519__p0__score0.95.png",
        "caption": "Figure 1: The workflow for answer quality judgment by a LLM judge. Initially, candidate models offer potential answers to the question, which are then combined with the question and presented to the judge model to obtain a judgment."
      },
      {
        "filename": "What_Factors_Influence_LLMs_Judgments_A_Case_Study_on_Question_Answering_2024.lrec-main.1519__p1__score0.70.png",
        "path": "human_eval_dataset_dirs/What_Factors_Influence_LLMs_Judgments_A_Case_Study_on_Question_Answering_2024.lrec-main.1519/What_Factors_Influence_LLMs_Judgments_A_Case_Study_on_Question_Answering_2024.lrec-main.1519__p1__score0.70.png",
        "caption": "Table 1: Factors may influence LLMs’ judgments. The factors highlighted in gray above have already been explored, while the factors below are newly introduced in this paper. and represent two candidate answers, which are presented to the judge to determine which one is superior. ‘+’ signifies a positive guidance for the candidate, while ‘-’ conveys the opposite meaning."
      }
    ],
    "acl_id": "2024.lrec-main.1519"
  },
  {
    "id": 116,
    "folder_name": "Word-Aware_Modality_Stimulation_for_Multimodal_Fusion_2024.lrec-main.1536",
    "title": "Word-Aware Modality Stimulation for Multimodal Fusion",
    "images": [
      {
        "filename": "Word-Aware_Modality_Stimulation_for_Multimodal_Fusion_2024.lrec-main.1536__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/Word-Aware_Modality_Stimulation_for_Multimodal_Fusion_2024.lrec-main.1536/Word-Aware_Modality_Stimulation_for_Multimodal_Fusion_2024.lrec-main.1536__p2__score1.00.png",
        "caption": "Figure 1: Overview of WA-MSF."
      },
      {
        "filename": "Word-Aware_Modality_Stimulation_for_Multimodal_Fusion_2024.lrec-main.1536__p3__score0.98.png",
        "path": "human_eval_dataset_dirs/Word-Aware_Modality_Stimulation_for_Multimodal_Fusion_2024.lrec-main.1536/Word-Aware_Modality_Stimulation_for_Multimodal_Fusion_2024.lrec-main.1536__p3__score0.98.png",
        "caption": "Figure 2: Design of the MSU-Layer."
      }
    ],
    "acl_id": "2024.lrec-main.1536"
  },
  {
    "id": 117,
    "folder_name": "ZenPropaganda_A_Comprehensive_Study_on_Identifying_Propaganda_Techniques_in_Russian_Coronavirus-Related_Media_2024.lrec-main.1548",
    "title": "ZenPropaganda A Comprehensive Study on Identifying Propaganda Techniques in Russian Coronavirus-Related Media",
    "images": [
      {
        "filename": "ZenPropaganda_A_Comprehensive_Study_on_Identifying_Propaganda_Techniques_in_Russian_Coronavirus-Related_Media_2024.lrec-main.1548__p3__score0.90.png",
        "path": "human_eval_dataset_dirs/ZenPropaganda_A_Comprehensive_Study_on_Identifying_Propaganda_Techniques_in_Russian_Coronavirus-Related_Media_2024.lrec-main.1548/ZenPropaganda_A_Comprehensive_Study_on_Identifying_Propaganda_Techniques_in_Russian_Coronavirus-Related_Media_2024.lrec-main.1548__p3__score0.90.png",
        "caption": "Table 2: Characteristics of text fragments."
      }
    ],
    "acl_id": "2024.lrec-main.1548"
  },
  {
    "id": 118,
    "folder_name": "Zero-Shot_Fact-Checking_with_Semantic_Triples_and_Knowledge_Graphs_2024.kallm-1.11",
    "title": "Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs",
    "images": [
      {
        "filename": "Zero-Shot_Fact-Checking_with_Semantic_Triples_and_Knowledge_Graphs_2024.kallm-1.11__p1__score1.00.png",
        "path": "human_eval_dataset_dirs/Zero-Shot_Fact-Checking_with_Semantic_Triples_and_Knowledge_Graphs_2024.kallm-1.11/Zero-Shot_Fact-Checking_with_Semantic_Triples_and_Knowledge_Graphs_2024.kallm-1.11__p1__score1.00.png",
        "caption": "Figure 1: An overview of our zero-shot learning system. By harnessing Wikidata for training the universal schema model, incorporating on-demand training with evidence triples, and leveraging OpenIE for triple-level inference, our system achieves enhanced improvements. Label S stands for SUPPORTS and R stands for REFUTES."
      }
    ],
    "acl_id": "2024.kallm-1.11"
  },
  {
    "id": 119,
    "folder_name": "mPLUG-DocOwl2_High-resolution_Compressing_for_OCR-free_Multi-page_Document_Understanding_2025.acl-long.291",
    "title": "mPLUG-DocOwl2 High-resolution Compressing for OCR-free Multi-page Document Understanding",
    "images": [
      {
        "filename": "mPLUG-DocOwl2_High-resolution_Compressing_for_OCR-free_Multi-page_Document_Understanding_2025.acl-long.291__p2__score1.00.png",
        "path": "human_eval_dataset_dirs/mPLUG-DocOwl2_High-resolution_Compressing_for_OCR-free_Multi-page_Document_Understanding_2025.acl-long.291/mPLUG-DocOwl2_High-resolution_Compressing_for_OCR-free_Multi-page_Document_Understanding_2025.acl-long.291__p2__score1.00.png",
        "caption": "Figure 2: Illustrations of different compressing methods for OCR-free document understanding."
      },
      {
        "filename": "mPLUG-DocOwl2_High-resolution_Compressing_for_OCR-free_Multi-page_Document_Understanding_2025.acl-long.291__p3__score1.00.png",
        "path": "human_eval_dataset_dirs/mPLUG-DocOwl2_High-resolution_Compressing_for_OCR-free_Multi-page_Document_Understanding_2025.acl-long.291/mPLUG-DocOwl2_High-resolution_Compressing_for_OCR-free_Multi-page_Document_Understanding_2025.acl-long.291__p3__score1.00.png",
        "caption": "Figure 3: The architecture of DocOwl2. Each image is independently encoded by the pipeline of Shape-adaptive Cropping, High-resolution Visual Encoding and High-resolution DocCompressor."
      }
    ],
    "acl_id": "2025.acl-long.291"
  },
  {
    "id": 120,
    "folder_name": "taz2024full_Analysing_German_Newspapers_for_Gender_Bias_and_Discrimination_across_Decades_2025.findings-acl.555",
    "title": "taz2024full Analysing German Newspapers for Gender Bias and Discrimination across Decades",
    "images": [
      {
        "filename": "taz2024full_Analysing_German_Newspapers_for_Gender_Bias_and_Discrimination_across_Decades_2025.findings-acl.555__p3__score0.90.png",
        "path": "human_eval_dataset_dirs/taz2024full_Analysing_German_Newspapers_for_Gender_Bias_and_Discrimination_across_Decades_2025.findings-acl.555/taz2024full_Analysing_German_Newspapers_for_Gender_Bias_and_Discrimination_across_Decades_2025.findings-acl.555__p3__score0.90.png",
        "caption": "Figure 1: Structure of the elements in the corpus, including all available metadata collected alongside the raw texts."
      }
    ],
    "acl_id": "2025.findings-acl.555"
  }
];