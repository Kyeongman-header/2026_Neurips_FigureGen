{
  "source_pdf": "/home/zzangmane/2025_null_Figure/pdfs/emnlp-2023/A_Mechanistic_Interpretation_of_Arithmetic_Reasoning_in_Language_Models_using_Causal_Mediation_Analysis_2023.emnlp-main.435.pdf",
  "page": 0,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: Visualization of our findings. We trace the flow of numerical information within Transformerbased LMs: given an input query, the model processes the representations of numbers and operators with early layers (A). Then, the relevant information is conveyed by the attention mechanism to the end of the input sequence (B). Here, it is processed by late MLP modules, which output result-related information into the residual stream (C).",
  "regionBoundary": {
    "x1": 323.52,
    "x2": 506.4,
    "y1": 213.12,
    "y2": 413.28
  },
  "score": 0.95,
  "reason": "Labeled blocks, arrows, and modules show a conceptual/system overview of information flow in a model."
}