{
  "source_pdf": "/home/zzangmane/2025_null_Figure/pdfs/naacl-2025/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458.pdf",
  "page": 4,
  "figureType": null,
  "name": "4",
  "caption": "Figure 4: Illustrated example on the generation of one chunk of output in GAME. Step ①: the LLM predicts the next chunk (Y2) and calculates the chunk attention feature (v̄2) without any attention editing. Step ②: the classifier (F ) predicts the hallucination score (c) for the generated chunk with the corresponding feature. If the score exceeds a predefined threshold, the chunk will be accepted. Otherwise, attention editing will be applied to regenerate the chunk. Step ③: the attention edit signal for each head is computed with the prior bias and the edit direction ∆ derived from the gradient of the score. Step ④: a new chunk is generated with the calculated attention editing signal and re-evaluated with the classifier. If no qualified chunk is accepted with number of regeneration attempts, the chunk with the highest score during the generation process will be accepted.",
  "regionBoundary": {
    "x1": 81.6,
    "x2": 514.0799999999999,
    "y1": 69.6,
    "y2": 176.16
  },
  "score": 1.0,
  "reason": "Depicts a multi-step process and system components, indicative of an overall framework overview."
}