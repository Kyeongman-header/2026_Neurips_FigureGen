{
  "source_pdf": "/home/zzangmane/2025_null_Figure/pdfs/naacl-2025/Gradient-guided_Attention_Map_Editing_Towards_Efficient_Contextual_Hallucination_Mitigation_2025.findings-naacl.458.pdf",
  "page": 1,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: Illustration of a decoder-only Transformer featuring a multi-head attention mechanism. Each row in an attention map represents a weight vector that sums to one, reflecting the current tokenâ€™s relationship with preceding tokens. A deeper color indicates a higher attention weight.",
  "regionBoundary": {
    "x1": 305.76,
    "x2": 525.12,
    "y1": 390.71999999999997,
    "y2": 505.91999999999996
  },
  "score": 1.0,
  "reason": "The image provides an architecture overview of a neural attention mechanism system."
}