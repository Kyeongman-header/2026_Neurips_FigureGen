{
  "paper_name": "IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy",
  "evaluated_at": "2025-12-28T00:41:28.008694",
  "figure_evaluations": [
    {
      "figure_file": "IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy__p4__score0.80.png",
      "caption": "Figure 4: The original data source, the evaluation metric, and the data size of each task.",
      "scores": {
        "Informativeness": {
          "score": 0.483,
          "sub_metrics": [
            {
              "question": "1.1. content coverage",
              "score": 0.72,
              "reason": "the figure clearly enumerates the benchmark’s major task categories (rule following, task execution, safety defense, tool use), and for each it reports the original data sources, evaluation metrics (f-1, rouge-l, accuracy, defense success rate), and dataset sizes. however, it does not convey other major components that are central to the paper’s full setup (e.g., hierarchical input types and priority orders, aligned vs conflict settings, strictness variations), and it includes no formulas or scoring definitions beyond metric names."
            },
            {
              "question": "1.2. standalone intelligibility",
              "score": 0.55,
              "reason": "a reader can understand that the benchmark is composed of multiple tasks with specified sources, metrics, and sizes, but the operational principle of iheval (how instruction hierarchy is instantiated, what constitutes a conflict, how scoring/aggregation is performed, and what “defense success rate” specifically measures) is not inferable from this figure alone."
            },
            {
              "question": "1.3. completeness",
              "score": 0.18,
              "reason": "the figure is a partial specification table for task construction and evaluation only; it does not summarize the full paper arc (benchmark design details, hierarchy definition and settings, experimental protocol, model results, analyses/ablations, key findings, and implications). therefore it is far from a beginning-to-end summary."
            }
          ]
        },
        "Fidelity": {
          "score": 0.777,
          "sub_metrics": [
            {
              "question": "2.1. hallucination: does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.75,
              "reason": "the figure is a summary table of tasks, data sources, metrics, and sizes. it does not introduce equations or novel methods. however, without the paper’s full task-by-task specification, i cannot verify that every listed dataset/metric pairing (e.g., specific metrics per subtask and the exact inclusion of listed tool-use datasets such as hand crafted + sep) is explicitly stated exactly as shown, so there is some residual risk of added or slightly altered items."
            },
            {
              "question": "2.2. relation correctness: are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.78,
              "reason": "the relationships shown (each scenario → subtasks; each subtask → data source, metric, and size) are internally coherent and plausible for an instruction-hierarchy evaluation benchmark. still, i cannot confirm from the provided context alone that the paper defines these exact mappings (e.g., which datasets belong to intrinsic vs injected tool use, and that defense success rate is the metric for the safety defense hijack/extraction split) exactly as displayed."
            },
            {
              "question": "2.3. label accuracy: are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.8,
              "reason": "labels like rule following, task execution (extraction/generation/classification), safety defense (hijack/extraction), and tool use (intrinsic/injected) appear consistent with the paper context (iHeval) and common naming. citations and dataset names (ontonotes, mgsm, xlsum, terror-instruct, etc.) look standard. but i cannot fully verify that all labels/citations are exactly the ones used in the paper for each cell (including capitalization and exact dataset naming)."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.827,
          "sub_metrics": [
            {
              "question": "3.1. summarization: is it schematized focusing on the 'main contribution' rather than trivial details?",
              "score": 0.78,
              "reason": "the figure is a compact schematic table that summarizes tasks, data sources, metrics, and sizes—core orienting information rather than procedural minutiae. however, some cell content (many dataset names and citations) slightly crowds the layout and can feel like detail-heavy inventory rather than a high-level takeaway."
            },
            {
              "question": "3.2. contextual match: does this figure function well as a supplementary material to help understanding when reading the caption or text?",
              "score": 0.84,
              "reason": "as a supplement, it clearly maps each task category to its provenance, evaluation metric, and sample size, which supports readers trying to understand experimental setup quickly. readability is somewhat limited by small font and dense multi-line text, but the structure still matches the caption and likely aids lookup."
            },
            {
              "question": "3.3. redundancy: does it avoid decorative elements or unnecessary information unrelated to the core ideas?",
              "score": 0.86,
              "reason": "the design is largely functional with minimal decoration; color is used to separate categories and aligns with the content. the primary redundancy/unnecessary load comes from packing multiple citations/dataset lists into cells, which adds visual clutter but remains topically relevant rather than decorative."
            }
          ]
        },
        "Design Quality": {
          "score": 0.879,
          "sub_metrics": [
            {
              "question": "4.1. direction: does the diagram flow from left to right or top to bottom?",
              "score": 0.9,
              "reason": "the figure is organized as a left-to-right sequence of four labeled blocks, which reads naturally; the internal rows (data/metric/size) also follow a consistent top-to-bottom structure."
            },
            {
              "question": "4.2. crossing: do the connection lines avoid crossing each other?",
              "score": 1.0,
              "reason": "there are no connecting lines or arrows, so there is no risk of crossings."
            },
            {
              "question": "4.3. proximity: are functionally closely related modules physically placed near each other?",
              "score": 0.85,
              "reason": "each category groups its related items (data, metric, size) within the same block, keeping related information together; however, some items span multiple tasks and could benefit from clearer within-block grouping or separators."
            },
            {
              "question": "4.4. alignment: are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.9,
              "reason": "the blocks and their internal rows are cleanly aligned with consistent baselines and column widths; minor alignment variance arises from differing text lengths and line breaks."
            },
            {
              "question": "4.5. hierarchy: do the important main components stand out in size, thickness, or position?",
              "score": 0.8,
              "reason": "the four top headers are prominent via colored tabs and bold text, creating clear primary structure; secondary hierarchy (e.g., subtypes like single-turn/multi-turn, extraction/generation/classification) is present but less visually differentiated."
            },
            {
              "question": "4.6. margin: is there sufficient margin between elements?",
              "score": 0.75,
              "reason": "overall whitespace between the four blocks is adequate, but within blocks some text feels tight (especially multi-line entries), reducing readability and creating slight crowding."
            },
            {
              "question": "4.7. consistency: are components with similar roles represented with the same shape and color?",
              "score": 0.95,
              "reason": "all main categories use consistent rectangular blocks with similarly styled colored headers; repeated row labels (data/metric/size) are consistently placed and styled, and color usage is stable across the figure."
            }
          ]
        },
        "Creativity": {
          "score": 0.28,
          "sub_metrics": [
            {
              "question": "5.1. metaphor: to what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.28,
              "reason": "the figure mainly uses labeled boxes and color-coding to denote task categories and fields (data/metric/size). while color acts as a lightweight symbol system, there are no distinctive icons, pictograms, or strong visual metaphors mapping abstract ideas (e.g., hierarchy, sourcing, evaluation) to concrete symbols beyond standard tabular headings."
            },
            {
              "question": "5.2. novelty: does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.22,
              "reason": "the design closely resembles a conventional summary table/slide-style schematic: rectangular sections, pastel headers, and straightforward typography. it is clean and readable but not stylistically distinctive or visually inventive relative to common nlp paper figures."
            },
            {
              "question": "5.3. adaptability: does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.34,
              "reason": "it adapts the layout to the paper by aligning four benchmark categories with consistent columns (data/metric/size) and using category-specific colors, which helps scanning. however, it largely remains within a uniform grid/table paradigm rather than introducing a more tailored structure (e.g., hierarchical flow, relational diagram, or dependency view) that reflects the paper’s emphasis on hierarchy and evaluation composition."
            }
          ]
        },
        "weighted_total": 0.649
      }
    },
    {
      "figure_file": "IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy__p1__score1.00.png",
      "caption": "Figure 1: Four categories of the instruction hierarchy and the corresponding priority orders of instructions. Conflict instructions are shown in red. Models are expected to follow the instruction with the higher priority.",
      "scores": {
        "Informativeness": {
          "score": 0.55,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage: Does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.55,
              "reason": "the figure covers the core conceptual components of iheval’s setup: four hierarchy categories (rule following, task execution, safety defense, tool use), the involved input sources (system, user, conversation history, tool output), and illustrates alignment vs conflict via red conflicting instructions and correct/incorrect model behaviors. however, it omits many major paper-level components: dataset scale (3,538 examples), the nine tasks, the aligned/conflict/reference evaluation settings and scoring details, the programmable evaluation procedure, and the main quantitative results (shown elsewhere as figure 2 in the paper). no formulas are relevant here, but key benchmark design dimensions and metrics are not included in this figure."
            },
            {
              "question": "1.2. Standalone Intelligibility: Can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.85,
              "reason": "yes: the figure clearly communicates the operating principle—there is an instruction hierarchy with explicit priority orders, conflicts can occur between higher- and lower-priority instructions, and models should follow the higher-priority instruction. concrete mini-examples with check/cross marks make the expected behavior easy to infer. minor ambiguity remains about how evaluation is computed (e.g., exact scoring, how often each scenario appears, and how conversation history is constructed), but the high-level mechanism is understandable."
            },
            {
              "question": "1.3. Completeness: Is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.25,
              "reason": "no. the figure is a focused schematic of the hierarchy categories and priority orders, not a summary of the full paper. it does not include the benchmark composition details, task list, experimental protocol, model roster, main findings/plots, analyses (e.g., strictness effects, scaling), or conclusions/implications. it represents only one conceptual piece rather than an end-to-end summary."
            }
          ]
        },
        "Fidelity": {
          "score": 0.967,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.95,
              "reason": "the figure content matches the paper context: it depicts the four instruction-hierarchy categories (rule following, task execution, safety defense, tool use), the involved input sources (system/user/conversation history/tool output), and example conflicts highlighted in red. no extra methodologies, datasets, or formulas beyond what is described are introduced. minor risk: the specific illustrative examples (e.g., japan itinerary, slack users, access granted code) are demonstrative; they appear consistent with the described benchmark design but cannot be fully verified from the provided text alone."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.98,
              "reason": "the priority relations shown are consistent with the stated hierarchy: system messages highest, then user messages, then conversation history, then tool outputs. the four panels correctly map to priority comparisons: system > user; system > conversation history (multi-turn); system > user (task execution and safety defense examples); user > tool output (tool use example). conflict-vs-follow behavior (cross vs check) aligns with 'models are expected to follow higher-priority instructions'."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.97,
              "reason": "labels for categories (rule following, task execution, safety defense, tool use) and message types (system message, user message, model response, tool output) match the terminology in the provided excerpt. the priority-order annotations (e.g., system message > user message; user message > tool output) are correctly named and placed. no apparent mislabeling of components."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.8,
          "sub_metrics": [
            {
              "question": "3.1. summarization",
              "score": 0.8,
              "reason": "the figure is largely schematized around the main contribution (four instruction-hierarchy categories + priority ordering + conflict vs correct behavior) and uses concrete mini-examples to operationalize the idea. however, some example text is longer than necessary (e.g., full code snippet, multi-line trip plan), which slightly shifts attention toward instance details rather than the abstract principle."
            },
            {
              "question": "3.2. contextual match",
              "score": 0.9,
              "reason": "it works well as support for the caption and surrounding discussion: the four panels map cleanly to the described categories, the legend clarifies roles (system/user/model/tool), and the red conflict cues plus check/x outcomes make the intended interpretation quickly accessible while reading."
            },
            {
              "question": "3.3. redundancy",
              "score": 0.7,
              "reason": "most elements serve comprehension, but there is some redundancy/visual clutter: repeated chat icons, repeated check/x markers, and relatively elaborate ui-like decoration (speech-bubble styling, tool window chrome) that is not strictly required to convey hierarchy and conflict. trimming repeated ornaments and shortening example text would improve signal-to-noise."
            }
          ]
        },
        "Design Quality": {
          "score": 0.893,
          "sub_metrics": [
            {
              "question": "4.1. direction: does the diagram flow from left to right or top to bottom?",
              "score": 0.9,
              "reason": "the figure is organized as four panels laid out left-to-right, each read top-to-bottom within the panel; the intended reading order is clear."
            },
            {
              "question": "4.2. crossing: do the connection lines avoid crossing each other?",
              "score": 1.0,
              "reason": "there are no explicit connecting lines/arrows between elements that could cross; relationships are conveyed by spatial grouping and labels."
            },
            {
              "question": "4.3. proximity: are functionally closely related modules physically placed near each other?",
              "score": 0.95,
              "reason": "within each panel, system/user/model/tool elements are grouped tightly, and the success/failure examples sit directly next to the relevant interaction, supporting quick association."
            },
            {
              "question": "4.4. alignment: are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.85,
              "reason": "the four panels align cleanly and internal elements largely follow consistent vertical stacking; minor misalignments arise from varying text box sizes and differing internal layouts across panels."
            },
            {
              "question": "4.5. hierarchy: do the important main components stand out in size, thickness, or position?",
              "score": 0.8,
              "reason": "panel titles and the red conflict text provide emphasis, and the four-category structure is prominent; however, priority ordering could be made more visually dominant (e.g., stronger directional cues or clearer ranking markers) rather than relying mainly on captions and small labels."
            },
            {
              "question": "4.6. margin: is there sufficient margin between elements?",
              "score": 0.8,
              "reason": "overall whitespace and panel padding are adequate, but some panels feel dense (multiple stacked chat bubbles, icons, and check/cross marks), which slightly reduces breathing room."
            },
            {
              "question": "4.7. consistency: are components with similar roles represented with the same shape and color?",
              "score": 0.95,
              "reason": "system/user/model/tool are consistently encoded via recurring icons and box styles; conflict text is consistently red and correctness uses consistent green check/red cross indicators."
            }
          ]
        },
        "Creativity": {
          "score": 0.62,
          "sub_metrics": [
            {
              "question": "5.1. metaphor: to what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.72,
              "reason": "the figure uses recognizable icons (system/user/model/tool) and visual cues (red conflict text, check/x marks) to concretize the abstract notion of instruction hierarchy and priority. the metaphors are clear and functional, though they rely on fairly standard ui-symbol conventions rather than more inventive symbolic mapping."
            },
            {
              "question": "5.2. novelty: does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.48,
              "reason": "the style resembles a common \"four-panel schematic\" used in nlp papers: rounded boxes, icon+speech bubbles, and check/x outcome markers. it is clean and coherent, but not especially distinctive or unconventional compared with typical explanatory diagrams in the field."
            },
            {
              "question": "5.3. adaptability: does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.66,
              "reason": "the layout is tailored to the paper’s taxonomy (four categories) and demonstrates priority orders per category with consistent visual grammar, which supports quick comparison across scenarios. however, it still largely follows a uniform panel structure rather than introducing a more task-specific, hierarchy-first visualization (e.g., explicit layered stacks/flows) that would more strongly depart from standard designs."
            }
          ]
        },
        "weighted_total": 0.766
      }
    },
    {
      "figure_file": "IHEval_Evaluating_Language_Models_on_Following_the_Instruction_Hierarchy__p3__score1.00.png",
      "caption": "Figure 3: IHEval covers four categories and nine tasks. Detailed examples and instructions are in Figures 8~16.",
      "scores": {
        "Informativeness": {
          "score": 0.6,
          "sub_metrics": [
            {
              "question": "1.1. content coverage: does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.72,
              "reason": "the figure captures the benchmark’s major structural components: four categories (rule following, task execution, safety defense, tool use), the aligned vs conflict conditions, and the nine-task breakdown (e.g., extraction/generation/classification under task execution; intrinsic vs injected instruction under tool use). however, it omits many paper-level specifics needed for full content coverage, such as dataset size details, exact evaluation metrics/scoring definitions, model lists/results, and any formalization/precise hierarchy definition beyond what’s implied by the layouts."
            },
            {
              "question": "1.2. standalone intelligibility: can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.83,
              "reason": "the figure is largely self-explanatory: it visually encodes the inputs (system/user/history/tool), shows how aligned vs conflict variants are constructed, and illustrates how single-turn vs multi-turn settings differ. a reader can infer that the benchmark tests whether models follow higher-priority instructions when conflicts appear. some details are still ambiguous without text (e.g., exact priority ordering in all cases, how scoring is computed, and what constitutes a pass/fail for each task)."
            },
            {
              "question": "1.3. completeness: is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.25,
              "reason": "the figure summarizes only the benchmark coverage/design slice (categories, tasks, and conflict/alignment setups). it does not summarize the full paper arc (motivation/related work, dataset construction details, programmable evaluation procedure specifics, experimental results and analyses, ablations, limitations, and conclusions). thus it is not a beginning-to-end summary."
            }
          ]
        },
        "Fidelity": {
          "score": 0.917,
          "sub_metrics": [
            {
              "question": "2.1. hallucination: does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.93,
              "reason": "the figure presents iHeval as covering four categories (rule following, task execution, safety defense, tool use) and nine tasks with aligned/conflict settings, which matches the described benchmark structure. it also uses elements consistent with the paper’s framing (system message, user message, conversation history/multi-turn, tool output, injected instruction). no extraneous formulas are introduced. minor risk: the figure adds some specific task-type labels (e.g., extraction/generation/classification; intrinsic instruction vs injected instruction) that are plausible within the paper’s taxonomy but are not verifiable from the provided excerpt alone."
            },
            {
              "question": "2.2. relation correctness: are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.9,
              "reason": "the figure correctly encodes the key relationship emphasized in the paper: hierarchical instruction settings with aligned vs conflict conditions, spanning single-turn and multi-turn, plus tool-use scenarios where tool outputs can carry instructions (injected) that may conflict with higher-priority directives. this is consistent with the paper’s statement that it covers four input types and alignment/conflict variants across tasks. slight uncertainty remains because the excerpt does not fully specify the internal subdivision of the nine tasks (e.g., mapping of task execution into extraction/generation/classification), so full relation verification is limited."
            },
            {
              "question": "2.3. label accuracy: are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.92,
              "reason": "labels for the four categories (rule following, task execution, safety defense, tool use) match the paper text and the earlier figure description. the aligned/conflict labels also align with the paper’s terminology. icon/role labels (system message, user message, model response, tool output) are consistent with the paper’s hierarchy discussion. minor possible mismatch: some sublabels (intrinsic instruction, injected instruction; extraction/generation/classification) are not explicitly confirmed in the provided excerpt, though they do not contradict it."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.78,
          "sub_metrics": [
            {
              "question": "overall readability",
              "score": 0.78,
              "reason": "the figure is generally understandable: it uses clear section headers, consistent iconography, and a simple aligned-vs-conflict layout across categories, which supports quick scanning. however, readability is reduced by small text (especially inside the small panels), dense multi-panel composition, and reliance on color (e.g., red for conflict) that may be less accessible in print or for color-vision deficiencies. the amount of content packed into one page makes it harder to parse at a glance without zooming."
            }
          ]
        },
        "Design Quality": {
          "score": 0.851,
          "sub_metrics": [
            {
              "question": "4.1. direction: does the diagram flow from left to right or top to bottom?",
              "score": 0.82,
              "reason": "the layout reads primarily top-to-bottom by category bands (rule following, task execution, safety defense, tool use), with left-to-right progression within each band (aligned vs conflict, single-turn vs multi-turn). flow is mostly clear, though multiple parallel lanes reduce a single dominant reading path."
            },
            {
              "question": "4.2. crossing: do the connection lines avoid crossing each other?",
              "score": 0.98,
              "reason": "there are essentially no connector lines; the figure relies on grouped panels and labels. as a result, there is no meaningful line-crossing clutter."
            },
            {
              "question": "4.3. proximity: are functionally closely related modules physically placed near each other?",
              "score": 0.9,
              "reason": "related tasks are clustered within clearly labeled category sections, and aligned/conflict variants are adjacent. icons and mini-panels that belong together are grouped tightly, supporting quick comparison."
            },
            {
              "question": "4.4. alignment: are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.78,
              "reason": "most panels follow a grid-like structure with consistent column pairing (aligned vs conflict) and row grouping. however, some internal elements (badges, text boxes, and small callouts) appear slightly uneven, and spacing varies across sections."
            },
            {
              "question": "4.5. hierarchy: do the important main components stand out in size, thickness, or position?",
              "score": 0.84,
              "reason": "category headers use distinct color bars and are visually prominent; subtask labels (e.g., extraction/generation/classification) are also differentiated. the aligned/conflict distinction is clear via placement and emphasis, though dense text in boxes competes with the hierarchy cues."
            },
            {
              "question": "4.6. margin: is there sufficient margin between elements?",
              "score": 0.72,
              "reason": "overall margins between major sections are adequate, but within sections several mini-panels and text boxes are tightly packed, making the figure feel busy and potentially harder to read at paper column scale."
            },
            {
              "question": "4.7. consistency: are components with similar roles represented with the same shape and color?",
              "score": 0.92,
              "reason": "role encoding is consistent: similar icons represent system/user/model/tool elements, and category color themes are applied coherently. aligned vs conflict formatting is repeated across tasks, supporting visual consistency."
            }
          ]
        },
        "Creativity": {
          "score": 0.62,
          "sub_metrics": [
            {
              "question": "5.1. metaphor: to what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.72,
              "reason": "the figure maps the abstract instruction-hierarchy setup into a concrete chat-like interface with role icons (system/user/model/tool), plus concise labels for categories and tasks. the aligned/conflict distinction is encoded via spatial grouping and color accents, which functions as a visual shorthand. however, most semantics still rely on text headers (e.g., “rule following”, “task execution”) rather than more iconic metaphors beyond the message-role symbols."
            },
            {
              "question": "5.2. novelty: does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.48,
              "reason": "the overall design resembles standard nlp benchmark overview schematics: rounded boxes, pastel section headers, chat bubbles, and small role/tool icons. the composition is clean and coherent but not especially distinctive relative to common conference-paper infographic conventions."
            },
            {
              "question": "5.3. adaptability: does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.66,
              "reason": "the layout is well adapted to the paper’s taxonomy (four categories × multiple tasks) using a grid of mini-panels that mirrors the benchmark structure and contrasts single-turn vs multi-turn where relevant. it meaningfully uses grouping, color-coding, and consistent iconography to support the hierarchy theme. still, it stays within a fairly uniform panel-based template rather than introducing a more bespoke, hierarchy-native visualization (e.g., explicit priority flow/graph) throughout."
            }
          ]
        },
        "weighted_total": 0.754
      }
    }
  ]
}