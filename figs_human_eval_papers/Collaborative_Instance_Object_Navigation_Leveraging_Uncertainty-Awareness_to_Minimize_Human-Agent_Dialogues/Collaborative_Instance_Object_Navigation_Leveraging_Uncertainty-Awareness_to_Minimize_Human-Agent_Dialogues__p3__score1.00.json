{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2412.01250v3.pdf",
  "page": 3,
  "figureType": null,
  "name": "2",
  "caption": "Figure 2. Graphical depiction of AIUTA: left shows its interaction cycle with the user, and right provides an exploded view of our method. ① The agent receives an initial instruction I: “Find a c =<object category>”. ② At each timestep t, a zero-shot policy π [53], comprising a frozen object detection module [24], selects the optimal action at. ③ Upon detection, the agent performs the proposed AIUTA. Specifically, ④ the agent first obtains an initial scene description of observation Ot from a VLM. Then, a Self-Questioner module leverages an LLM to automatically generate attribute-specific questions to the VLM, acquiring more information and refining the scene description with reduced attribute-level uncertainty, producing Srefined. ⑤ The Interaction Trigger module then evaluates Srefined against the “facts” related to the target, to determine whether to terminate the navigation (if the agent believes it has located the target object ⑥), or to pose template-free, natural-language questions to a human ⑦, updating the “facts” based on the response ⑧.",
  "regionBoundary": {
    "x1": 94.56,
    "x2": 517.4399999999999,
    "y1": 72.0,
    "y2": 235.2
  },
  "score": 1.0,
  "reason": "Depicts overall system architecture and flow of agent-user interaction with labeled components."
}