{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2024.acl-long.773.pdf",
  "page": 1,
  "figureType": null,
  "name": "2",
  "caption": "Figure 2: Comparison of previous adversarial prompts and PAP, ordered by three levels of humanizing. The first level treats LLMs as algorithmic systems: for instance, GCG (Zou et al., 2023) generates prompts with gibberish suffix via gradient synthesis; Deng et al. (2023b) exploits “side-channels” like low-resource languages. The second level progresses to treat LLMs as instruction followers: they usually rely on unconventional instruction patterns to jailbreak (e.g., virtualization or role-play), e.g., Yu et al. (2023) learn the distribution of virtualization-based jailbreak templates to produce jailbreak variants, while PAIR (Chao et al., 2023) asks LLMs to improve instructions as an “assistant” and often leads to prompts that employ virtualization or persona. We introduce the highest level to humanize and persuade LLMs as human-like communicators, and propose PAP. PAP seamlessly weaves persuasive techniques into jailbreak prompt construction, which highlights the risks associated with more complex and nuanced human-like communication to advance AI safety.",
  "regionBoundary": {
    "x1": 69.6,
    "x2": 525.12,
    "y1": 53.76,
    "y2": 166.07999999999998
  },
  "score": 1.0,
  "reason": "Diagram provides an overview of prompt types, categorization, and humanizing levels for LLMs."
}