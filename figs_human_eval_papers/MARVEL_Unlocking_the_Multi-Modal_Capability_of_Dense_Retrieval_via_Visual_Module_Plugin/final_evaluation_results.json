{
  "paper_name": "MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin",
  "evaluated_at": "2025-12-28T01:09:55.491014",
  "figure_evaluations": [
    {
      "figure_file": "MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin__p3__score1.00.png",
      "caption": "Figure 2: The Architecture of Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL). We first pretrain the visual modules using the image-caption alignment task (Figure 2(a)) and then finetune the language model to conduct multi-modal retrieval (Figure 2(b)).",
      "scores": {
        "Informativeness": {
          "score": 0.633,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage",
              "score": 0.78,
              "reason": "The figure covers the major architectural components and training stages described for MARVEL: a T5-ANCE/MS MARCO–pretrained language model, a plug-in visual module (CLIP image encoder + projection layer), visual-module adaptation pretraining via image–caption contrastive objectives (shown as L_CI and L_IC), and retrieval finetuning with modality-balanced hard negatives (L_LM) over queries, text docs, and image docs. However, it omits other major paper elements that are often central to understanding the full method, such as explicit retrieval scoring/embedding formulation, indexing/KNN search depiction, any late interaction or pooling specifics, and dataset construction details (e.g., ClueWeb22-MM pipeline) and evaluation metrics/baselines—so it is strong on model/training coverage but not exhaustive."
            },
            {
              "question": "1.2. Standalone Intelligibility",
              "score": 0.72,
              "reason": "A reader can infer the high-level operating principle: (a) align images with captions by contrastive pretraining of a visual plug-in attached to a pretrained text retriever, then (b) finetune for multi-modal retrieval using balanced hard negatives across modalities. The flow, modality inputs, and what is trained vs. frozen are visually indicated. Still, key operational details needed for full understanding are unclear from the figure alone: what representation is produced (single embedding vs. sequence), how similarity is computed, how multi-modal documents are represented at retrieval time (caption+image? image-only?), and how negatives are selected/constructed; thus it’s understandable at a conceptual level but not sufficient to reproduce or precisely follow the algorithm."
            },
            {
              "question": "1.3. Completeness",
              "score": 0.4,
              "reason": "The figure summarizes the core method pipeline (training of the visual module and finetuning for retrieval) but does not summarize the paper end-to-end. It does not include dataset creation (ClueWeb22-MM), experimental setup and results, ablations/analyses, or broader retrieval pipeline components (indexing/search). Therefore, it is not a comprehensive beginning-to-end summary of the paper, only of the proposed architecture and training regimen."
            }
          ]
        },
        "Fidelity": {
          "score": 0.877,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.85,
              "reason": "The figure largely stays within elements described in the provided context (T5-ANCE/MS MARCO pretrained LM, CLIP image encoder, projection layer as a plug-in, caption–image contrastive pretraining, then LM finetuning for multimodal retrieval). However, it introduces specific loss symbols (LCI, LIC, LLM) and a two-directional contrastive setup explicitly in the diagram; while plausible, these exact notations are not evidenced in the excerpt, so they may be diagram-specific additions beyond what is explicitly stated here."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.9,
              "reason": "The high-level pipeline matches the paper description: (a) adapt/pretrain the visual module with image–caption contrastive alignment, (b) incorporate the visual module’s image features into the text retriever (shown as a plug-in) and finetune for multimodal retrieval with modality-balanced negatives. The relationships (CLIP image encoder → projection → LM input space; unified encoder usage for query/text/image after plug-in; freezing vs finetuning separation) are consistent with the stated approach, though the exact loss decomposition and which parameters are frozen at each stage cannot be fully verified from the excerpt alone."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.88,
              "reason": "Key labels align with the paper context: MARVEL, visual module plug-in, CLIP image encoder, projection layer, MS MARCO pretrained language model, contrastive pretraining, and modality-balanced hard negative finetuning. Minor potential mismatch is that the figure labels the core as a generic 'MS MARCO Pretrained Language Model' rather than explicitly 'T5-ANCE' (which the paper identifies as the base retriever), so the labeling is slightly less precise than the paper’s naming."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.78,
          "sub_metrics": [
            {
              "question": "Overall Readability",
              "score": 0.78,
              "reason": "The figure is largely understandable and conveys a two-stage workflow (visual-module adaptation pretraining → modality-balanced finetuning) with clear panel separation (a)/(b), directional flow, and consistent visual metaphors (inputs on left, model in middle, objectives/results on right). However, readability is reduced by high visual density (many icons, labels, and example boxes in limited space), small font sizes for several key labels (loss names, example captions/text doc snippet), and reliance on color cues (green checkmarks, colored boxes) that may be less accessible in grayscale or for color-vision deficiencies. Some elements feel busy (multiple example images/captions per panel) and compete with the main message, slightly increasing cognitive load despite an overall coherent structure."
            }
          ]
        },
        "Design Quality": {
          "score": 0.833,
          "sub_metrics": [
            {
              "question": "4.1. Direction: Does the diagram flow from left to right or top to bottom?",
              "score": 0.85,
              "reason": "Both subfigures clearly read left-to-right (inputs → model/plugin → training objective/output). The (a)/(b) stacking adds an overall top-to-bottom progression, though the dual-direction structure may require a brief moment to parse."
            },
            {
              "question": "4.2. Crossing: Do the connection lines avoid crossing each other?",
              "score": 0.9,
              "reason": "Arrows/links largely do not cross; the few multi-input arrows converge cleanly into the model blocks. Visual separation between the two panels also helps prevent perceived crossings."
            },
            {
              "question": "4.3. Proximity: Are functionally closely related modules physically placed near each other?",
              "score": 0.9,
              "reason": "Related elements are grouped well: captions/images on the left, the language model with the visual plugin centrally, and the corresponding objectives/results on the right. Within the plugin, CLIP encoder and projection layer are colocated and encapsulated."
            },
            {
              "question": "4.4. Alignment: Are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.78,
              "reason": "Major blocks follow a clean horizontal layout in each panel, but some internal elements (e.g., example boxes/images on the right) have slight misalignments and uneven baselines, reducing grid-like neatness."
            },
            {
              "question": "4.5. Hierarchy: Do the important main components stand out in size, thickness, or position?",
              "score": 0.88,
              "reason": "The central model blocks are prominent (large, boxed, with labels), and the two-stage structure (pretrain vs finetune) is clearly separated. However, multiple highlights (colors, icons, flames/checkmarks) compete for attention somewhat."
            },
            {
              "question": "4.6. Margin: Is there sufficient margin between elements?",
              "score": 0.7,
              "reason": "Overall spacing is adequate, but the right-side example regions (images, captions, and text box) feel dense, with tight padding inside some boxes and limited whitespace between adjacent annotated elements."
            },
            {
              "question": "4.7. Consistency: Are components with similar roles represented with the same shape and color?",
              "score": 0.82,
              "reason": "Consistent use of boxed modules for model components and consistent left-input iconography across panels. Some semantic encoding is consistent (e.g., orange for images), but mixed emphasis styles (green checks, red outlines, blue/purple text boxes) are not always systematically mapped to a single role."
            }
          ]
        },
        "Creativity": {
          "score": 0.543,
          "sub_metrics": [
            {
              "question": "5.1. Metaphor: To what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.58,
              "reason": "The figure uses concrete visual metaphors (plug-in block, snowflake/fire for frozen/trainable components, checkmarks for positives, thumbnail images for modalities) to stand in for abstract training/architecture concepts. However, most of the abstraction is still conveyed through standard boxes/arrows and explicit text labels rather than richer symbolic encoding, so the metaphor layer is present but not especially deep."
            },
            {
              "question": "5.2. Novelty: Does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.42,
              "reason": "Overall styling matches a familiar deep-learning schematic template: modular blocks, dashed plug-in boundary, arrows, and (a)/(b) subpanels. The snowflake/fire glyph convention and inclusion of real image thumbnails add some distinctiveness, but these are also increasingly common in recent vision-language papers; the visual language is not strongly unique."
            },
            {
              "question": "5.3. Adaptability: Does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.63,
              "reason": "The two-stage layout (adaptation pretraining in (a) and modality-balanced finetuning in (b)) is tailored to MARVEL’s method and clearly separates objectives and data flows, which reflects paper-specific needs rather than a single generic pipeline. Still, the composition remains within standard left-to-right block-diagram conventions without major departures in form."
            }
          ]
        },
        "weighted_total": 0.733
      }
    },
    {
      "figure_file": "MARVEL_Unlocking_the_Multi-Modal_Capability_of_Dense_Retrieval_via_Visual_Module_Plugin__p0__score1.00.png",
      "caption": "Figure 1: Retrieval Pipeline with Our MARVEL Model. MARVEL incorporates the visual module plugin, aiming to unlock the multi-modal capabilities of well trained dense retrieval model.",
      "scores": {
        "Informativeness": {
          "score": 0.583,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage: Does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.65,
              "reason": "The figure conveys the key high-level components: (i) a dense retriever backbone (T5-ANCE) pretrained on MS MARCO, (ii) a visual module plugin trained via image–text contrastive alignment, (iii) unified encoding of queries and multimodal documents, and (iv) KNN search producing a ranking list over text and image docs. However, it omits several major elements described in the paper such as architectural specifics of how image features are injected into T5-ANCE (e.g., mapping/projection details), what is frozen vs. fine-tuned, training objectives/negative sampling and retrieval loss definitions, and dataset construction details (WebQA, ClueWeb22-MM). No formulas are shown."
            },
            {
              "question": "1.2. Standalone Intelligibility: Can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.75,
              "reason": "A reader can infer the overall pipeline: queries and multimodal documents are embedded by MARVEL; images are processed by a vision module; contrastive pretraining aligns image and text representations; then embeddings are used for KNN retrieval to produce a ranked list mixing modalities. Still, several aspects are ambiguous without the text: what exactly constitutes a “document” embedding when both text and image exist, how/where the vision module output is integrated with the language encoder, and whether the system supports joint multimodal documents vs. separate image-doc and text-doc collections."
            },
            {
              "question": "1.3. Completeness: Is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.35,
              "reason": "The figure is an overview of the approach, not a full paper summary. It does not cover the later-paper elements: ClueWeb22-MM dataset creation pipeline, experimental setup and baselines, quantitative results (MRR gains), ablations/analyses (e.g., attention/overfitting discussion, mapping image semantics into embedding space), or implementation/training strategy details. Thus it is not complete from start-to-end of the paper."
            }
          ]
        },
        "Fidelity": {
          "score": 0.857,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.85,
              "reason": "The figure largely matches the paper’s described pipeline (T5-ANCE/MS MARCO pretraining, adding a visual module plugin, image-caption contrastive pretraining, and retrieval via embedding/KNN). A mild potential over-specification is that it visually suggests three distinct encoders/boxes and a particular staging (“MARVEL Encoders” then “Vision Module”) that is more schematic than explicitly enumerated in the text; however, no clearly alien formulas or unrelated modules are introduced."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.82,
              "reason": "Core relationships are consistent: a text dense retriever (T5-ANCE) is pretrained on MS MARCO; a visual module is pretrained with image–text contrastive alignment; image features are incorporated as inputs to the retriever to enable unified encoding for queries and multimodal documents; retrieval is done with KNN over embeddings producing a ranked list. Slight ambiguity: the figure’s “Query” and “Document” boxes under “MARVEL Encoders” may imply both are encoded identically and that the vision module feeds only the document side; the paper emphasizes incorporating visual features as inputs to the unified encoder for multimodal documents, which the figure mostly conveys but could be interpreted as a more modular/late attachment than described."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.9,
              "reason": "Labels align well with the paper: “MARVEL,” “Vision Module,” “Contrastive Training/Align” (image-caption contrastive pretraining), “MS MARCO” (pretraining corpus for T5-ANCE), “KNN Search,” and modality icons for queries/image docs/text docs are consistent with the described method. Minor issue: “MARVEL Encoders” is not a precise named component in the text (the paper frames it as a unified encoder based on T5-ANCE plus a visual module plugin), but it is a reasonable shorthand rather than a mislabel."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.78,
          "sub_metrics": [
            {
              "question": "Overall Readability",
              "score": 0.78,
              "reason": "The figure is largely readable and communicates the end-to-end pipeline (queries + image/text docs → MARVEL encoders + vision module plugin → embedding space → KNN search → ranking). The iconography and color-coding help, and the main blocks are visually separated. However, readability is reduced by (i) small text inside the dashed modules (e.g., 'MS MARCO', 'Contrastive Training', 'Align'), (ii) multiple visual metaphors (several icons, 3D embedding plot, stacked bars) competing for attention, and (iii) some ambiguous flow/semantics (e.g., how 'Pretrain' relates to contrastive alignment vs. retrieval training; where exactly the vision module injects features). Overall, it functions well as an overview but would benefit from slightly larger fonts, fewer simultaneous motifs, and clearer arrow labeling to minimize interpretation effort."
            }
          ]
        },
        "Design Quality": {
          "score": 0.807,
          "sub_metrics": [
            {
              "question": "4.1. Direction: Does the diagram flow from left to right or top to bottom?",
              "score": 0.75,
              "reason": "The overall pipeline is mostly top-to-bottom (inputs at top, model/contrastive training in middle, retrieval output at bottom-right), with a secondary left-to-right progression inside the main box. However, multiple arrows in different directions (including lateral and diagonal arrows) slightly weakens an unambiguous primary reading direction."
            },
            {
              "question": "4.2. Crossing: Do the connection lines avoid crossing each other?",
              "score": 0.9,
              "reason": "Arrows and connectors are largely routed to avoid intersections; there are no prominent line crossings. Some connectors come close to other elements/borders but do not materially cross, keeping visual tracing straightforward."
            },
            {
              "question": "4.3. Proximity: Are functionally closely related modules physically placed near each other?",
              "score": 0.85,
              "reason": "Related components are grouped: inputs and legend are near the top, encoders are grouped within the MARVEL box, and the contrastive training block groups images/text alignment. The KNN search and ranking list are adjacent. Minor separation exists between the 'Vision Module' and the contrastive training block despite their relationship, but the connection is still clear."
            },
            {
              "question": "4.4. Alignment: Are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.7,
              "reason": "Key modules (Query/Document, contrastive training panel, Vision Module, ranking list) are generally aligned, but several elements (icons, arrows, the 3D embedding axes, and some labels) are not consistently snapped to a common grid, creating slight visual irregularity."
            },
            {
              "question": "4.5. Hierarchy: Do the important main components stand out in size, thickness, or position?",
              "score": 0.85,
              "reason": "The dashed main container and the large blocks (encoders, contrastive training, vision module, ranking list) establish clear hierarchy; the title-like labels and boxed regions help. Some smaller icons and decorative elements (e.g., embedding axes) compete mildly for attention, but main components still dominate."
            },
            {
              "question": "4.6. Margin: Is there sufficient margin between elements?",
              "score": 0.7,
              "reason": "Within the main dashed box, spacing is generally adequate, but a few areas feel tight: the top row of icons/labels, arrows near box boundaries, and the lower-left embedding plot adjacent to the KNN arrow. Margins are acceptable but not generous."
            },
            {
              "question": "4.7. Consistency: Are components with similar roles represented with the same shape and color?",
              "score": 0.9,
              "reason": "Color coding is consistent (query/star orange-red, image docs yellow/orange, text docs blue), and similar roles use similar icon styles. Boxed modules follow a coherent visual language. Minor inconsistency arises from mixed illustration styles (photo thumbnails vs. schematic icons), but role encoding remains consistent."
            }
          ]
        },
        "Creativity": {
          "score": 0.55,
          "sub_metrics": [
            {
              "question": "5.1. Metaphor: To what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.72,
              "reason": "The pipeline is communicated largely through concrete icons and symbols (query star, image/text document icons, encoder blocks, KNN search scatter plot, ranking list). These effectively stand in for abstract components like modalities, retrieval space, and ranking. However, several key ideas (e.g., “align”, “contrastive training”, “vision module plugin”) remain mostly text-labeled rather than metaphorically depicted beyond arrows/boxes."
            },
            {
              "question": "5.2. Novelty: Does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.38,
              "reason": "The figure uses a fairly standard ML system-diagram aesthetic: dashed container, labeled boxes, arrows, and generic modality icons. While clean and readable, the visual language closely resembles common conference paper pipeline schematics, with limited distinctive stylistic elements or unconventional graphical encoding."
            },
            {
              "question": "5.3. Adaptability: Does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.55,
              "reason": "The layout is tailored to the paper’s story (pretraining encoder, contrastive image-text alignment, then retrieval/ranking), and it integrates modality-specific cues (image/text docs) to support the MARVEL “plugin” narrative. Still, it largely follows a conventional top-down pipeline structure and common modular partitioning, without major departures in layout strategy."
            }
          ]
        },
        "weighted_total": 0.715
      }
    }
  ]
}