{
  "paper_name": "Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective",
  "evaluated_at": "2025-12-28T01:06:51.394316",
  "figure_evaluations": [
    {
      "figure_file": "Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective__p0__score0.60.png",
      "caption": "Figure 1: Top: An example from the LLaVA instruction data. The training data can be overly detailed to exceed the model’s visual perception limits. Bottom: Average log-likelihood of the LLaVA (7b) model predicting EOS at positions labeled as EOS during instruction tuning. Training the model with overly detailed data leads to a decrease in its tendency to stop generation.",
      "scores": {
        "Informativeness": {
          "score": 0.3,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage: Does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.25,
              "reason": "The figure conveys one core motivation/observation (overly detailed instruction data correlates with reduced EOS likelihood during tuning) using a qualitative example and a simple training-curve plot. However, it omits most major technical components discussed in the paper context (e.g., the EOS-decision analysis methodology, saliency/information-flow analysis, Selective EOS Supervision objective, Scoring EOS Supervision metrics, and the data filtering strategy), and contains no formulas."
            },
            {
              "question": "1.2. Standalone Intelligibility: Can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.55,
              "reason": "As a standalone, it is reasonably clear that: (i) training data can be overly detailed relative to visual evidence, and (ii) during instruction tuning the model becomes less likely to output EOS (lower log-likelihood), which is linked to longer generation and potential hallucination. But it does not explain the system/approach for mitigation (how EOS decisions are modeled/used, what is changed in training, what filtering does), so the operating principle of the proposed solution is not understandable from this figure alone."
            },
            {
              "question": "1.3. Completeness: Is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.1,
              "reason": "No. The figure only captures an early motivating example and one empirical trend about EOS likelihood during instruction tuning. It does not summarize the full paper arc (analysis of EOS decision behavior, proposed objectives/metrics, filtering pipeline, experimental evaluations, and results)."
            }
          ]
        },
        "Fidelity": {
          "score": 0.9,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.95,
              "reason": "The figure contains only elements described in the paper context and caption: an example of overly detailed LLaVA instruction data (top) and a plot of average log-likelihood for predicting EOS at EOS-labeled positions over instruction-tuning epochs (bottom). No extraneous methods, formulas, or unexplained components are introduced beyond what the caption states."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.85,
              "reason": "The intended relationship—overly detailed training data correlates with reduced tendency to stop generation—is consistent with the caption and the downward trend of EOS log-likelihood across epochs. However, the figure itself does not explicitly operationalize or quantify “overly detailed” in the plotted data, so the causal phrasing (“Training the model with overly detailed data leads to...”) is not directly evidenced by a comparison condition within the figure."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.9,
              "reason": "Key labels align with the paper description: 'Training Data' example from LLaVA instruction data, 'Log-likelihood' on the y-axis, 'Epoch' on the x-axis, and mention of 'LLaVA (7b)' and 'EOS'. Minor ambiguity remains because the plot does not label the specific dataset subset as “overly detailed” beyond the top-panel tag, but the terminology itself matches the paper’s framing."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.823,
          "sub_metrics": [
            {
              "question": "3.1. Summarization: Is it schematized focusing on the 'Main Contribution' rather than trivial details?",
              "score": 0.78,
              "reason": "The figure cleanly conveys the central idea (overly detailed training data reduces EOS-stopping tendency) by pairing a representative data example with a single trend plot. However, the text block in the top panel is somewhat long, and the plot’s y-axis (log-likelihood) is not immediately interpretable without careful reading, which slightly weakens the schematic focus."
            },
            {
              "question": "3.2. Contextual Match: Does this figure function well as a supplementary material to help understanding when reading the caption or text?",
              "score": 0.86,
              "reason": "With the caption, the top/bottom structure works well: the qualitative example motivates the hypothesis and the quantitative curve supports it. The visual mapping from ‘overly detailed’ to ‘decreased EOS likelihood across epochs’ is understandable and supportive of the surrounding argument, though a brief in-figure label clarifying what lower log-likelihood implies (less stopping) would further reduce cognitive load."
            },
            {
              "question": "3.3. Redundancy: Does it avoid decorative elements or unnecessary information unrelated to the core ideas?",
              "score": 0.83,
              "reason": "The components are largely purposeful (example + measured effect). Minor redundancy comes from including a fairly large image and extensive quoted training text; readability could improve by tightening the excerpt or highlighting only the relevant over-detailed phrases, but there are no obviously decorative, unrelated elements."
            }
          ]
        },
        "Design Quality": {
          "score": 0.836,
          "sub_metrics": [
            {
              "question": "4.1. Direction: Does the diagram flow from left to right or top to bottom?",
              "score": 0.85,
              "reason": "The figure has a clear top (example/data snippet) and bottom (training trend plot) organization, which reads naturally top-to-bottom; within the top row it also reads left-to-right (image then text card)."
            },
            {
              "question": "4.2. Crossing: Do the connection lines avoid crossing each other?",
              "score": 1.0,
              "reason": "There are no connecting arrows/edges between components, so no line-crossing issues arise."
            },
            {
              "question": "4.3. Proximity: Are functionally closely related modules physically placed near each other?",
              "score": 0.9,
              "reason": "The image and its corresponding training-data excerpt are adjacent in the top panel, and the bottom panel directly supports the claim about EOS tendency; related elements are grouped well."
            },
            {
              "question": "4.4. Alignment: Are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.8,
              "reason": "Top-row cards are cleanly aligned, and the plot occupies a consistent lower block; minor misalignment comes from the rounded-card styling and unequal internal padding between the image card and text card."
            },
            {
              "question": "4.5. Hierarchy: Do the important main components stand out in size, thickness, or position?",
              "score": 0.75,
              "reason": "The two-part structure is clear, but the key takeaway (decreasing EOS log-likelihood) is not strongly emphasized beyond the line itself; the “overly detailed” tag helps but competes with other text density."
            },
            {
              "question": "4.6. Margin: Is there sufficient margin between elements?",
              "score": 0.7,
              "reason": "Overall spacing is adequate, but the bottom plot is relatively tight vertically, and the y-axis label/points and surrounding whitespace feel slightly cramped compared to the generous padding of the top cards."
            },
            {
              "question": "4.7. Consistency: Are components with similar roles represented with the same shape and color?",
              "score": 0.85,
              "reason": "The top elements share consistent rounded-card styling and muted palette; however, the plot uses a different visual language (standard axes and markers), and the highlight color/typography for emphasized text is not reused elsewhere."
            }
          ]
        },
        "Creativity": {
          "score": 0.407,
          "sub_metrics": [
            {
              "question": "5.1. Metaphor: To what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.25,
              "reason": "The figure primarily uses literal elements (example image, excerpted training text, and a log-likelihood vs. epoch plot). The only mild metaphorical cue is the qualitative label/highlight “overly detailed,” which flags the abstract notion of “exceeding perception limits,” but it is not encoded through distinctive symbols/icons or compact metaphorical visual language."
            },
            {
              "question": "5.2. Novelty: Does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.35,
              "reason": "The composition follows a common ML-paper pattern: illustrative example panel paired with a simple quantitative trend plot. Styling (rounded boxes, highlight annotation) is clean but standard; it does not introduce a notably distinctive visual idiom beyond typical conference-figure aesthetics."
            },
            {
              "question": "5.3. Adaptability: Does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.62,
              "reason": "The layout is purpose-fit: it juxtaposes a concrete training-data example (with emphasized “overly detailed” portion) directly above the EOS log-likelihood trend, making the causal narrative (detail level ↔ stopping tendency) immediately legible. While still within conventional figure structures, the tailored pairing and annotation choices reflect adaptation to the paper’s specific argument."
            }
          ]
        },
        "weighted_total": 0.653
      }
    },
    {
      "figure_file": "Less_is_More_Mitigating_Multimodal_Hallucination_from_an_EOS_Decision_Perspective__p4__score0.70.png",
      "caption": "Figure 4: Illustration of the probability distribution derived from our proposed Selective EOS Supervision. Arrows indicate the maximizing and minimizing effects of the training objective on the probability of each word. When the label is not EOS, the EOS token is excluded from the probability distribution.",
      "scores": {
        "Informativeness": {
          "score": 0.34,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage: Does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.32,
              "reason": "The figure conveys a single core idea: how Selective EOS Supervision modifies the probability distribution/training signal by excluding EOS when the label is non-EOS, and encouraging EOS when the label is EOS. However, it does not cover other major paper components (e.g., the EOS-decision analysis methodology, saliency/information-flow setup, Scoring EOS Supervision and data filtering metrics/strategy, experimental settings/results), nor does it present explicit formulas/objectives beyond a qualitative schematic."
            },
            {
              "question": "1.2. Standalone Intelligibility: Can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.58,
              "reason": "With the caption, a reader can infer the local operating principle of the proposed objective: probability mass is selectively maximized/minimized with special handling of EOS depending on whether the target is EOS. But the exact loss definition, how the distribution is altered (renormalization, masking), and how this connects to hallucination mitigation and EOS decision behavior are not fully recoverable from the schematic alone."
            },
            {
              "question": "1.3. Completeness: Is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.12,
              "reason": "The figure is not a paper-level summary; it focuses narrowly on one training-objective mechanism (Selective EOS Supervision) and omits the broader narrative and contributions across the paper (analysis, hypotheses/validation, filtering strategy, evaluations, and conclusions)."
            }
          ]
        },
        "Fidelity": {
          "score": 0.873,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.86,
              "reason": "The figure is a conceptual illustration (probability masses p1…py…pEOS with maximize/minimize arrows) consistent with describing a modified training objective. It does not introduce extra modules or unrelated formulas. Minor risk: the depiction of an explicit “excluded from the probability distribution” operation could be interpreted as a specific normalization/masking mechanism; if the paper’s formalism is more nuanced (e.g., masking EOS only in loss rather than in the distribution itself), the graphic may slightly over-specify."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.82,
              "reason": "The left/right panels correctly communicate the intended conditional behavior: when y≠EOS, training increases probability of the labeled token while suppressing others and treats EOS differently (excluded); when y=EOS, training increases EOS probability. However, the graphic’s phrasing/visual implies EOS is removed from the distribution (as opposed to removed from competing negatives in the loss), and the maximize/minimize effects on “each word” are schematic rather than a precise relationship to the actual objective; this may blur the exact mathematical relation if the paper defines a specific selective loss."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.94,
              "reason": "Key method name “Selective EOS Supervision” is correctly referenced in the caption and matches the paper context. EOS is consistently labeled (pEOS, y=EOS, y≠EOS). Token probabilities (p1…pj…py) are generic but not misleading for a distribution illustration."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.867,
          "sub_metrics": [
            {
              "question": "3.1. Summarization: Is it schematized focusing on the 'Main Contribution' rather than trivial details?",
              "score": 0.86,
              "reason": "The schematic cleanly abstracts the key mechanism of Selective EOS Supervision (how probability mass is encouraged/suppressed depending on whether the label is EOS) without drifting into implementation-level details. The only mild readability cost is that the symbols (p1…pj…pEOS, y≠EOS vs y=EOS) require prior familiarity with token-level objectives, so the main contribution is present but not fully self-evident at a glance."
            },
            {
              "question": "3.2. Contextual Match: Does this figure function well as a supplementary material to help understanding when reading the caption or text?",
              "score": 0.82,
              "reason": "With the caption, it effectively complements the text by visually contrasting the two training cases and the inclusion/exclusion of EOS in the distribution. However, without surrounding context it is somewhat ambiguous what “maximize/minimize” precisely means (e.g., relative to which loss term), and the dotted placeholder for intermediate tokens may leave readers unsure about how broad the effect is across the vocabulary."
            },
            {
              "question": "3.3. Redundancy: Does it avoid decorative elements or unnecessary information unrelated to the core ideas?",
              "score": 0.92,
              "reason": "The design is minimal and largely free of decorative clutter; all elements (two-panel comparison, arrows, highlighted bars) serve the conceptual point. A small redundancy is the repeated p1…pj labeling and multiple arrows, which slightly increases visual parsing effort but does not introduce unrelated content."
            }
          ]
        },
        "Design Quality": {
          "score": 0.907,
          "sub_metrics": [
            {
              "question": "4.1. Direction: Does the diagram flow from left to right or top to bottom?",
              "score": 0.92,
              "reason": "The two panels read clearly left-to-right: token probabilities progress from p1…py…pEOS (left) and p1…pj…pEOS (right), with labels under each panel clarifying the condition."
            },
            {
              "question": "4.2. Crossing: Do the connection lines avoid crossing each other?",
              "score": 1.0,
              "reason": "There are no explicit connecting lines; arrows are placed above elements without intersections or visual crossings."
            },
            {
              "question": "4.3. Proximity: Are functionally closely related modules physically placed near each other?",
              "score": 0.9,
              "reason": "Within each panel, the probability bars and their corresponding arrows/labels are colocated; the two conditions are separated but adjacent, supporting comparison."
            },
            {
              "question": "4.4. Alignment: Are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.9,
              "reason": "The bar-like blocks are evenly spaced and aligned along a baseline; text labels (p1, p2, …) are aligned beneath. Minor perceived misalignment comes from differing bar heights and the dotted placeholder."
            },
            {
              "question": "4.5. Hierarchy: Do the important main components stand out in size, thickness, or position?",
              "score": 0.82,
              "reason": "Key ideas (maximize/minimize and EOS vs non-EOS cases) are highlighted via bold/dark bars and top labels. However, the hierarchy between normal tokens, special EOS behavior, and excluded EOS (left panel) is conveyed mostly by caption/annotation rather than strong visual emphasis."
            },
            {
              "question": "4.6. Margin: Is there sufficient margin between elements?",
              "score": 0.86,
              "reason": "Internal spacing between bars and arrows is generally adequate and uncluttered. The top labels and arrows are somewhat tight vertically, and the panel boundary between left/right could be more pronounced to prevent slight crowding at the center."
            },
            {
              "question": "4.7. Consistency: Are components with similar roles represented with the same shape and color?",
              "score": 0.95,
              "reason": "Both panels use the same bar shapes and color scheme (light vs dark) and consistent arrow glyphs for effects; placeholders (ellipsis, dotted box) are also consistent in intent."
            }
          ]
        },
        "Creativity": {
          "score": 0.443,
          "sub_metrics": [
            {
              "question": "5.1. Metaphor: To what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.55,
              "reason": "The figure uses concrete visual surrogates (token probability bars p1…pEOS, arrows indicating maximize/minimize, and the explicit y=EOS vs y≠EOS split) to stand in for the abstract training-objective behavior. However, it remains a fairly literal schematic of a distribution/objective rather than employing richer symbolic metaphor beyond standard arrows-and-blocks notation."
            },
            {
              "question": "5.2. Novelty: Does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.32,
              "reason": "The style is a conventional ML paper schematic: minimal grayscale blocks, ellipses for omitted items, and directional arrows. It is clear but not visually distinctive; it resembles common template figures used to explain loss shaping or probability redistribution."
            },
            {
              "question": "5.3. Adaptability: Does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.46,
              "reason": "The side-by-side comparison (y≠EOS vs y=EOS) is tailored to the paper’s central mechanism (selectively including/excluding EOS in the distribution) and directly supports the narrative. Still, the overall layout follows a standard comparative schematic pattern rather than introducing a notably customized visual organization."
            }
          ]
        },
        "weighted_total": 0.686
      }
    }
  ]
}