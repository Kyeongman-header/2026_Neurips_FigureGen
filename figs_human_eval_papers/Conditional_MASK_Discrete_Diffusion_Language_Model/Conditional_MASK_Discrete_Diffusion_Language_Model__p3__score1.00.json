{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2025.emnlp-main.450.pdf",
  "page": 3,
  "figureType": null,
  "name": "2",
  "caption": "Figure 2: Overview of the training (forward) and inference (backward) processes in Diffusion-EAGS. Training (left): Entropy-based Noise Scheduling (ENS) determines which tokens in the masked sequence, denoted by [M ], should be denoised at each timestep based on the position entropy H(xi). These tokens are then generated using the diffusion model with parameters θ, and the loss is computed using a cross-entropy (C.E.) diffusion loss. Inference (right): Starting from a fully masked sequence conditioned on Y , Entropy-Adaptive Gibbs Sampling (EAGS) iteratively refines the sequence by focusing on high-entropy tokens, denoted as Mt, based on a threshold τt, yielding stable and coherent text generation.",
  "regionBoundary": {
    "x1": 69.6,
    "x2": 525.12,
    "y1": 69.6,
    "y2": 235.2
  },
  "score": 1.0,
  "reason": "Diagram presents an overall framework showing both training and inference system processes."
}