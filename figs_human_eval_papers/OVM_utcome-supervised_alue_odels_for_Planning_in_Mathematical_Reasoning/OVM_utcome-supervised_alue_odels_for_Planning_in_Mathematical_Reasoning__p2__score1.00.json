{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2024.findings-naacl.55.pdf",
  "page": 2,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: (a): When evaluating partial paths (here for the first two steps), reward focuses on the current states, while value focuses on the unseen future outcomes. (b): Given a question q and a solution path [s1, · · · , sm, a], models are trained to predict path correctness (circled output scalar on the last token). Outcome supervision replicates the final answer’s correctness label across all steps (indicated by shaded labels), causing the model to implicitly learn to foresee the future, predicting values for partial paths. By contrast, process supervision details per-step correctness labels, causing the model to learn to predict step-level correctness, i.e. reward. Correct steps and answers are colored in yellow and incorrect ones in grey.",
  "regionBoundary": {
    "x1": 77.75999999999999,
    "x2": 520.3199999999999,
    "y1": 69.6,
    "y2": 235.2
  },
  "score": 1.0,
  "reason": "Illustrates the overall architecture of reward/value estimation, outcome, and process supervision in a system."
}