{
  "paper_name": "LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation",
  "evaluated_at": "2025-12-28T01:00:31.789022",
  "figure_evaluations": [
    {
      "figure_file": "LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation__p4__score1.00.png",
      "caption": "Figure 2: A step-by-step illustration of constructing the ABDD.",
      "scores": {
        "Informativeness": {
          "score": 0.44,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage: Does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.45,
              "reason": "Figure 2 focuses specifically on the ABDD construction pipeline (ground truth/origin context → conflict entity selection via Wiki attributes → corpus retrieval/selection → LLM rewriting to create conflict context → packaging fields like question/ground-truth/origin/conflict entity). It does not cover other major paper components such as the proposed Authority Bias metrics (Inaccuracy/Correctiveness/Misleading ratios), the experimental setup/results across six LLMs, or the mitigation framework CDEQ. No formulas/metric definitions are included."
            },
            {
              "question": "1.2. Standalone Intelligibility: Can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.67,
              "reason": "The step-by-step flow is visually clear: starting from a QA instance with ground truth and original context, selecting a conflicting entity using structured knowledge (Wiki attributes like person/date), forming a corpus/selection around the conflict entity, using an LLM to produce a conflict context, and outputting a dataset instance containing both origin and conflict contexts plus metadata. However, key operational specifics are implicit (how the conflict entity is chosen, constraints to preserve style/structure, how LLM rewriting is prompted/validated, and what guarantees the conflict targets the answer-bearing fact), limiting full understanding without the text."
            },
            {
              "question": "1.3. Completeness: Is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.2,
              "reason": "No. The figure covers only the dataset construction portion (ABDD). It does not summarize the paper’s end-to-end storyline: the Authority Bias phenomenon characterization, the full dataset/metric suite, quantitative evaluations across models/tasks, comparisons to baselines, and the proposed mitigation method (CDEQ) and its results."
            }
          ]
        },
        "Fidelity": {
          "score": 0.9,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.9,
              "reason": "The figure presents a procedural pipeline for constructing ABDD using elements consistent with the paper context (Question, Origin Context, Ground Truth, Wiki/corpus lookup, selecting a conflict entity, generating conflict context, and packaging fields). It does not introduce formulas or unrelated modules beyond what is implied by the ABDD construction description. Minor ambiguity remains because the exact use of an LLM in the construction pipeline (as drawn) is not fully verifiable from the provided excerpt alone."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.85,
              "reason": "The relationships shown are coherent: starting from (Question, Ground Truth) and an original context span, then using an external resource (Wiki/corpus) to identify a conflicting entity (e.g., replacing 'Saint Peter' with 'Mary Quant'), producing a conflict context, and forming the final dataset tuple (Question, Ground Truth, Conflict Entity, Origin Context, Conflict Context). This aligns with the stated goal of a conflict construction method derived from SQuAD. Slight uncertainty remains about whether the LLM is actually used to generate/validate the conflict context versus simpler rule-based substitution, which the figure implies."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.95,
              "reason": "Labels such as 'Question', 'Ground Truth', 'Origin Context', 'Conflict Context', 'Wiki', 'Corpus', and 'Conflict Entity' are clear and match the described dataset construction concept (ABDD). The caption correctly states it is a step-by-step illustration of constructing the ABDD. No evident mislabeling of major components is present."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.72,
          "sub_metrics": [
            {
              "question": "Overall Readability",
              "score": 0.72,
              "reason": "The figure is largely readable and communicates a left-to-right pipeline, but readability is reduced by small text (especially paragraph snippets within boxes), dense labeling, and multiple similarly styled elements that are easy to visually conflate. Color helps differentiate (e.g., conflict in red), yet some labels/icons are tiny and require zooming; spacing and alignment are adequate but not optimized for quick scanning. Overall, the main flow is understandable, though fine details are hard to parse at typical paper viewing size."
            }
          ]
        },
        "Design Quality": {
          "score": 0.85,
          "sub_metrics": [
            {
              "question": "4.1. Direction: Does the diagram flow from left to right or top to bottom?",
              "score": 0.95,
              "reason": "The pipeline clearly progresses left-to-right with sequential arrows and staged panels, making the construction process easy to follow."
            },
            {
              "question": "4.2. Crossing: Do the connection lines avoid crossing each other?",
              "score": 0.9,
              "reason": "Connections are primarily simple arrows between adjacent blocks with no evident line crossings; any branching is visually separated and does not tangle."
            },
            {
              "question": "4.3. Proximity: Are functionally closely related modules physically placed near each other?",
              "score": 0.85,
              "reason": "Inputs/outputs within each step are grouped closely (e.g., contexts and their labels), though some semantic groupings (e.g., ground-truth sourcing vs. corpus creation) could be even tighter to reduce eye travel across mini-panels."
            },
            {
              "question": "4.4. Alignment: Are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.75,
              "reason": "Major blocks are generally aligned along a horizontal baseline, but within-step elements (small boxes/icons) show slight uneven vertical alignment and spacing, reducing perceived neatness."
            },
            {
              "question": "4.5. Hierarchy: Do the important main components stand out in size, thickness, or position?",
              "score": 0.8,
              "reason": "Primary stages are separated into clear panels with arrows, and the red conflict context draws attention; however, the relative importance of intermediate artifacts (e.g., corpus, wiki attributes) is not strongly differentiated by size/weight."
            },
            {
              "question": "4.6. Margin: Is there sufficient margin between elements?",
              "score": 0.8,
              "reason": "Overall whitespace is adequate, but some internal clusters appear tight (notably around the wiki/attribute icons and small labeled boxes), which can make the step feel slightly crowded at paper scale."
            },
            {
              "question": "4.7. Consistency: Are components with similar roles represented with the same shape and color?",
              "score": 0.9,
              "reason": "Context boxes use consistent rounded-rectangle styling; the conflict context is consistently highlighted in red while normal context uses neutral tones. Minor inconsistency arises from mixed icon styles (attribute icons vs. text boxes), but roles remain mostly clear."
            }
          ]
        },
        "Creativity": {
          "score": 0.517,
          "sub_metrics": [
            {
              "question": "5.1. Metaphor: To what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.62,
              "reason": "The figure uses concrete, easily decoded visual metaphors (boxed nodes, arrows, color-coding for conflict, and entity/attribute icons like Person/Date) to represent an otherwise abstract dataset-construction pipeline. However, the metaphors remain fairly standard for NLP/IR pipelines and rely more on flowchart conventions than on richer symbolic/iconic encodings."
            },
            {
              "question": "5.2. Novelty: Does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.38,
              "reason": "Stylistically, it largely follows a common ACL-style schematic: left-to-right process blocks, rounded rectangles, minimal icon set, and simple emphasis via red highlight. The distinctive element is modest (the conflict context emphasized in red and the specific stepwise depiction of conflict injection), but overall it resembles typical dataset/pipeline diagrams."
            },
            {
              "question": "5.3. Adaptability: Does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.55,
              "reason": "The layout is appropriately tailored to the paper’s contribution (a step-by-step ABDD construction narrative), and the sequential visual story aligns well with the method description. Still, it does not significantly break from uniform design principles beyond straightforward stepwise decomposition; it remains a conventional linear pipeline rather than a more customized structure (e.g., emphasizing decision points, parallelism, or feedback loops) that could more uniquely reflect the method’s nuances."
            }
          ]
        },
        "weighted_total": 0.685
      }
    },
    {
      "figure_file": "LLMs_Trust_Humans_More_That_s_a_Problem_Unveiling_and_Mitigating_the_Authority_Bias_in_Retrieval-Augmented_Generation__p1__score1.00.png",
      "caption": "Figure 1: Illustration of Authority Bias in RAG systems. In simple queries, the LLM relies solely on database knowledge for the answer. However, in more complex scenarios with conflicting user-provided and database knowledge, the LLM tends to favor the user’s input, even if incorrect. We characterize this phenomenon of LLMs in RAG as Authority Bias.",
      "scores": {
        "Informativeness": {
          "score": 0.55,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage: Does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.55,
              "reason": "The figure clearly covers the core RAG pipeline components relevant to the claimed phenomenon (user query/context, retrieval/database with chunks, prompt combination, LLM generation) and contrasts a non-conflict vs conflict case to illustrate authority bias. However, it omits major paper elements beyond the illustrative phenomenon: the ABDD dataset construction, the three proposed metrics (Inaccuracy/Correctiveness/Misleading Ratios), and the mitigation framework (CDEQ: conflict localization, credibility assessment, query enhancement). No formulas/metrics are visualized."
            },
            {
              "question": "1.2. Standalone Intelligibility: Can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.85,
              "reason": "Yes for the main idea: it is easy to infer the RAG workflow and the key experimental manipulation (adding user-provided conflicting context) and the observed failure mode (model outputs user-asserted but incorrect answer). The comparison structure (simple query → correct; conflict prompt → biased) and labeling (Authority Bias) make the operating principle and intended takeaway understandable. Minor ambiguity remains about why/when the model prefers the user (no mechanism details), but the phenomenon is intelligible."
            },
            {
              "question": "1.3. Completeness: Is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.25,
              "reason": "The figure is an introductory illustration rather than an end-to-end summary. It captures the motivation/problem statement (authority bias under user–retrieval conflict in RAG) but does not summarize the paper’s full arc: dataset (ABDD), metrics, experimental evaluation across models/tasks, and the proposed mitigation (CDEQ) and comparative results. Thus it is far from covering the paper from start to finish."
            }
          ]
        },
        "Fidelity": {
          "score": 0.9,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.9,
              "reason": "The figure content aligns with the paper’s described RAG setting and the Authority Bias phenomenon (user context vs retrieved database chunks, LLM generation). It does not introduce equations/metrics or extra methodological modules (e.g., CDEQ details) that would be absent from this introductory illustration. Minor potential over-specificity exists in depicting a particular example entity (e.g., CEO name) and interface-style icons, but these are illustrative rather than new technical components."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.92,
              "reason": "The relationships are consistent with the paper’s narrative: a retriever accesses an indexed database to return chunks; the prompt combines the user query (optionally with user-provided context) plus retrieved evidence; the LLM generates an answer. The key causal claim illustrated—conflict between user-provided context and retrieved facts leading the LLM to favor the user input (authority bias)—matches the described phenomenon. The diagram is schematic and does not mis-state any processing order or dependency among RAG components."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.88,
              "reason": "Major components are labeled in line with common RAG terminology and the paper context: User, Query, Retrieval/Database, Chunks, Prompt/Combine, LLM, Generation, and the highlighted 'Authority Bias'. The only notable issue is minor textual/label noise (e.g., 'Chuck 2' vs 'Chunk 2' in the embedded prompt text in the paper excerpt; in the provided figure it appears as 'Chunk 2', which is fine). Overall, labels correspond to the paper’s described elements and do not misname a method."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.72,
          "sub_metrics": [
            {
              "question": "Overall Readability",
              "score": 0.72,
              "reason": "The main narrative (simple query vs. user-provided conflicting context leading to different outputs) is visually traceable and the color-coding helps separate conditions. However, readability is reduced by (i) small font sizes in several text blocks (chunks, prompts) that become hard to parse at typical paper zoom levels, (ii) dense on-figure prose (full-sentence chunks/prompts) where short paraphrases would suffice, (iii) multiple nested dashed boxes and repeated labels that add visual clutter, and (iv) mixed icon styles and decorative elements (robot faces, warning sign) that draw attention away from the core flow. Overall, understandable but somewhat busy and text-heavy."
            }
          ]
        },
        "Design Quality": {
          "score": 0.807,
          "sub_metrics": [
            {
              "question": "4.1. Direction: Does the diagram flow from left to right or top to bottom?",
              "score": 0.9,
              "reason": "Overall information flow is clearly left-to-right: User/Query on the left, Retrieval in the middle, Generation/LLM on the right. The top (normal case) vs bottom (conflict case) comparison is also readable."
            },
            {
              "question": "4.2. Crossing: Do the connection lines avoid crossing each other?",
              "score": 0.75,
              "reason": "Most connectors are clean, but there are a few places where arrows/paths visually intersect or come very close (notably around the central Retrieval block and the red dashed conflict prompt routing), creating mild visual clutter even if not severe ambiguity."
            },
            {
              "question": "4.3. Proximity: Are functionally closely related modules physically placed near each other?",
              "score": 0.85,
              "reason": "Retrieval subcomponents (Indexing/Database/Retrieve/Chunks) are grouped; Generation and LLM are grouped. The two scenarios (simple vs conflict) are separated, though some repeated elements (prompts/chunks) are spatially distant from their corresponding LLM outputs, requiring eye travel."
            },
            {
              "question": "4.4. Alignment: Are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.8,
              "reason": "Major blocks align reasonably (left inputs, central retrieval, right generation), but internal elements (dashed prompt boxes, labels, arrows) have slight misalignments and inconsistent baseline/centering, especially in the bottom red conflict pathway."
            },
            {
              "question": "4.5. Hierarchy: Do the important main components stand out in size, thickness, or position?",
              "score": 0.85,
              "reason": "Primary modules (User side, Retrieval, Generation) are prominent via large containers and central placement. The warning icon and 'Authority Bias' label draw attention, though the figure also contains many small text blocks that compete with the main message."
            },
            {
              "question": "4.6. Margin: Is there sufficient margin between elements?",
              "score": 0.7,
              "reason": "Several regions are dense: the retrieval chunk text boxes and combined prompt boxes are packed, and the bottom scenario has tight spacing between the red dashed boundary, prompt box, and arrows. Some labels sit close to borders, reducing breathing room."
            },
            {
              "question": "4.7. Consistency: Are components with similar roles represented with the same shape and color?",
              "score": 0.8,
              "reason": "Consistent use of dashed boxes for prompts/queries and container blocks for modules; blue for normal flow and red for conflict/user-provided context helps. However, mixed icon styles (user icons, database icons, robot) and varied border styles (solid, dashed, rounded) introduce minor inconsistency."
            }
          ]
        },
        "Creativity": {
          "score": 0.627,
          "sub_metrics": [
            {
              "question": "5.1. Metaphor: To what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.78,
              "reason": "The figure concretizes the abstract notion of “authority bias” through clear visual metaphors: user vs. retrieval lanes, the LLM as an agent icon, green check/red cross outcomes, and a warning triangle to signal bias. These symbols effectively encode trust/preference and correctness without heavy text dependence. However, some key ideas (e.g., ‘conflict’, ‘authority’) are still conveyed largely via labels and dashed boxes rather than richer metaphorical encoding."
            },
            {
              "question": "5.2. Novelty: Does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.46,
              "reason": "The overall aesthetic resembles a standard NLP/system-architecture schematic: rounded rectangles, dashed grouping boxes, arrows, and familiar iconography (user silhouette, database cylinder, LLM chip/robot). The user-vs-retrieval comparison is clear but not especially stylistically distinctive. The combination of correctness markers and caution icon adds some personality, yet the design remains close to common RAG pipeline illustrations."
            },
            {
              "question": "5.3. Adaptability: Does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.64,
              "reason": "The layout is tailored to the paper’s core claim by explicitly contrasting two scenarios (simple query vs. query with user-provided conflicting context) and showing the same retrieval contents leading to different generations. The “VS” structure and conflict callout make the narrative argument immediate. Still, it largely adheres to conventional left-to-right pipeline grouping and common modular boxes, rather than introducing a substantially new layout paradigm."
            }
          ]
        },
        "weighted_total": 0.721
      }
    }
  ]
}