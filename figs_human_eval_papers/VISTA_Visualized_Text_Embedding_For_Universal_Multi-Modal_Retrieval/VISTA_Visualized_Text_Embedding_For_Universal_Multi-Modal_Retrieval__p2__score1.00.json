{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2024.acl-long.175.pdf",
  "page": 2,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: The model architecture of our VISTA model. We use the pre-trained language model as the foundation, making the ViT encoder transfer the Image to recognized tokens of the text encoder.",
  "regionBoundary": {
    "x1": 321.59999999999997,
    "x2": 511.2,
    "y1": 77.28,
    "y2": 259.68
  },
  "score": 1.0,
  "reason": "Diagram shows an overview of a system combining encoders for image, text, and multimodal data."
}