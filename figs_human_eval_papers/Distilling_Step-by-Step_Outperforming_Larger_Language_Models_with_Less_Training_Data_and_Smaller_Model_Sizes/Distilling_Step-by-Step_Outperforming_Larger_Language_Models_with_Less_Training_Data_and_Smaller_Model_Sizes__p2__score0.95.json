{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2023.findings-acl.507.pdf",
  "page": 2,
  "figureType": null,
  "name": "2",
  "caption": "Figure 2: Overview on Distilling step-by-step. We first utilize CoT prompting to extract rationales from an LLM (Section 3.1). We then use the generated rationales to train small task-specific models within a multi-task learning framework where we prepend task prefixes to the input examples and train the model to output differently based on the given task prefix (Section 3.2).",
  "regionBoundary": {
    "x1": 69.6,
    "x2": 525.12,
    "y1": 69.6,
    "y2": 241.92
  },
  "score": 0.95,
  "reason": "Depicts a high-level workflow and system overview for LLM and smaller model pipeline."
}