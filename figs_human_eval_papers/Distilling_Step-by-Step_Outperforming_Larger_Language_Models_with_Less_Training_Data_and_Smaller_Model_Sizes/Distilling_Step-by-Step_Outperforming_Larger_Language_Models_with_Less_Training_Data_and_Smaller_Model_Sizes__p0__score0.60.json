{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2023.findings-acl.507.pdf",
  "page": 0,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: While large language models (LLMs) offer strong zero/few-shot performance, they are challenging to serve in practice. Traditional ways of training small task-specific models, on the other hand, requires large amount of training data. We propose Distilling step-by-step, a new paradigm that extracts rationales from LLMs as informative task knowledge into training small models, which reduces both the deployed model size as well as the data required for training.",
  "regionBoundary": {
    "x1": 309.59999999999997,
    "x2": 518.4,
    "y1": 211.67999999999998,
    "y2": 317.28
  },
  "score": 0.6,
  "reason": "Conceptual overview of model comparison; partially a framework but not detailed architecture."
}