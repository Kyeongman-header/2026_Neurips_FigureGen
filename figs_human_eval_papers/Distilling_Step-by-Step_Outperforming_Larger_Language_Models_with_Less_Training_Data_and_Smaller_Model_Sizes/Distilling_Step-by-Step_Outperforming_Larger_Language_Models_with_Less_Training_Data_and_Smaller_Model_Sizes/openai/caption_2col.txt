Overview of Distilling step-by-step: an LLM is prompted to generate task labels and chain-of-thought rationales for unlabeled inputs, then a smaller T5 model is trained multi-task to predict both labels and rationales from inputs, enabling LLM-level accuracy with fewer examples and much smaller deployed models.