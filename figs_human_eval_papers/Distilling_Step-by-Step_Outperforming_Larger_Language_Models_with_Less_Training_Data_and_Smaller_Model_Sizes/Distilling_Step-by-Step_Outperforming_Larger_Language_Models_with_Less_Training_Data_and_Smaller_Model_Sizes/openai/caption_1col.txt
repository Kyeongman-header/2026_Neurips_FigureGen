Overview of Distilling step-by-step: a large language model is prompted to generate task labels and natural-language rationales, and a smaller T5 model is trained multi-task to predict both, improving data efficiency and enabling LLM-level performance with substantially smaller deployed models.