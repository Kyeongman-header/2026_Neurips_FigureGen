{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2024.findings-emnlp.463.pdf",
  "page": 2,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: Comparison of different frameworks: SFT, DPO, and SVPO. The top panel shows the typical pipeline of SFT and DPO, where GPT-4 does not indicate which step in yl led to the mistake. The bottom panel illustrates the pipeline of SVPO. Step-level preferences are autonomously generated via MCTS, where Q-values (represented by node colors) indicate potential reasoning errors.",
  "regionBoundary": {
    "x1": 70.56,
    "x2": 524.16,
    "y1": 70.56,
    "y2": 297.12
  },
  "score": 1.0,
  "reason": "Depicts an end-to-end framework with labeled modules and process flow for system understanding."
}