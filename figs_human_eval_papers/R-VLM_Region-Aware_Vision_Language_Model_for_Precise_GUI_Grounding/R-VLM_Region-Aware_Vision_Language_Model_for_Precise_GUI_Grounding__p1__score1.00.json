{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2025.findings-acl.501.pdf",
  "page": 1,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: Illustration of the Region-Aware Vision Language Model (R-VLM). Our approach consists of two modules for precise GUI grounding: (a) A two-stage zoom-in grounding process that refines predictions via a zoomed-in view of region proposal. After obtaining an initial prediction from the model using GUI screenshot and user instruction, which serves as a region proposal, we zoom-in around this region and pass it through the model again for second-stage grounding. (b) An IoU-aware weighted cross-entropy loss that provides a smooth learning signal based on the IoU value rather than strictly fitting to ground-truth bounding box. This loss assigns weights to pseudo bounding boxes according to their IoU value with ground-truth to emphasize high IoU grounding predictions.",
  "regionBoundary": {
    "x1": 72.0,
    "x2": 523.1999999999999,
    "y1": 65.75999999999999,
    "y2": 250.07999999999998
  },
  "score": 1.0,
  "reason": "Illustrates system pipelines and interactions between components for two methods."
}