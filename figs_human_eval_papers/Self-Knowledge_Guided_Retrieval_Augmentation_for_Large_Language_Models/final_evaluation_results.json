{
  "paper_name": "arXiv_2310.05002v1_cs.CL_8_Oct_2023",
  "evaluated_at": "2025-12-28T02:16:45.146369",
  "figure_evaluations": [
    {
      "figure_file": "arXiv_2310.05002v1_cs.CL_8_Oct_2023__p3__score1.00.png",
      "caption": "Figure 3: Illustration of k-nearest-neighbor search to elicit the self-knowledge to the question qt.",
      "scores": {
        "Informativeness": {
          "score": 0.417,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage: Does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.45,
              "reason": "The figure conveys one specific mechanism (k-NN search over labeled training questions to infer whether a new question is known/unknown using an encoder and similarity). However, it omits other major components of the SKR framework described in the paper context (e.g., how self-knowledge labels are collected via with/without retrieval performance, how the retriever is adaptively invoked afterward, prompting-based elicitation vs small model training strategies, and any formal definitions/equations). It is informative for the k-NN elicitation variant but not for the overall method."
            },
            {
              "question": "1.2. Standalone Intelligibility: Can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.6,
              "reason": "A reader can infer the local operating principle: encode the test question qt, compare it to embeddings of training questions separated into positive/negative (known/unknown), and decide via nearest neighbors (Pos./Neg.). Still, key details needed for full understanding are missing: what positives/negatives mean operationally, what the encoder is trained/frozen on, what similarity metric/k is used, and what action is taken after the known/unknown decision."
            },
            {
              "question": "1.3. Completeness: Is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.2,
              "reason": "No. This figure illustrates only one sub-step (k-NN-based elicitation of self-knowledge) rather than the end-to-end SKR pipeline (collection of self-knowledge, alternative elicitation methods, and using self-knowledge to control retrieval augmentation and final answering), and it does not reflect experimental setup/results or broader paper narrative."
            }
          ]
        },
        "Fidelity": {
          "score": 0.75,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.8,
              "reason": "The figure shows training questions split into known/unknown (q^+ and q^-), an encoder (with frozen parameters), the test question q_t, and a positive/negative decision via nearest-neighbor-style matching. These elements are consistent with the paper’s described approach of eliciting self-knowledge by referring to collected training questions and using either LLM prompting or a small model. Minor risk: the diagram’s specific contrastive-style “Pos./Neg.” visualization and explicit q^+/q^- notation may be a schematic interpretation rather than an exact component naming in the text, but it does not introduce clearly foreign mechanisms beyond what SKR implies."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.7,
              "reason": "The relationships depicted—encoding q_t, comparing against encoded training questions with known/unknown labels, and using nearest neighbors to infer the self-knowledge label—match the stated idea of eliciting self-knowledge by referring to previously collected training questions (kNN-style). However, the broader SKR pipeline also includes retrieval augmentation decisions (whether to call a retriever) and collection of labels via performance with/without retrieval, which are not represented in this specific figure (acceptable for a subfigure). The encoder being frozen is plausible but not fully supported by the provided text excerpt."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.75,
              "reason": "Labels like “Training Questions,” “Encoder,” “Question q_t,” and “Pos./Neg.” are consistent with a kNN elicitation module. Still, the paper’s terminology centers on “known/unknown” self-knowledge and strategies (LLM prompting or small trainable model). The figure uses Pos./Neg. rather than known/unknown directly, and the q^+/q^- notation may not exactly match the paper’s labels, though the mapping is clear and not misleading."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.72,
          "sub_metrics": [
            {
              "question": "Overall Readability",
              "score": 0.72,
              "reason": "The main flow (training questions → encoder → query qt → neighbor selection) is visually clear, with a strong left-to-right pipeline and consistent color cues for positive/negative examples. However, readability is reduced by several factors: small text (e.g., q^+_1, q^-_2) and thin lines that may blur at typical paper zoom levels; mixed visual metaphors (boxes, dashed circles, scattered points) without an explicit legend; and some clutter (many q symbols) that communicates ‘a set’ but can look busy. The caption helps, but the figure would be easier to parse with larger labels, fewer exemplar variables, and clearer annotation of what the dashed circle and colored points represent."
            }
          ]
        },
        "Design Quality": {
          "score": 0.85,
          "sub_metrics": [
            {
              "question": "4.1. Direction: Does the diagram flow from left to right or top to bottom?",
              "score": 0.82,
              "reason": "Overall left-to-right progression is clear: training questions on the left feed an encoder, which produces a query embedding and then a Pos./Neg. decision on the right. Minor ambiguity remains because the rightmost cluster is arranged more radially than strictly directional."
            },
            {
              "question": "4.2. Crossing: Do the connection lines avoid crossing each other?",
              "score": 0.9,
              "reason": "Arrows/links are mostly non-overlapping and do not create confusing crossings. The only slight visual congestion is where multiple arrows converge near the encoder, but they still remain readable."
            },
            {
              "question": "4.3. Proximity: Are functionally closely related modules physically placed near each other?",
              "score": 0.88,
              "reason": "The two groups of training questions are colocated and both connect directly to the encoder; the question qt is placed adjacent below the encoder indicating its role. The embedding/neighbor set and Pos./Neg. outcome are grouped on the right, maintaining functional locality."
            },
            {
              "question": "4.4. Alignment: Are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.78,
              "reason": "Main blocks (training set container, encoder, right-side output area) are reasonably aligned on a horizontal axis. However, internal elements (lists of questions, right-side points) are not grid-aligned by design, and spacing is slightly irregular around the encoder inputs."
            },
            {
              "question": "4.5. Hierarchy: Do the important main components stand out in size, thickness, or position?",
              "score": 0.86,
              "reason": "Key components (Training Questions container and Encoder block) are visually prominent via size and bounding boxes, and the Pos./Neg. label is emphasized with color. The central encoder placement supports hierarchy, though line weights are fairly uniform so emphasis relies mostly on size/color."
            },
            {
              "question": "4.6. Margin: Is there sufficient margin between elements?",
              "score": 0.84,
              "reason": "Most elements have adequate whitespace, with clear separation between left container, encoder, and right output. The densest area is immediately around the encoder where multiple arrows and labels cluster, but margins remain acceptable."
            },
            {
              "question": "4.7. Consistency: Are components with similar roles represented with the same shape and color?",
              "score": 0.87,
              "reason": "Training-question lists use consistent formatting; the two classes are consistently colored (green for positive/known, red for negative/unknown). The encoder is consistently represented as a single yellow block. Minor inconsistency: the right-side embedding points mix shape/outline styles without an explicit legend."
            }
          ]
        },
        "Creativity": {
          "score": 0.303,
          "sub_metrics": [
            {
              "question": "5.1. Metaphor: To what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.32,
              "reason": "Uses a standard schematic metaphor (encoder block, embeddings as dots, green/red for positive/negative, dashed grouping boxes) to concretize retrieval/classification, but relies on conventional ML diagram tropes rather than distinctive symbolic/iconic replacements for the abstract ideas."
            },
            {
              "question": "5.2. Novelty: Does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.18,
              "reason": "Visual language and composition closely match common representation-learning figures (Siamese/dual-encoder style, positive/negative samples, embedding space dots). Color and layout choices are typical and do not introduce a notably unique visual style."
            },
            {
              "question": "5.3. Adaptability: Does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.41,
              "reason": "Some tailoring to the paper’s SKR setting is evident (explicit separation of known/unknown training questions and kNN pos/neg decision around qt), but the overall layout remains a generic encoder-to-embedding-space pipeline without a more paper-specific or unconventional arrangement."
            }
          ]
        },
        "weighted_total": 0.608
      }
    },
    {
      "figure_file": "arXiv_2310.05002v1_cs.CL_8_Oct_2023__p2__score1.00.png",
      "caption": "Figure 2: The overall pipeline of our SKR method. We first collect self-knowledge from training questions according to the performance with or without external information (§ 3.1). Then we use the LLMs themselves or explicit small trainable models to elicit self-knowledge of a question qt by referring to the collected self-knowledge from training questions (§ 3.2). Finally, we use the self-knowledge to the new question and adaptively call a retriever (§ 3.3).",
      "scores": {
        "Informativeness": {
          "score": 0.667,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage: Does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.75,
              "reason": "The figure captures the core components of SKR (collecting self-knowledge via performance w/ vs. w/o retrieval, eliciting self-knowledge using LLM or small model with known/unknown training questions, and using it to decide whether to call the retriever). However, it omits many major methodological details typically present in the paper, such as the exact criteria/metric for labeling known vs. unknown, how retrieved passages are incorporated into prompts, specifics of the retriever/corpus, and any key equations or training objectives beyond the high-level blocks."
            },
            {
              "question": "1.2. Standalone Intelligibility: Can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.85,
              "reason": "The pipeline structure is clear and readable: training questions are partitioned into LLM-knows vs. LLM-unknowns (based on retrieval impact), a classifier/elicitor predicts known/unknown for a test question by referencing those sets, and retrieval is used only when predicted unknown. A viewer can grasp the decision-gated retrieval principle, though implementation details (what “known/unknown” operationally means and how elicitation works) are not fully inferable."
            },
            {
              "question": "1.3. Completeness: Is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.4,
              "reason": "The figure summarizes the method section’s end-to-end workflow, but it does not cover the broader paper arc: motivation/negative retrieval example, related work positioning, dataset/evaluation setup, baselines, experimental results, ablations, and conclusions/limitations. Thus it is not a complete summary of the paper from start to finish."
            }
          ]
        },
        "Fidelity": {
          "score": 0.95,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.95,
              "reason": "The figure shows only elements described in the provided paper context/caption: collecting self-knowledge by comparing LLM vs. retrieval-augmented performance on training questions, eliciting self-knowledge using the LLM itself or small trainable models with reference to known/unknown training questions, and using that decision to optionally call a retriever. No extra formulas or unexplained modules are introduced beyond these described components."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.93,
              "reason": "The pipeline relationships are consistent with the caption: (1) collect known/unknown sets from training questions based on performance with/without external info; (2) use those labeled training questions as references for a new question qt to predict known/unknown; (3) if unknown, call retriever and answer with external info, else answer directly. The directional flow and gating logic (known → no retrieval; unknown → retrieval) matches the described adaptive retrieval augmentation."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.97,
              "reason": "Major labels align with the paper: 'Collecting Self-Knowledge', 'Eliciting Self-Knowledge', 'Using Self-Knowledge', 'retriever', 'LLM itself / small trainable models', and the known/unknown decision. The use of qt for the test question and qi for training questions is consistent with the caption/text, and there are no obvious misnamings of components."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.77,
          "sub_metrics": [
            {
              "question": "3.1. Summarization: Is it schematized focusing on the 'Main Contribution' rather than trivial details?",
              "score": 0.82,
              "reason": "The figure cleanly abstracts the SKR method into three sequential stages (collecting → eliciting → using self-knowledge) and highlights the key decision (known vs unknown) that triggers retrieval. It stays at an algorithmic/pipeline level rather than implementation minutiae. Minor readability dilution comes from repeated iconography and the inclusion of q+ / q− lists, which add notation load without substantially improving the high-level message."
            },
            {
              "question": "3.2. Contextual Match: Does this figure function well as a supplementary material to help understanding when reading the caption or text?",
              "score": 0.86,
              "reason": "As a companion to the caption, it supports the narrative well: the three panels align one-to-one with the described steps and visually explains how training questions inform a test-time routing decision. Terminology matches the paper (“collecting/eliciting/using self-knowledge”). Small risks: the meaning of q+ vs q− and the criteria for labeling “known/unknown” are not self-evident without the surrounding text, so standalone interpretability is moderate rather than high."
            },
            {
              "question": "3.3. Redundancy: Does it avoid decorative elements or unnecessary information unrelated to the core ideas?",
              "score": 0.63,
              "reason": "Core content is present, but there is noticeable redundancy: repeated LLM and Wikipedia/retrieval icons across panels; multiple dashed boxes; and duplicated labeling (“known/unknown”) that could be simplified. The use of logos and some styling elements (heavy borders, repeated containers) is mildly decorative and consumes space that could otherwise increase text/label legibility."
            }
          ]
        },
        "Design Quality": {
          "score": 0.886,
          "sub_metrics": [
            {
              "question": "4.1. Direction: Does the diagram flow from left to right or top to bottom?",
              "score": 0.95,
              "reason": "The pipeline is clearly organized left-to-right across three labeled stages, reinforced by a large right-pointing arrow and consistent placement of inputs/outputs within each panel."
            },
            {
              "question": "4.2. Crossing: Do the connection lines avoid crossing each other?",
              "score": 0.9,
              "reason": "Arrows and connectors are mostly local within panels and do not cross; the layout avoids tangled routing. Minor visual complexity exists in the first panel with multiple arrows, but still no crossings."
            },
            {
              "question": "4.3. Proximity: Are functionally closely related modules physically placed near each other?",
              "score": 0.9,
              "reason": "Each stage groups its relevant components (inputs, models, outputs) tightly within a bordered subpanel, and the three stages are placed adjacent in the pipeline order."
            },
            {
              "question": "4.4. Alignment: Are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.85,
              "reason": "The three major panels and titles are aligned well; internal elements are mostly centered and symmetric. Some small elements (e.g., labels and icons within the first and third panels) are slightly uneven, reducing perfect grid regularity."
            },
            {
              "question": "4.5. Hierarchy: Do the important main components stand out in size, thickness, or position?",
              "score": 0.9,
              "reason": "The three main stages are emphasized via large containers and clear headings; the global dashed border and central arrow establish primary structure. Sub-elements are visually secondary, though some icons/boxes compete slightly in salience due to strong colors."
            },
            {
              "question": "4.6. Margin: Is there sufficient margin between elements?",
              "score": 0.8,
              "reason": "Overall spacing is adequate, but the first panel feels somewhat dense (multiple boxes and arrows close together), and the outer dashed boundary is relatively tight around the content."
            },
            {
              "question": "4.7. Consistency: Are components with similar roles represented with the same shape and color?",
              "score": 0.9,
              "reason": "Panels share consistent structure and styling; 'known' vs 'unknown' is consistently color-coded (yellow tags) and repeated. Minor inconsistency arises from mixing iconography (LLM/retriever icons) with box primitives, but roles remain clear."
            }
          ]
        },
        "Creativity": {
          "score": 0.55,
          "sub_metrics": [
            {
              "question": "5.1. Metaphor: To what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.72,
              "reason": "Abstract stages (collecting/eliciting/using self-knowledge) are mapped to concrete visual tokens: LLM icon, Wikipedia/retrieval icon, known/unknown labels, and grouped question sets. The metaphor is clear and functional, though it relies on familiar AI/paper iconography rather than introducing richer or more original symbolic substitutes."
            },
            {
              "question": "5.2. Novelty: Does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.38,
              "reason": "The figure largely follows a standard ML pipeline schematic (three-panel flow, dashed boxes, arrows, pastel blocks, and recognizable service/corpus icons). It is clean, but stylistically close to common paper templates without distinctive visual language or unconventional illustration."
            },
            {
              "question": "5.3. Adaptability: Does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.55,
              "reason": "The three-stage decomposition aligns well with the method narrative (collect → elicit → use) and the known/unknown branching is tailored to the paper’s key idea (adaptive retrieval). However, the overall structure remains a conventional left-to-right pipeline rather than a notably customized layout beyond the branching decision point."
            }
          ]
        },
        "weighted_total": 0.764
      }
    },
    {
      "figure_file": "arXiv_2310.05002v1_cs.CL_8_Oct_2023__p0__score0.95.png",
      "caption": "Figure 1: Comparison between two responses given by InstructGPT. The retrieved passages are relevant but not particularly helpful for solving the question, which influences the model’s judgment and leads to incorrect answers.",
      "scores": {
        "Informativeness": {
          "score": 0.25,
          "sub_metrics": [
            {
              "question": "1.1. Content Coverage: Does it include all major components or formulas mentioned in the paper without omission?",
              "score": 0.25,
              "reason": "The figure only illustrates a motivating failure case: an LLM answer without retrieval vs with retrieved passages that degrade performance. It does not cover the paper’s main proposed components (collecting/eliciting/using self-knowledge, training question memory, detectors, adaptive retrieval policy) and includes no formulas or algorithmic details."
            },
            {
              "question": "1.2. Standalone Intelligibility: Can one understand the general operating principle of the system just by looking at this figure without reading the paper?",
              "score": 0.45,
              "reason": "One can infer the high-level problem statement (retrieval can distract an LLM even when the retrieved text is relevant) and the intended takeaway (retrieval should be applied selectively). However, the figure does not explain how the proposed method makes that decision or how self-knowledge is computed/used, so the system’s operating principle is not clear from this figure alone."
            },
            {
              "question": "1.3. Completeness: Is the content from the beginning to the end of the paper summarized and included?",
              "score": 0.05,
              "reason": "No. This figure is not a paper-wide summary; it does not depict the full SKR pipeline, training/inference flow, evaluation setup, or results. It serves as an introductory example rather than an end-to-end summary."
            }
          ]
        },
        "Fidelity": {
          "score": 0.917,
          "sub_metrics": [
            {
              "question": "2.1. Hallucination: Does it avoid including components or formulas not mentioned in the paper?",
              "score": 0.95,
              "reason": "The figure depicts only elements described in the paper context for Figure 1: a question with ground-truth answer, retrieved passages, and two InstructGPT responses (one without retrieval and one influenced by retrieval). No extra formulas, modules, or novel components are introduced beyond what is stated."
            },
            {
              "question": "2.2. Relation Correctness: Are the relationships between major components or formulas mentioned in the paper accurately represented?",
              "score": 0.9,
              "reason": "It correctly conveys the intended relationship: retrieved passages can be relevant yet unhelpful and may negatively affect the model’s answer, flipping it from correct to incorrect. The causal depiction (retrieval influencing judgment) matches the described takeaway, though the figure is illustrative rather than demonstrating a guaranteed causal mechanism."
            },
            {
              "question": "2.3. Label Accuracy: Are the names of major components or methodologies mentioned in the paper accurately labeled?",
              "score": 0.9,
              "reason": "Labels such as \"Question (Answer: Yes)\" and \"Retrieved Passages\" align with the paper’s description, and the caption correctly attributes the comparison to InstructGPT. The figure does not misname SKR or other methods; it simply illustrates retrieval augmentation effects as stated."
            }
          ]
        },
        "Overall Readability": {
          "score": 0.74,
          "sub_metrics": [
            {
              "question": "Overall Readability",
              "score": 0.74,
              "reason": "The figure is easy to follow at a glance: it clearly contrasts the no-retrieval vs. retrieval-augmented responses with green/red indicators and left-to-right flow, and the question/answer pairing is legible. However, readability is moderately reduced by small text in the retrieved passage snippet (truncated and hard to parse), reliance on color/checkmark/cross cues (potentially less clear in grayscale or for color-vision deficiencies), and slightly busy visual elements (icons/arrows) that add clutter without improving the core contrast."
            }
          ]
        },
        "Design Quality": {
          "score": 0.85,
          "sub_metrics": [
            {
              "question": "4.1. Direction: Does the diagram flow from left to right or top to bottom?",
              "score": 0.95,
              "reason": "The figure clearly reads left-to-right: question on the left, then arrows to model responses on the right (with retrieved passage influence depicted). Arrow directions reinforce the intended flow."
            },
            {
              "question": "4.2. Crossing: Do the connection lines avoid crossing each other?",
              "score": 0.9,
              "reason": "Connectors are mostly non-crossing; the two arrows from the retrieved passage to the two responses are separated and do not visibly intersect, keeping relationships easy to follow."
            },
            {
              "question": "4.3. Proximity: Are functionally closely related modules physically placed near each other?",
              "score": 0.9,
              "reason": "Each response is placed close to its associated icon and correctness mark, and the retrieved passage sits near the point where it influences both answers. The question and its answers are spatially grouped logically."
            },
            {
              "question": "4.4. Alignment: Are nodes aligned neatly horizontally and vertically according to an invisible grid?",
              "score": 0.75,
              "reason": "Major elements are broadly aligned (question box left; two response boxes right), but the retrieved passage box and connector placements feel slightly ad hoc, with minor misalignment in vertical spacing between the two answer rows."
            },
            {
              "question": "4.5. Hierarchy: Do the important main components stand out in size, thickness, or position?",
              "score": 0.8,
              "reason": "Primary components (question and two contrasting answers) are prominent via larger boxes and central placement; correctness is emphasized with green check/red cross. However, the retrieved passage’s role could be made more visually distinct (e.g., stronger label/contrast) relative to other boxes."
            },
            {
              "question": "4.6. Margin: Is there sufficient margin between elements?",
              "score": 0.8,
              "reason": "Overall spacing is adequate, but some items (icons, arrows, and box borders) are relatively tight, especially around the retrieved passage and the arrows entering the right-hand answer boxes."
            },
            {
              "question": "4.7. Consistency: Are components with similar roles represented with the same shape and color?",
              "score": 0.85,
              "reason": "The two answers are rendered with consistent box styles and parallel structure; correctness is consistently encoded with green/red. Minor inconsistency comes from mixed icon styles (model icons vs. retrieved-passage icon) and varying arrow color usage not fully explained by a legend."
            }
          ]
        },
        "Creativity": {
          "score": 0.443,
          "sub_metrics": [
            {
              "question": "5.1. Metaphor: To what extent are abstract concepts replaced with concrete icons, symbols, or abbreviations?",
              "score": 0.42,
              "reason": "Uses concrete UI-like elements (checkmark/cross, arrows, boxed text) to symbolize correctness vs. incorrectness and causal influence, but largely relies on literal text snippets rather than stronger metaphorical icons/symbol systems for concepts like “retrieval distraction” or “internal knowledge.”"
            },
            {
              "question": "5.2. Novelty: Does the generated image have a unique style that differentiates it from common templates?",
              "score": 0.33,
              "reason": "The figure resembles a standard paper schematic/comparison panel: question on the left, retrieved passage, two outputs with success/failure markers. Styling and visual language are conventional (boxes, arrows, ticks/crosses) with limited distinctive visual identity beyond minor polish."
            },
            {
              "question": "5.3. Adaptability: Does it apply a layout more suitable for the specific paper, breaking away from uniform design principles?",
              "score": 0.58,
              "reason": "The layout is tailored to the paper’s key claim (retrieval can harm) by juxtaposing no-retrieval vs retrieval-conditioned outputs and visually encoding the negative effect. While still following common comparison patterns, it is appropriately customized to communicate this specific failure mode succinctly."
            }
          ]
        },
        "weighted_total": 0.64
      }
    }
  ]
}