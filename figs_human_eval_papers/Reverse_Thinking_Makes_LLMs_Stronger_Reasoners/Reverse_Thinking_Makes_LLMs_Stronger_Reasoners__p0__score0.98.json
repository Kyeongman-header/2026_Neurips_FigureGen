{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2025.naacl-long.434.pdf",
  "page": 0,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: Comparison between symbolic knowledge distillation (SKD) and our method. (1) the teacher model generates multiple reasoning chains for a given question, (2) SKD supervised fine-tunes on the correct reasoning chains, and (3) our method incorporates bidirectional reasoning, learning from both Q-to-A and Ato-Q using our multi-task objectives.",
  "regionBoundary": {
    "x1": 308.15999999999997,
    "x2": 526.0799999999999,
    "y1": 221.76,
    "y2": 447.35999999999996
  },
  "score": 0.98,
  "reason": "Depicts an overall system architecture with stepwise process and objectives for a reasoning framework."
}