{
  "source_pdf": "/home/zzangmane/2025_null_FigureGen/for_human_eval_papers/2023.findings-acl.528.pdf",
  "page": 0,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: Main idea of PARC: we enhance zero-shot learning for low-resource languages (LRLs) by crosslingual retrieval from labeled/unlabeled high-resource languages (HRLs). (a) An LRL input sample is taken as query by the cross-lingual retriever to retrieve the semantically most similar HRL sample from the HRL corpus. The label of the retrieved HRL sample is obtained either from the corpus (labeled setting) or by self-prediction (unlabeled setting). (b) The retrieved HRL sample together with its label and the input sample are reformulated as prompts. The cross-lingual retrievalaugmented prompt is created by concatenation and taken by the MPLM for prediction. Our experiments show that PARC outperforms other zero-shot methods and even finetuning.",
  "regionBoundary": {
    "x1": 315.84,
    "x2": 528.0,
    "y1": 211.67999999999998,
    "y2": 453.59999999999997
  },
  "score": 1.0,
  "reason": "Depicts system overviews for retrieval and prediction workflows in an NLP framework."
}