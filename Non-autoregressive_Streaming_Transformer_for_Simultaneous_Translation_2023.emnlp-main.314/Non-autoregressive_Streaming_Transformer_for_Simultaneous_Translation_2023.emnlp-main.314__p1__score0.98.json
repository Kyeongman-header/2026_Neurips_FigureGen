{
  "source_pdf": "/home/zzangmane/2025_null_Figure/pdfs/emnlp-2023/Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314.pdf",
  "page": 1,
  "figureType": null,
  "name": "1",
  "caption": "Figure 1: Illustration of the non-monotonicity problem and the source-info leakage bias in the training of autoregressive SiMT models. In this case, the AR SiMT model learns to predict at the third time step based on the source contexts \"布什 (Bush)\", \"与 (and)\", \"沙龙 (Sharon)\", and the ground truth contexts \"Bush\", \"held\". Although the source token \"举行 (hold)\" has not been read yet, it is exposed to the AR SiMT model through its corresponding token \"held\" in the ground truth context.",
  "regionBoundary": {
    "x1": 74.88,
    "x2": 515.52,
    "y1": 79.67999999999999,
    "y2": 200.16
  },
  "score": 0.98,
  "reason": "Shows the overall architecture of an encoder-decoder system with annotated data flow."
}