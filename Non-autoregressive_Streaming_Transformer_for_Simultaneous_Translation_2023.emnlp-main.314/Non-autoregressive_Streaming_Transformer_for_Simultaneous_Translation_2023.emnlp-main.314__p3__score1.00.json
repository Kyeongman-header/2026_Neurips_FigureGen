{
  "source_pdf": "/home/zzangmane/2025_null_Figure/pdfs/emnlp-2023/Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314.pdf",
  "page": 3,
  "figureType": null,
  "name": "2",
  "caption": "Figure 2: Overview of the proposed non-autoregressive streaming Transformer (NAST). Upon receiving a source token, NAST upsamples it λ times and feeds them to the decoder as a chunk. NAST can generate blank token ϵ or repetitive tokens (both highlighted in gray) to find reasonable READ/WRITE paths adaptively. We train NAST using the non-monotonic latent alignment loss (Shao and Feng, 2022) with the alignment-based latency loss to achieve translation of high quality while maintaining low latency.",
  "regionBoundary": {
    "x1": 74.39999999999999,
    "x2": 521.28,
    "y1": 79.2,
    "y2": 180.95999999999998
  },
  "score": 1.0,
  "reason": "Depicts an end-to-end system architecture with encoder and decoder modules."
}