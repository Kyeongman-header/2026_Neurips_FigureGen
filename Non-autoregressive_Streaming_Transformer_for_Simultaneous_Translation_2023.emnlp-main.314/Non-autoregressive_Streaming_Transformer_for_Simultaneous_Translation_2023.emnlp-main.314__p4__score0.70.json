{
  "source_pdf": "/home/zzangmane/2025_null_Figure/pdfs/emnlp-2023/Non-autoregressive_Streaming_Transformer_for_Simultaneous_Translation_2023.emnlp-main.314.pdf",
  "page": 4,
  "figureType": null,
  "name": "3",
  "caption": "Figure 3: Illustration of cross-attention with different chunk wait-k strategies.",
  "regionBoundary": {
    "x1": 91.2,
    "x2": 262.08,
    "y1": 81.6,
    "y2": 223.2
  },
  "score": 0.7,
  "reason": "Shows conceptual visualization of decoder positions vs. source with different k, not detailed architecture."
}